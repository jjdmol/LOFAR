#                                                         LOFAR IMAGING PIPELINE
#
#                                 BBS reducer (BlackBoard Selfcal) master recipe
#                                                             Marcel Loose, 2012
#                                                                loose@astron.nl
# ------------------------------------------------------------------------------

import sys
import lofarpipe.support.lofaringredient as ingredient

from lofarpipe.support.baserecipe import BaseRecipe
from lofarpipe.support.group_data import load_data_map, store_data_map
from lofarpipe.support.group_data import validate_data_maps
from lofarpipe.support.remotecommand import ComputeJob
from lofarpipe.support.remotecommand import RemoteCommandRecipeMixIn

class bbs_reducer(BaseRecipe, RemoteCommandRecipeMixIn):
    """
    Run bbs-reducer in a non-distributed way on a number of MeasurementSets.
    
    **Arguments**
    
    A mapfile describing the data to be processed.
    """
    inputs = {
        'parset': ingredient.FileField(
            '-p', '--parset',
            dest="parset",
            help="BBS configuration parset"
        ),
        'executable': ingredient.ExecField(
            '--executable',
            help="The full path to the BBS-reducer executable"
        ),
        'instrument_mapfile': ingredient.FileField(
            '--instrument-mapfile',
            help="Full path to the mapfile containing the names of the "
                 "instrument model files generated by the `parmdb` recipe"
        ),
        'sky_mapfile': ingredient.FileField(
            '--sky-mapfile',
            help="Full path to the mapfile containing the names of the "
                 "sky model files generated by the `sourcedb` recipe"
        ),
        'data_mapfile': ingredient.StringField(
            '--data-mapfile',
            help="Full path to the mapfile that will contain the names of the "
                 "data files that were successfully processed by BBS"
        ),
    }
    
    outputs = {
        'mapfile': ingredient.FileField(
            help="Full path to a mapfile describing the processed data"
        )
    }
    

    def __init__(self):
        """
        Initialize our data members.
        """
        super(bbs_reducer, self).__init__()
        self.bbs_map = list()
        self.jobs = list()


    def _make_bbs_map(self):
        """
        This method bundles the contents of three different map-files.
        All three map-files contain a list of tuples of hostname and filename.
        The contents of these files are related by index in the list. They
        form triplets of MS-file, its associated instrument model and its
        associated sky model.

        The data structure `self.bbs_map` is a list of tuples, where each
        tuple is a pair of hostname and the aforementioned triplet.

        For example:
        bbs_map[0] = ('locus001',
            ('/data/L29697/L29697_SAP000_SB000_uv.MS',
            '/data/scratch/loose/L29697/L29697_SAP000_SB000_uv.MS.instrument',
            '/data/scratch/loose/L29697/L29697_SAP000_SB000_uv.MS.sky')
        )
        
        Returns `False` if validation of the three map-files fails, otherwise
        returns `True`.
        """
        self.logger.debug("Creating BBS map-file using: %s, %s, %s" %
                          (self.inputs['args'][0],
                           self.inputs['instrument_mapfile'],
                           self.inputs['sky_mapfile']))
        data_map = load_data_map(self.inputs['args'][0])
        instrument_map = load_data_map(self.inputs['instrument_mapfile'])
        sky_map = load_data_map(self.inputs['sky_mapfile'])

        if not validate_data_maps(data_map, instrument_map, sky_map):
            self.logger.error("Validation of input data mapfiles failed")
            return False

        self.bbs_map = [
            (dat[0], (dat[1], ins[1], sky[1]))
            for dat, ins, sky in zip(data_map, instrument_map, sky_map)
        ]
        
        return True


    def _handle_errors(self):
        """
        Handle errors from the node scripts. If all jobs returned a (fatal)
        error, then the recipe should abort; return 1.
        Otherwise it should report that some jobs failed and continue with the
        remaining, successfully processed Measurement Set files; return 0.
        """
        if self.error.isSet():
            # Abort if all jobs failed
            if all(job.results['returncode'] != 0 for job in self.jobs):
                self.logger.error("All BBS-reducer jobs failed. Bailing out!")
                return 1
            else:
                self.logger.warn(
                    "Some BBS-reducer jobs failed, "
                    "continuing with succeeded runs"
            )
        return 0


    def _write_data_mapfile(self):
        """
        Write a new data map-file containing only the successful runs.
        """
        outdata = []
        for job in self.jobs:
            if job.results['returncode'] == 0:
                # The first item in job.arguments is a tuple of file names, 
                # whose first item is the name of the MS-file
                # (see `_make_bbs_map` for details).
                outdata.append((job.host, job.arguments[0][0]))

        # Write output data-mapfile
        self.logger.debug(
            "Writing data map file: %s" % self.inputs['data_mapfile']
        )
        store_data_map(self.inputs['data_mapfile'], outdata)
        self.outputs['mapfile'] = self.inputs['data_mapfile']


    def go(self):
        """
        This it the actual workhorse. It is called by the framework. We pass
        three arguments to the node script: a tuple of file names (MS-file,
        parmdb-file, sourcedb-file), the path to the BBS-reducer executable,
        and the environment variables that are stored in self.environment.
        """
        self.logger.info("Starting BBS-reducer run")
        super(bbs_reducer, self).go()

        # Create a bbs_map describing the file mapping on disk
        if not self._make_bbs_map():
            return 1

        # Create and schedule the compute jobs
        command = "python %s" % (self.__file__.replace('master', 'nodes'))
        for host, files in self.bbs_map:
            self.jobs.append(
                ComputeJob(
                    host, command, 
                    arguments=[
                        files,
                        self.inputs['executable'],
                        self.inputs['parset'],
                        self.environment
                    ]
                )
            )
        self._schedule_jobs(self.jobs)

        # Write output data map-file
        self._write_data_mapfile()

        # Handle errors, if any.
        return self._handle_errors()


if __name__ == '__main__':
    sys.exit(bbs_reducer().main())

