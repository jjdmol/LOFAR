\documentclass{llncs}
\begin{document}
\newcommand{\comment}[1]{}
\begin{abstract}
Lorem ipsum.
\end{abstract}
\section{Introduction}

\comment{
  EUROPAR 2011:
    - abstract 24 jan
    - deadline 31 jan
    - 12 pagina's LNCS, incl broncode
}

\comment{
lofar:
  - overview
  - #stations
  - data rates

pulsar pipeline:  
  - new astronomical opportunities:
        - dynamic focus -> hundreds of simultaenous observations, which regular dishes must do sequentially
        - broad sky view -> surveys
        - extremely high data rates (up to 200 Gbit/s in, 18 Gbit/s out)
                - disks limits output rate, to be raised to 80Gbit/s out.

==> power of parallel machines + flexibility of software = powerful telescope                

software correlator benefits:
  the- flexibility
  - fast rollout of experimental features
  - easy bugfixing
  - high level programming -> advanced and complex features

}

The LOFAR (LOw Frequency ARray) telescope is the first of a new generation of radio telescopes. Instead of using a set of large, expensive dishes, LOFAR uses many thousands of simple antennas. Every antenna observes the full sky, and the telescope can be aimed through signal processing techniques. LOFAR's novel design allows the telescope to perform wide-angle observations as well as to observe in multiple directions simultaneously, neither of which are possible when using traditional dishes. In several ways, LOFAR will be the largest telescope in the world, and will enable ground-breaking research in several areas of astronomy and particle physics~\cite{Bruyn:02}.

Another novelty is the elaborate use of software to process the telescope data in real time. Previous generations of telescopes depended on custom-made hardware to combine data, because of the high data rates and processing requirements. The availability of sufficiently powerful supercomputers however, allow the use of software to combine telescope data, creating a more flexible and reconfigurable instrument.

For processing LOFAR data, we use an IBM BlueGene/P (BG/P) supercomputer. The LOFAR antennas are grouped into stations, and each station sends its data (up to 200 Gbit/s for all stations) to the BG/P super computer. Inside the BG/P, the data are split and recombined using both real-time signal processing routines as well as two all-to-all exchanges. The output data streams are sufficiently reduced in size in order to be able to stream them out of the BG/P and store them on disks in our storage cluster.

The stations can be configured to observe in several directions in parallel, but have to divide their output bandwidth among them. In this paper, we present the \emph{pulsar pipeline}, an extension to the LOFAR software which allows the telescope to be aimed in hundreds of directions simultaneously at LOFAR's full observational bandwidth, a feat which cannot be matched by any other telescope. The data streams corresponding to each observational direction, called \emph{beams}, are generated through (weighted) summations of the station inputs, which are demultiplexed using an all-to-all exchange, and routed to the storage cluster.

Using the pulsar pipeline, astronomers can observe known pulsars, as well as discover new ones, with unprecedented sensitivity.  % TODO: mention other uses / observation types

In this paper, we will show how a software solution and the use of massive parallellism allows us to achieve this feat. We provide an in-depth study on all performance aspects, real-time behaviour, and scaling characteristics. The paper is organised as follows.

\section{Related Work}

MWA.

LOFAR imaging pipeline \cite{Romein:10a}

\section{LOFAR}

\comment{
  Hardware:
    - stations and antennas
    - network

  Processing in general:
    - station processing
        - how aiming works
    - online processing
    - offline processing
}

\section{IBM BlueGene/P}

We use a 3-rack IBM BlueGene/P (BG/P) supercomputer for the real-time processing of station data. We will describe the key features of the BG/P, but more information can be found elsewhere~\cite{IBM:08}.

Our system contains 12,480 processor cores that provide 42.4 TFLOPS peak processing power. One chip contains four PowerPC~450 cores, running at a modest 850~Mhz clock speed to reduce power consumption and to increase package density. Each core has two floating-point units (FPUs) that provide support for operations on complex numbers.

A rack consists of 64 \emph{psets}, each of which is a group of 64 compute cores 

A rack consists of 64 \emph{psets}, which is a group of compute cores

\comment{
  BG/P explanation:
    - basic architecture and size of our installation
    - IO nodes and compute nodes
    - internal networks
    - external networks and storage nodes
}

\cite{IBM:08}

\section{Pulsar pipeline}

\comment{
  Pulsar pipeline:
     - 1st transpose
     - pre-beamforming signal processing 
     - beam forming / stokes
     - pre-transpose reordering
     - 2nd transpose
     - post-transpose reordering
     - send to storage
}

\section{Results}

\comment{
  Intro belooft:
    - performance
    - real-time (relevant in dit paper als performance al onderhanden genomen wordt?)
    - scalability

  Scalability:
    - scale on one rack in #beams

  Performance:
    - percent of peak performance of major components
    - network bw use in 2nd transpose
}

\cite{Hessels:09}

\section{Conclusions}

\comment{
  We have shown:
    - beam forming implementation to form 200+ beams
    - performance figures
    - results from deployed system
}

\bibliographystyle{plain}
\bibliography{lofar}

\end{document}
