#!/bin/bash -e
# THIS SHOULD BE DONE USING YOU OWN ACCOUNT

# TODO:
# Script to export the needed/minimal data from the xml stat file.
# If this can be automated we could automate the actual detailed statistics
# of a pipeline: Create pipeline reports.
# 

# Steps needed to checkout, install and run pipelines
# all work will done in local user space without a chance of disruption operations.

# *****************************************************************************
# 1. Prepare the directories and build for doing pipeline runs
# a. Checkout the current branch:
#    Username and password of the svn are: lofar-guest and lofar-guest
mkdir ~/source/7058 -p
cd ~/source/7058
# this might take some time:
svn export --ignore-externals  https://svn.astron.nl/LOFAR/branches/CEP-Pipeline-Task7058 LOFAR

# b. build the pipeline framework and needed executables:
# CMake collects information about the environment and creates a make file
mkdir ~/build/7058/gnu_debug -p
cd ~/build/7058/gnu_debug 
cmake -DBUILD_PACKAGES="Offline" -DBUILD_ASKAPsoft=OFF -DUSE_OPENMP=ON  ~/source/7058/LOFAR
make install -j4     				 # this might take some time

# create the directory where logfiles and other runtime files will be stored
mkdir ~/build/7058/gnu_debug/installed/var/run/pipeline -p

#  c. Create a 'local' copy of the config file and generator script
mkdir ~/parset
cp ~/source/7058/LOFAR/CEP/Pipeline/helper_scripts/selfcal.settings ~/parset
cp ~/source/7058/LOFAR/CEP/Pipeline/helper_scripts/create_selfcal_parset.py ~/parset

# *****************************************************************************
# 2. Prepare the files based on a msss_imaging_pipeline parset
#    This step must be repeated for each new observation/field to process
#  b. Schedule a imaging pipeline, write down the obsid. Wait till the parset is 
#     created on the headnode (LHN001) and kill the pipeline run
#  c. secure a selfcal pipeline based on the now generated parset
cp <imagingparset_path> ~/parset

# ******************************************************************
# 3. Prepare the enviroment. 
# this must be done each time you log in to the headnode

cd ~/build/7058/gnu_debug
use Lofar                   # always use the latest production software, if you want cutting edge: use LofIm
use Pythonlibs              # needed for pywcs
. lofarinit.sh              # sets the paths to use the current installation


# **********************************************************************
#  4. Create a parset for an individual run
#  These step must be taking for each new sample/run/experiment
#  a. Adapt the settings file to your wishes:
#     - Fill in unique run name
#     - Fill in your username (you might need to create this dir on the locus nodes 
#     - Create the locus list (you could copy the list in the original pipeline parset)
#       The list should by a python style list of strings       
#     - select the number of major cycles 
cd ~/parset
emacs -nw ./selfcal.settings   # or use vim or nano

#  b. Run the generator:
#     Use a unique name for each parset: The parset name is used internally in the pipeline
#     to generate the location of the logfiles et.al.
python ./create_selfcal_parset.py  ./selfcal.settings ./selfcal_imager_pipeline.parset ./Unique_name.parset

#  c. Run the pipeline
python installed/bin/selfcal_imager_pipeline.py  \
        selfcal_imager_pipeline.parset -c ~/build/7058/gnu_debug/installed/share/pipeline/pipeline.cfg -d
		
# ***********************************************************************
# 5 Results of the pipeline.

# a. The created images can be found in the by you specified output location.

# b. The 'performance' of the pipeline
#    The performace stats of the pipeline are created and collected 'on the fly'. They are 
#    stored in an xml file without a lot of structure or documentation.
#    A python script has been created that collects  the information.
#    Calculates some statistics and presents them in a plot.

cd ~/source/7058/LOFAR/CEP/Pipeline/helper_scripts

# now run the visualization program on the statistics file as created by the pipeline
# The statistics file is produced next the the logfile:
# ~/build/7058/gnu_debug/installed/var/run/<Parsetname>/logs/<timestamp>/statistics.xml
# The program might run for a long time: It takes about 30 seconds to paint a figure on a very limited
# dataset.

python aggregate_stats.py <statistics.xml>











