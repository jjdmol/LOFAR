-----------------------------------
Cobalt in DFDs
-----------------------------------

The data flow through Cobalt is described in DFDs in

  cobalt-data-flow.eap (Enterprise Architect Project file)

Please update the above diagram along with this document. The data through Cobalt flows though Queues between processes. Both are briefly described before the full pipeline is described in more detail.

---------
Processes
---------

Each process receives data from an external source and/or a queue, processed it, and puts the result into a different queue. A process runs in its own thread, typically started using OpenMP. As a result, we have a massively concurrent processing pipeline. Note that each thread must run in real time, that is, the collective processing for one step should take less than the length of the data in time (typically, 1s blocks are used).

---------
Queues
---------

* a Queue: a thread-safe FIFO deque.
* or a Pool: a pair of two Queues: .filled and .free, to allow an exchange and reuse of preallocated objects. The producer pops data holders from .free and writes them to .filled, and the consumer pops elements from .filled and, when done, puts them back in .free. Note that this mechanism also puts a hard limit on the size of both Queues, since no more than the preallocated number of objects is ever present in either Queue.

The majority of the processing pipeline acts by forwarding 'blocks': a set of data spanning a fixed length of samples, typically ~1s long.

-----------------------------------
Station Input Pipeline
-----------------------------------

This is where it all starts: RSP packets are received for each station in the form of UDP packets:

* StationInput::readRSPRealTime (4)

	Reads the RSP packets in bulk from an UDP socket using the recvmmsg() system call. Each station emits its data using 4 RSP boards, and one readRSPRealTime thread is started for each one of them, all acting on the same rspDataPool. Received RSP packets are checked for validity, and the packetError() flag is set for each invalid packet.

	Transports:
		* rspDataPool.free -> rspDataPool.filled

	Stop conditions:
		* input queue gives NULL
		* thread receives SIGHUP

* StationInput::writeRSPRealTime (1)

	Holds two blocks ("current" and "next") and reads RSP packets from the rspDataPool. Each RSP packet is written to "current" and/or "next", depending on their timestamp and coarse delay compensation.

	Transports:
		* rspDataPool.filled -> rspDataPool.free

	Stop conditions:
		* RSP input queue gives NULL (used in edge cases)
		* deadline for "current" passes (the block end time + maximum network latency)

The above threads allow blocks to be filled with RSP data. The following processes control the generation and forwarding of blocks to handle the data from a single station:

* StationMetaData::computeMetaData (1)

	The starting point of each block. A block is cleared and annotated with its sequence number as well as the appropriate delays from casacore. Each block represents the data of all subbands from one station, ordered by receiver. That is, the subbands that will be sent to node 0 are stored first, followed by the subbands that will be sent to node 1, etc.

	Transports:
		* metaDataPool.free -> metaDataPool.filled

	Stop conditions:
		* nrBlocks are generated

* StationInput::processInput (1)

	Calls StationInput::writeRSPRealTime to fill each block with RSP data.

	Transports:
		* metaDataPool.filled -> mpiQueue

	Stop condition:
		* input queue gives NULL

* MPISender::sendBlocks (1)

	Scatters subbands across the receiving nodes. This can be done efficiently in bulk, because all subbands destined for the same receiver are stored next to each other.

	Transports:
		* mpiQueue -> metaDataPool.free

	Stop condition:
		* input queue gives NULL


-----------------------------------
CPU Processing Pipeline
-----------------------------------

This diagram describes the data flow through Cobalt after the MPI exchange step. The following processes are involved:

* MPIReceiver::receiveInput (1)

	Receives a block of subbands from each station over MPI, effectively filling a datastructure with dimensions [station][subband][sample]. Both data and metadata (flags, delays) are received, and jointly stored in the receivePool.filled queue.

	Transports:
		* receivePool.free -> receivePool.filled

	Stop conditions:
		* nrBlocks are received

* Pipeline::transposeInput (1)

	Transposes the input data structure from
		[station][subband][sample]
	to
		#subband x [station][sample]
	to allow each subband to be processed independently by the GPU later on.

	Transports:
		* receivePool.filled -> receivePool.free
		* SubbandProc::inputPool.free -> SubbandProc::inputPool.filled

	Stop conditions:
		* input queue gives NULL

* Pipeline::preprocessSubbands (1 per SubbandProc)

	Replaces flagged samples with zeroes.

	Transports:
		* SubbandProc::inputPool.filled -> SubbandProc::processPool.filled

	Stop conditions:
		* input queue gives NULL

* Pipeline::processSubbands (1 per SubbandProc)

	Transforms station input to Cobalt output using GPU processing. The station-input objects are returned upstream, and output is sent downstream. Note that this process ties two processing chains together (input and output).

	Transports:
		* SubbandProc::processPool.filled -> SubbandProc::inputPool.free
		* SubbandProc::outputPool.free -> SubbandProc::outputPool.filled

	Stop conditions:
		* input queue gives NULL

* Pipeline::postprocessSubbands (1 per SubbandProc)

	Applies results from metadata computation on the data coming from the GPU. For example, in the correlator, weights based on the flags are applied here, as is the post-GPU integration to obtain integration times bigger than the maximum block size that can be processed on the GPU. In case of post-GPU integration, some input blocks do inherently not produce output (!handOff), and thus are not forwarded to the next step but are sent back upstream instead.

	Transports:
		* SubbandProc::outputPool.filled -> SubbandProc::outputPool.free (!handOff)
		* SubbandProc::outputPool.filled -> Pipeline::writePool[subband] ( handOff)

	Stop conditions:
		* input queue gives NULL

* SubbandProc::writeOutput (1 per subband)

	Is responsible for emitting subbands over the network to outputProc. For the correlator, the subbands are mapped to files 1:1, so the subbands can be forwarded directly over the network. For the beam former, the different TABs are copied out of each subband and put into the MultiSender::queue.

	Transports:
		* Pipeline::writePool[subband] -> SubbandProc::outputPool.free
		* NEW -> MultiSender::queue (BeamFormer)

	Stop conditions:
		* input queue gives NULL

* MultiSender::process (1)

	Sends blocks representing a single subband of a single TAB to their respective receivers. The MultiSender makes sure that different TABs that end up in the same outputProc will be multiplexed over a single connection.

	Transports:
		* MultiSender::queue -> DELETE

	Stop conditions:
		* multiSender.kill() is called

-----------------------------------
OutputProc [Correlator]
-----------------------------------

The Correlator threads in outputProc use the following data flow:

* InputThread::process (1 per file)

	Reads blocks sent by GPUProc over Ethernet.

	Transports:
		* outputPool.free -> outputPool.filled

	Stop conditions:
		* TCP connection drops

* SubbandOutputThread::doWork (1 per file)

	Writes blocks to disk.

	Transports:
		* outputPool.filled -> outputPool.free

	Stop conditions:
		* input queue gives NULL

-----------------------------------
OutputProc [BeamFormer]
-----------------------------------

The BeamFormer threads in outputProc are more complicated, since they receive individual subbands for multiple TABs over a set of TCP connections. The received data needs to be woven together and redistributed to be written as separate files.

* MultiReceiver::receiveLoop (1 per connection)

	Receives a stream of blocks over any number of TCP connections, each block representing a single subband from a single TAB.

	Transports:
		* NEW -> BlockCollector::inputQueue

	Stop conditions:
		* TCP connection drops

* BlockCollector::inputLoop (1 per file)

	Collects subbands from a single TAB, and emits them using BlockCollector::emit once either all subbands are received, or, in case of data loss, room needs to be made for newer blocks. The BlockCollector::emit weaves together the received subbands, and zeroes subbands that were lost.

	Transports:
		* inputQueue -> outputQueue

	Stop conditions:
		* input queue gives NULL

* BlockCollector::outputLoop (1 per file)

	Forwards emitted blocks to the writer

	Transports:
		* outputQueue -> DELETE
		* outputPool.free -> outputPool.filled

	Stop conditions:
		* input queue gives NULL

* TABOutputThread::doWork (1 per file)

	Writes blocks to disk.

	Transports:
		* outputPool.filled -> outputPool.free

	Stop conditions:
		* input queue gives NULL
