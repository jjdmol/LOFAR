==================
LOFAR Grid Meeting
==================
----------------
15 November 2010
----------------

Introduction to LOFAR Pipeline Software
---------------------------------------

- What is a pipeline?

  - Shepherds a given dataset through an analysis process.
  - Pipelines are not necessarily well defined. For instance, we're still
    working on figuring out exactly what the "standard imaging pipeline"
    should do -- when should it flag? How much should the data be compressed?,
    and so on. Therefore, a pipeline is defined by stringing together a series
    of standard processing blocks (or "recipes"), togther with some
    configuration information.

- The LOFAR framework

  - Written in pure Python.
  - Structured as a series of "recipes", each one of which performs a given
    processing step. For example, one recipe might run NDPPP (flagging &
    compression of the data), another might run BBS (calibration).
  - Recipes can call other recipes as part of their processing run.
  - The pipeline itself is just defined as another recipe, and calls other
    recipes as required.

- Recipes

  - Must be able to handle running arbitrary tasks.
  - For example, some recipes wrap external code (like NDPPP), which has no
    programmatic interface: the recipe can simply set up & spawn the process,
    then wait to collect its return code.
  - Other recipes can perform processing in pure Python: for example, they
    might manipulate a dataset using pyrap.
  - This means there is no "standard" way a recipe must be structured; this is
    up to the programmer defining it.
  - The framework provides many helpers and shortcuts for common functionality
    required in defining a recipe, though.

- Parallelisation

  - In general, the pipeline tasks are embarassingly parallel: we are
    processing hundreds of independent subbands at the same time. Therefore,
    the pipeline simply distributes independent jobs to the compute nodes and
    collects the results.
  - Previous versions of the framework used IPython to do this. This is still
    available, but its use is not now encouraged.
  - The pipeline provides its own simple job dispatch system:

    - A recipe on the head node is associated with a "node script", again in
      pure Python, which runs on the compute nodes. The node script may take a
      number of arguments (usually, for eg, the name of the data file to
      processes).
    - The recipe can use SSH or mpirun to start the node scripts in parallel
      across all compute nodes, handing each one the appropriate parameters.
    - Adding a different dispatch mechanism is easy, providing it supports
      similar semantics (ie, start a command and wait for it to finish).
    - There are mechanisms to schedule somewhat intelligently, eg limiting the
      number of simultaneous jobs which are started per node.
    - After scheduling a bunch of jobs on the compute nodes, the pipeline will
      then wait for them to complete. Success is signalled via exit status of
      the job.

  - Cluster layout

    - The layout of the LOFAR cluster is described by means of a "clusterdesc"
      file.
    - The pipeline can read this clusterdesc file to obtain the name of the
      cluster head node and the compute nodes which are available for it to send
      jobs to.

  - "Mapping" of data

    - Not all data on the LOFAR storage nodes can be accessed by each compute
      node. Instead, the compute nodes in a given subcluster can only access
      the storage nodes in that cluster.
    - In general, the imaging pipeline is started by simply passing it a list
      of paths to the data to be processed.
    - It is possible to work out from the path to a storage node which
      subcluster it is a part of, and hence which compute nodes can access the
      data.
    - Processing is scheduled on compute nodes which can access the data in a
      round-robin fashion.
    - This understanding of paths is (obviously) fragile, but the LOFAR system
      provides little other useful metadata to work from. 
    - The "mapper" task which decides which compute node should process each
      input dataset is easily extendable/replacable to meet other
      requirements.

- Logging

  - Logging is performed using the standard Python logging module.
  - Output is configurable as part of the pipeline configuration file. In
    general, logging is set to a disk file and stdout on the head node.
  - As might be expected, you can specify the verbosity etc.
  - The compute nodes log to the head node using TCP connections (ie, using
    Python's logging.SocketHandler). The head node then adds any data they
    send to its own logging stream.
  - There is a built-in mechanism to scan the logging stream for arbitrary
    regular expressions, and thereby flag potential problems etc.
  - There is (currently) no other channel by which the compute nodes send
    feedback (beyond their exit status) to the head node (but I hope to
    implement a simple mechanism based on passing pickled Python objects over
    a TCP stream shortly).

- Results

  - Broadly speaking, the imaging pipeline picks data up from the storage
    nodes and processes it, writing results onto scratch disks on the compute
    nodes.
  - Intermediate data products are kept on the compute nodes -- for example,
    output from NDPPP will be processed by BBS and the imager on the same
    node, thus minimising data transport.
  - The resulting images will usually be copied to a shared storage location
    for collection/archiving.
  - If required, a copying step for intermediate data products could also be
    added.

- Dependencies

  - The system makes extensive use of the Python standard library
  - The only non-core Python component required for basic operation is the
    lofar.parameterset Python module, available from LOFARSOFT.
  - Obviously, individual recipes may have more demanding requirements, either
    because they need to call external programs or they require libraries like
    pyrap for their own processing.

- Documentation and further information

  - Available from USG Subversion:
    http://usg.lofar.org/svn/code/trunk/src/pipeline/.
  - Uses a standard, distutils-based installation.
  - Documentation (in Sphinx format, and still a work in progress) is also
    available from that location.
  - An online snapshot of the documentation is at
    http://urchin.earth.li/~jds/pipeline/.
