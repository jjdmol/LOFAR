# LOFAR IMAGING PIPELINE
# imager_bbs: start isolated master/node new_bss runs on compute nodes
# For seperate processing of timeslices
#                                                         LOFAR IMAGING PIPELINE
#
#                                                BBS (BlackBoard Selfcal) recipe
#                                                         Wouter Klijn
#                                                         klijn@astron.nl
# ------------------------------------------------------------------------------
from __future__ import with_statement
import sys
import collections
import os

from lofarpipe.support.remotecommand import RemoteCommandRecipeMixIn
from lofarpipe.support.baserecipe import BaseRecipe
from lofarpipe.support.group_data import load_data_map, store_data_map, validate_data_maps
import lofarpipe.support.lofaringredient as ingredient
from lofarpipe.support.remotecommand import ComputeJob

class imager_bbs(BaseRecipe, RemoteCommandRecipeMixIn):
    """

    **Arguments**

    A mapfile describing the data to be processed.
    """
    inputs = {
        'initscript': ingredient.FileField(
            '--initscript',
            help = "Initscript to source (ie, lofarinit.sh)"
        ),
        'parset': ingredient.FileField(
            '-p', '--parset',
            help = "BBS configuration parset"
        ),
        'bbs_executable': ingredient.StringField(
            '--bbs-executable',
            help = "BBS standalone executable (bbs-reducer)"
        ),
        'instrument_mapfile': ingredient.FileField(
            '--instrument-mapfile',
            help = "Full path to the mapfile containing the names of the "
                 "instrument model files generated by the `parmdb` recipe"
        ),
        'sky_mapfile': ingredient.FileField(
            '--sky-mapfile',
            help = "Full path to the mapfile containing the names of the "
                 "sky model files generated by the `sourcedb` recipe"
        ),
        'id': ingredient.IntField(
            '--id',
            default = 0,
            help = "Optional integer id for distinguishing multiple runs"
        ),
        'mapfile': ingredient.StringField(
            '--mapfile',
            help = "Full path to the file containing the output data products"
        ),
    }

    outputs = {
        'mapfile': ingredient.FileField(
            help = "Full path to a mapfile describing the processed data"
        )
    }


    def go(self):
        self.logger.info("Starting imager_bbs run")
        super(imager_bbs, self).go()

        # Load the data
        ms_map = load_data_map(self.inputs['args'][0])
        parmdb_map = load_data_map(self.inputs['instrument_mapfile'])
        sky_map = load_data_map(self.inputs['sky_mapfile'])

        #Check if the input has equal length and on the same nodes
        if not validate_data_maps(ms_map, parmdb_map, sky_map):
            self.logger.error("The combination of mapfiles failed validation:")
            self.logger.error(ms_map)
            self.logger.error(parmdb_map)
            self.logger.error(sky_map)
            return 1

        # Create the jobs
        jobs = []
        outnames = collections.defaultdict(list)
        node_command = " python %s" % (self.__file__.replace("master", "nodes"))
        for (ms, parmdb, sky) in zip(ms_map, parmdb_map, sky_map):
            #host is same for each entry (validate_data_maps)
            (host, ms_list) = ms
            (host, parmdb_list) = parmdb
            (host, sky_list) = sky

            #Write data maps to mapfiles
            map_dir = os.path.join(
                        self.config.get("layout", "job_directory"), "mapfiles")
            run_id = str(self.inputs.get("id"))
            ms_list_path = os.path.join(map_dir, host + "_ms_" + run_id + ".map")
            store_data_map(ms_list_path, [ms])
            parmdb_list_path = os.path.join(map_dir, host + "_parmdb_" + run_id + ".map")
            store_data_map(parmdb_list_path, [parmdb])
            sky_list_path = os.path.join(map_dir, host + "_sky_" + run_id + ".map")
            store_data_map(sky_list_path, [sky])

            outnames[host].extend(ms_list)

            arguments = [self.inputs['bbs_executable'],
                         self.inputs['parset'],
                         ms_list_path, parmdb_list_path, sky_list_path]
            self.logger.info(arguments)
            jobs.append(ComputeJob(host, node_command, arguments))

        # start and wait till all are finished
        self._schedule_jobs(jobs)

        if self.error.isSet():   #if one of the nodes failed
            self.logger.error("One of the nodes reporting while performing"
                              "a BBS run. Aborting")
            return 1

        # return the output: The measurement set that are calibrated:
        # calibrated data is placed in the ms sets
        store_data_map(self.inputs['mapfile'], ms_map)
        self.outputs['mapfile'] = self.inputs['mapfile']
        return 0


if __name__ == '__main__':
    sys.exit(imager_bbs().main())
