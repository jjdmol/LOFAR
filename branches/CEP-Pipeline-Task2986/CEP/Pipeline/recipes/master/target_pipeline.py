#                                                         LOFAR IMAGING PIPELINE
#
#                                                     Calibrator Pipeline recipe
#                                                             Marcel Loose, 2011
#                                                                loose@astron.nl
# ------------------------------------------------------------------------------

import os
import sys

from lofarpipe.support.control import control
from lofar.parameterset import parameterset
import lofarpipe.support.lofaringredient as ingredient

class target_pipeline(control):
    """
    The target pipeline can be used to calibrate the observation of a "target"
    source using an instrument database that was previously determined using
    the calibrator_pipeline.

    This pipeline will perform the following operations:
    - DPPP: flagging, using standard parset
    - Demix the relevant A-team sources (for now using python script, later
      to use DPPP), using the A-team sourcedb.
    - Run BBS to correct for instrumental effects using the instrument database
      from an earlier calibrator_pipeline run.
    """

    # These input arguments should ultimately be provided by the parameterset
    # file, once the scheduler will fill in the appropriate keys. For now,
    # let the user specify the path to the instrument mapfile.
    inputs = {
        'instrument_mapfile': ingredient.FileField(
            '--instrument-mapfile',
            help="Full path to the mapfile containing the names of the "
                 "instrument model files generated by the `parmdb` recipe"
        )
    }

    def __init__(self):
        control.__init__(self)
        self.parset = parameterset()


    def usage(self):
        print >> sys.stderr, "Usage: %s [options] <parset-file>" % sys.argv[0]
        return 1


    def go(self):
        """
        Read the parset-file that was given as input argument, and set the
        jobname before calling the base-class's `go()` method.
        """
        try:
            parset_file = self.inputs['args'][0]
        except IndexError:
            return self.usage()
        self.parset.adoptFile(parset_file)
        # Set job-name to basename of parset-file w/o extension, if it's not
        # set on the command-line with '-j' or '--job-name'
        if not self.inputs.has_key('job_name'):
            self.inputs['job_name'] = (
                os.path.splitext(os.path.basename(parset_file))[0]
            )
        super(target_pipeline, self).go()


    def pipeline_logic(self):
        """
        Define the individual tasks that comprise the current pipeline.
        This method will be invoked by the base-class's `go()` method.
        """

        # Create a parameter-subset containing only python-control stuff.
        py_parset = self.parset.makeSubset(
            'ObsSW.Observation.ObservationControl.PythonControl.')

        # Generate a datamap-file, which is a parset-file containing
        # key/value pairs of hostname and list of MS-files.
        data_mapfile = self.run_task(
            "cep2_datamapper",
            observation_dir=py_parset.getString('observationDirectory')
#            parset=self.inputs['args'][0]
        )['mapfile']

        # Create a sourcedb based on sourcedb's input argument "skymodel"
        # (see, e.g., tasks.cfg file).
        sourcedb_mapfile = self.run_task("sourcedb", data_mapfile)['mapfile']

        # Produce a GVDS file describing the data on the compute nodes.
        gvds_file = self.run_task("vdsmaker", data_mapfile)['gvds']

        # Read metadata (e.g., start- and end-time) from the GVDS file.
        vdsinfo = self.run_task("vdsreader", gvds=gvds_file)

        # Create a parameter-subset for DPPP and write it to file.
        ndppp_parset = os.path.join(
            self.config.get("layout", "job_directory"),
            "parsets", "NDPPP[0].parset")
        py_parset.makeSubset('DPPP[0].').writeFile(ndppp_parset)

        # Run the Default Pre-Processing Pipeline (DPPP);
        dppp_mapfile = self.run_task(
            "ndppp", data_mapfile,
            data_start_time=vdsinfo['start_time'],
            data_end_time=vdsinfo['end_time'],
            parset=ndppp_parset
        )['mapfile']

        # Demix the relevant A-team sources
        demix_mapfile = self.run_task("demixing", dppp_mapfile)['mapfile']

        # Create a parameter-subset for BBS and write it to file.
        bbs_parset = os.path.join(
            self.config.get("layout", "job_directory"),
            "parsets", "BBS.parset")
        py_parset.makeSubset('BBS.').writeFile(bbs_parset)

        # Run BBS to calibrate the target source(s).
        bbs_mapfile = self.run_task(
            "new_bbs", demix_mapfile,
            parset=bbs_parset,
            instrument_mapfile=self.inputs['instrument_mapfile'],
            sky_mapfile=sourcedb_mapfile
        )['mapfile']

        # Create another parameter-subset for a second DPPP run.
        ndppp_parset = os.path.join(
            self.config.get("layout", "job_directory"),
            "parsets", "NDPPP[1].parset")
        py_parset.makeSubset('DPPP[1].').writeFile(ndppp_parset)

        # Do a second run of DPPP, just to remove NaN's from the MS
        self.run_task("ndppp", bbs_mapfile,
            clobber=False,
            suffix='',
            parset=ndppp_parset
        )

if __name__ == '__main__':
    sys.exit(target_pipeline().main())
