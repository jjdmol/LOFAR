[DEFAULT]
lofarroot = @CMAKE_INSTALL_PREFIX@
casaroot = @CASACORE_ROOT_DIR@
pyraproot = @PYRAP_ROOT_DIR@
hdf5root = $ENV{HDF5_ROOT}
wcsroot = @WCSLIB_ROOT_DIR@
pythonpath = @PYTHON_INSTALL_DIR@

# runtime_directory is a global FS (nfs, lustre) to exchange small files (parsets, vds, map files, etc)
runtime_directory = %(lofarroot)s/var/run/pipeline

# working_directory is the local dir in which temporary dataproducts reside
working_directory = /data/scratch/$ENV{USER}

recipe_directories = [%(pythonpath)s/lofarpipe/recipes]
task_files = [%(lofarroot)s/share/pipeline/tasks.cfg]

[layout]
job_directory = %(runtime_directory)s/%(job_name)s

[cluster]
clusterdesc = %(lofarroot)s/share/cep2.clusterdesc

[deploy]
engine_ppath = %(pythonpath)s:%(pyraproot)s/lib:/opt/cep/pythonlibs/lib/python/site-packages
engine_lpath = %(lofarroot)s/lib:%(casaroot)s/lib:%(pyraproot)s/lib:%(hdf5root)s/lib:%(wcsroot)s/lib

[logging]
log_file = %(runtime_directory)s/%(job_name)s/logs/%(start_time)s/pipeline.log
xml_stat_file = %(runtime_directory)s/%(job_name)s/logs/%(start_time)s/statistics.xml

[feedback]
# Method of providing feedback to LOFAR.
# Valid options:
#    messagebus    Send feedback and status using LCS/MessageBus
#    none          Do NOT send feedback and status
method = @PIPELINE_FEEDBACK_METHOD@

[docker]
# Docker image to load for worker nodes
image = lofar

[remote]
# Connection methods:
#   paramiko			a Python SSH library
#   mpirun                      start using MPI (mpiexec)
#   local                       run on the head node directly
#   ssh                         SSH to remote host
#   custom_cmdline              fill in "cmdline", and run it (see below)
#method = custom_cmdline

# -------------------------------------------
# A custom commandline for SLURM + Docker
# -------------------------------------------
#
# We take the following path to start a remote container:
#
#  [container] --SSH--> [host] --SRUN--> [worker node] --DOCKER--> [container]
#
# This path is needed because running SRUN from within the container needs a lot of cluster-specific infra
# (SLURM config files, Munge keys, correct Munge user ID, munged).
#
# Throughout this path, we maintain:
#   * userid, which is set to the user that started the master script container (-e LUSER=`id -u`)
#   * environment (sudo -p, docker -e K=V, etc)
#   * pseudo-tty to prevent buffering logs (ssh -tt, docker -t)
#
# host -> worker node:       srun -w {host} -N 1 -n 1 --jobid={slurm_job_id}
#                            Needs the specific host, because the test system does not have a global file system.
#
# worker node -> container:  docker run -t --rm -e LUSER={uid} -w /home/lofar -v /home/mol/.ssh:/home/lofar/.ssh:ro -v /globalhome/mol/regression_test:/globalhome/mol/regression_test -v /shared:/shared --net=host {docker_image}
#                            Starts the container on the worker node, with pretty much the same parameters as the master container:
#
#                            --rm: don't linger resources when the container exits
#                            -e LUSER=(uid): set the user to run as (the calling user)
#                            -h {host}: set container hostname (for easier debugging) (doesnt work yet, using --net=host instead)
#                            -v:   map the directories for input/output data, shared storage (parsets, vds files, etc)
#
#                            /bin/bash -c
#
#                            Required because the pipeline framework needs some bash functionality in the commands it starts.
cmdline = ssh -n -tt -x localhost srun -w {host} -N 1 -n 1 --jobid={slurm_job_id} docker run -t --rm -e LUSER={uid} -v %(runtime_directory)s:%(runtime_directory)s -v %(working_directory)s:%(working_directory)s --net=host {docker_image} /bin/bash -c "\"{command}\""
