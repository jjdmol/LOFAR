#                                                         LOFAR IMAGING PIPELINE
#
#                                 BBS reducer (BlackBoard Selfcal) master recipe
#                                                             Marcel Loose, 2012
#                                                                loose@astron.nl
# ------------------------------------------------------------------------------

import sys
import lofarpipe.support.lofaringredient as ingredient

from lofarpipe.support.baserecipe import BaseRecipe
from lofarpipe.support.data_map import DataMap, validate_data_maps
from lofarpipe.support.remotecommand import ComputeJob
from lofarpipe.support.remotecommand import RemoteCommandRecipeMixIn

class bbs_reducer(BaseRecipe, RemoteCommandRecipeMixIn):
    """
    Run bbs-reducer in a non-distributed way on a number of MeasurementSets.
    
    **Arguments**
    
    A mapfile describing the data to be processed.
    """
    inputs = {
        'parset': ingredient.FileField(
            '-p', '--parset',
            help="BBS configuration parset"
        ),
        'nthreads': ingredient.IntField(
            '--nthreads',
            default=8,
            help="Number of threads per process"
        ),
        'executable': ingredient.ExecField(
            '--executable',
            help="The full path to the BBS-reducer executable"
        ),
        'instrument_mapfile': ingredient.FileField(
            '--instrument-mapfile',
            help="Full path to the mapfile containing the names of the "
                 "instrument model files generated by the `parmdb` recipe"
        ),
        'sky_mapfile': ingredient.FileField(
            '--sky-mapfile',
            help="Full path to the mapfile containing the names of the "
                 "sky model files generated by the `sourcedb` recipe"
        ),
        'data_mapfile': ingredient.StringField(
            '--data-mapfile',
            help="Full path to the mapfile that will contain the names of the "
                 "data files that were processed by BBS"
        ),
    }
    outputs = {
        'data_mapfile': ingredient.FileField(
            help="Full path to a mapfile describing the processed data"
        ),
        'instrument_mapfile': ingredient.FileField(
            help="Full path to the (updated) mapfile containing the names of "
                 "the instrument model files that were processed by BBS"
        )
    }
    

    def __init__(self):
        """
        Initialize our data members.
        """
        super(bbs_reducer, self).__init__()
        self.bbs_map = list()
        self.jobs = list()
        self.data_map = DataMap()
        self.inst_map = DataMap()
        self.sky_map = DataMap()


    def _load_mapfiles(self):
        """
        Load data map file, instrument map file, and sky map file.
        Update the 'skip' fields in these map files: if 'skip' is True in any
        of the maps, then 'skip' must be set to True in all maps.
        """
        self.logger.debug("Loading map files:"
            "\n\tdata map: %s\n\tinstrument map: %s\n\tsky map: %s" % (
                self.inputs['args'][0], 
                self.inputs['instrument_mapfile'],
                self.inputs['sky_mapfile']
            )
        )
        self.data_map = DataMap.load(self.inputs['args'][0])
        self.inst_map = DataMap.load(self.inputs['instrument_mapfile'])
        self.sky_map = DataMap.load(self.inputs['sky_mapfile'])

        if not validate_data_maps(self.data_map, self.inst_map, self.sky_map):
            self.logger.error("Validation of input data mapfiles failed")
            return False

        # Update the skip fields of the three maps. If 'skip' is True in any of
        # these maps, then 'skip' must be set to True in all maps.
        for x, y, z in zip(self.data_map, self.inst_map, self.sky_map):
            x.skip = y.skip = z.skip = (x.skip or y.skip or z.skip)
        
        return True
    

    def _run_jobs(self):
        """
        Create and schedule the compute jobs
        """
        command = "python %s" % (self.__file__.replace('master', 'nodes'))
        self.data_map.iterator = DataMap.SkipIterator
        self.inst_map.iterator = DataMap.SkipIterator
        self.sky_map.iterator = DataMap.SkipIterator
        for data, inst, sky in zip(self.data_map, self.inst_map, self.sky_map):
            self.jobs.append(
                ComputeJob(
                    data.host, command, 
                    arguments=[
                        (data.file, inst.file, sky.file),
                        self.inputs['executable'],
                        self.inputs['parset'],
                        self.environment
                    ],
                    resources={
                        "cores": self.inputs['nthreads']
                    }
                )
            )
        self._schedule_jobs(self.jobs)


    def _update_mapfiles(self):
        """
        Update the data- and instrument- map files, taking into account any
        failed runs.
        """
        self.logger.debug("Updating map files:"
            "\n\tdata map: %s\n\tinstrument map: %s" % 
            (self.inputs['args'][0], self.inputs['instrument_mapfile'])
        )
        for job, data, inst in zip(self.jobs, self.data_map, self.inst_map):
            if job.results['returncode'] != 0:
                data.skip = inst.skip = True
        self.data_map.save(self.inputs['data_mapfile'])
        self.inst_map.save(self.inputs['instrument_mapfile'])
        self.outputs['data_mapfile'] = self.inputs['args'][0]
        self.outputs['instrument_mapfile'] = self.inputs['instrument_mapfile']


    def _handle_errors(self):
        """
        Handle errors from the node scripts. If all jobs returned a (fatal)
        error, then the recipe should abort; return 1.
        Otherwise it should report that some jobs failed and continue with the
        remaining, successfully processed Measurement Set files; return 0.
        """
        if self.error.isSet():
            # Abort if all jobs failed
            if all(job.results['returncode'] != 0 for job in self.jobs):
                self.logger.error("All BBS-reducer jobs failed. Bailing out!")
                return 1
            else:
                self.logger.warn(
                    "Some BBS-reducer jobs failed, "
                    "continuing with succeeded runs"
            )
        return 0


    def go(self):
        """
        This it the actual workhorse. It is called by the framework. We pass
        three arguments to the node script: a tuple of file names (MS-file,
        parmdb-file, sourcedb-file), the path to the BBS-reducer executable,
        and the environment variables that are stored in self.environment.
        """
        self.logger.info("Starting BBS-reducer run")
        super(bbs_reducer, self).go()

        # Load the required map-files.
        if not self._load_mapfiles():
            return 1

        # Create and schedule the compute jobs
        self._run_jobs()

        # Update the instrument map file, taking failed runs into account.
        self._update_mapfiles()

        # Handle errors, if any.
        return self._handle_errors()


if __name__ == '__main__':
    sys.exit(bbs_reducer().main())

