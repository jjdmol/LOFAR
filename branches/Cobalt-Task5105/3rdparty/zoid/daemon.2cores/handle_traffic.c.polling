/****************************************************************************/
/* ZEPTOOS:zepto-info */
/*     This file is part of ZeptoOS: The Small Linux for Big Computers.
 *     See www.mcs.anl.gov/zeptoos for more information.
 */
/* ZEPTOOS:zepto-info */
/* */
/* ZEPTOOS:zepto-fillin */
/*     $Id: handle_traffic.c,v 1.20 2007/06/27 15:10:45 iskra Exp $
 *     ZeptoOS_Version: 1.2
 *     ZeptoOS_Heredity: FOSS_ORIG
 *     ZeptoOS_License: GPL
 */
/* ZEPTOOS:zepto-fillin */
/* */
/* ZEPTOOS:zepto-gpl */
/*      Copyright: Argonne National Laboratory, Department of Energy,
 *                 and UChicago Argonne, LLC.  2004, 2005, 2006, 2007
 *      ZeptoOS License: GPL
 * 
 *      This software is free.  See the file ZeptoOS/misc/license.GPL
 *      for complete details on your rights to copy, modify, and use this
 *      software.
 */
/* ZEPTOOS:zepto-gpl */
/****************************************************************************/

#define _GNU_SOURCE /* For pthread_yield.  */
#include <assert.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include <errno.h>
#include <fcntl.h>
#include <pthread.h>
#include <signal.h>
#include <sys/time.h>
#include <unistd.h>

#include <bglmemmap.h>

#include "bgl.h"
#include "zoid.h"
#include "zoid_protocol.h"
#include "zoid_api.h"

static BGLQuad* recv_data_cb(void* priv, BGLQuad* softheader);
static void queue_message(struct zoid_buffer* buffer);
static char* handle_ciod_message(struct CioHeader* cioheader, char* data,
				 int* result_len, int pset_cpu_rank);
static void high_priority_send(struct zoid_buffer *buffer);
static void send_packet(void);

zoid_buf_pipe *recv_queue;
lock_pair recv_queue_locks;

zoid_buf_pipe *send_queue;
lock_pair send_queue_locks;

zoid_buf_pipe *high_priority_send_queue;
lock_pair high_priority_send_queue_locks;

zoid_buf_pipe *ack_queue;
lock_pair ack_queue_locks;

extern sigset_t sigusr1_set;

/* Indicates whether any more messages that need an ACK should wait.  */
static int currently_receiving_large_message = 0;

#define NYI() assert(0)

/*
 * If there is no large message currently being received, schedules the
 * given message to be received (by sending an ACK to the source).
 */
static void schedule_ack(struct zoid_buffer* buffer, 
			struct ZoidSoftHeader* softheader) {
    char* ack_buff;
    struct zoid_buffer* ack_buffer;

    ack_buff = __zoid_allocate_buffer(TREE_PACKET_SIZE);
    if (!ack_buff)
	/* Out of memory.  MT-unsafe!  */
	ack_buff = packet_buffer->data;

    ack_buffer = get_zoid_buffer(ack_buff);
    ack_buffer->result_len = ack_buffer->total_len = 0;

    ack_buffer->softheader = *softheader;
    ack_buffer->softheader.flags = ZOID_SOFTHEADER_ACK_PACKET;

    enqueue_zoid_buf(ack_queue, ack_buffer);
}


#if 0
/*
 * CIOD-packet-specific receiving code.  Handles message acknowledgements.
 */
static void
receive_ciod_packet(struct CNProc* cnproc, struct CioHeader* cioheader)
{
#if 0
    int i, *pkt = (int*)cnproc->current_buf;
    fprintf(stderr, "Received CIOD packet: _cpu %d, "
	    "_rankInCnodes %d, _reserved %d, _dataSize %d, "
	    "_treeAddress %d, _messageCode %d, _packetTotal %d, "
	    "_packetIndex %d\n", cioheader->_cpu,
	    cioheader->_rankInCnodes, cioheader->_reserved,
	    cioheader->_dataSize, cioheader->_treeAddress,
	    cioheader->_messageCode, cioheader->_packetTotal,
	    cioheader->_packetIndex);
    for (i = 0; i < 240 / 4; i++)
	fprintf(stderr, "%08x%c", pkt[i], ((i + 1) % 8 && i != 240 / 4 - 1 ?
					   ' ' : '\n'));
#endif
    if (cioheader->_packetIndex == 0)
    {
	/* First packet.  */
	cnproc->msg_length = cioheader->_packetTotal * TREE_DATA_SIZE;

	if (cioheader->_packetTotal > 1)
	{
	    /* CIOD protocol is rendezvous-based.  For multi-packet
	       messages, first packet must be acknowledged before the
	       rest are sent.  Create this acknowledgement here.  */
	    char* buffer;
	    struct zoid_buffer* ack_buffer;
	    struct CioHeader ack_cioheader;

	    buffer = __zoid_allocate_buffer(TREE_PACKET_SIZE);
	    if (!buffer)
		/* Out of memory.  MT-unsafe!  */
		buffer = packet_buffer->data;

	    ack_buffer = get_zoid_buffer(buffer);
	    ack_buffer->result_len = ack_buffer->total_len = 0;

	    ack_cioheader = *cioheader;
	    ack_cioheader._messageCode = MTC_ACK;
	    ack_cioheader._packetTotal = 1;
	    memcpy(&ack_buffer->softheader, &ack_cioheader,
		   sizeof(ack_cioheader));

	    /* Let the regular reply sending code do the rest.  */
	    queue_message(ack_buffer);
	}
    }
}
#endif


static void receive_zoid_packet(struct CNProc *cnproc, 
				struct ZoidSoftHeader *softheader) {
#if 0
    int i, *pkt = (int*)cnproc->current_buf;
    fprintf(stderr, "Received ZOID packet from %d, flags %d, "
	    "len %d @ %p\n", softheader->pset_cpu_rank,
	    softheader->flags, softheader->msg_length, cnproc->buffer);
    for (i = 0; i < 240 / 4; i++) {
	fprintf(stderr, "%08x%c", pkt[i], ((i + 1) % 8 && i != 240 / 4 - 1 ?
					   ' ' : '\n'));
    }
#endif

    if(softheader->flags & ZOID_SOFTHEADER_FIRST_PACKET) {
	/* Check if this is really the first message.  */
	if(cnproc->current_buf != cnproc->buffer->data) {
	    fprintf(stderr, "First packet received from %d while "
		    "reading another message\n",
		    softheader->pset_cpu_rank);
	    /* Nothing sensible to do, so let's just try to
	       continue.  */
	}

	cnproc->msg_length = softheader->msg_length;
    } else if(cnproc->current_buf == cnproc->buffer->data &&
	     !cnproc->buffer->errnum) {
	/* Received an unexpected packet without FIRST set.  */
	fprintf(stderr, "Received unexpected packet from %d\n",
		softheader->pset_cpu_rank);
    }

    if(softheader->msg_length != cnproc->msg_length)
	fprintf(stderr, "Message length mismatch from %d, found %d, "
		"expecting %d\n", softheader->pset_cpu_rank,
		softheader->msg_length, cnproc->msg_length);

    if (((unsigned int)cnproc->current_buf & 0xf) ||
	(cnproc->buffer->userbuf_in &&
	 cnproc->current_buf + TREE_DATA_SIZE >
	 cnproc->buffer->userbuf_in + cnproc->buffer->userbuf_in_len)) {
	/* See recv_data_cb.  We used an auxiliary buffer.  Copy the data
	   where it belongs.  */
	int len;
	len = cnproc->buffer->userbuf_in_len - (cnproc->current_buf -
						cnproc->buffer->userbuf_in);
	if (len > TREE_DATA_SIZE)
	    len = TREE_DATA_SIZE;
	memcpy(cnproc->current_buf, packet_buffer->data, len);
    }

    if ((softheader->flags & ZOID_SOFTHEADER_NEED_ACK_PACKET)) {
	cnproc->buffer->ack_sent = 1;
	schedule_ack(cnproc->buffer, softheader);
    }
}



static int
handle_received_packet(struct CNProc* cnproc,
		       struct ZoidSoftHeader* softheader)
{
    struct CioHeader* cioheader;

    cioheader = (softheader->zoid_id == ZOID_ID ?
		 NULL : (struct CioHeader*)softheader);

    /* If an error has been marked, we are going to drop the data anyway,
       and we probably don't have enough buffer space to hold it, so don't
       even try to place it neatly in the buffer.  */
    if (!cnproc->buffer->errnum)
	cnproc->current_buf += TREE_DATA_SIZE;

    /* Is this the last packet of a message? */
    if (cioheader ?
	cioheader->_packetIndex == cioheader->_packetTotal - 1 :
	softheader->flags & ZOID_SOFTHEADER_LAST_PACKET) {
	if (!cnproc->buffer->errnum) {
	    
	    /* Check if we have read all that we were supposed to.  */
	    if (cnproc->buffer->userbuf_in) {
		if (cnproc->current_buf - cnproc->buffer->userbuf_in <
		    cnproc->buffer->userbuf_in_len){
		    fprintf(stderr, "Received last packet from %d but read "
			    "only %d bytes of userbuf, expected %d\n",
			    softheader->pset_cpu_rank,
			    cnproc->current_buf - cnproc->buffer->userbuf_in,
			    cnproc->buffer->userbuf_in_len);
		}
	    } else {
		if (cnproc->current_buf - cnproc->buffer->data <
		    cnproc->msg_length) {
		    fprintf(stderr, "Received last packet from %d but read "
			    "only %d bytes, expected %d\n",
			    softheader->pset_cpu_rank,
			    cnproc->current_buf - cnproc->buffer->data,
			    cnproc->msg_length);
		}
	    }
	}

	if (!cioheader && (softheader->flags & ZOID_SOFTHEADER_ASSERT_PACKET)) {
	    fprintf(stderr, "Process %d detected an internal error in "
		    "communication code!\n",
		    cn_procs[softheader->pset_cpu_rank].pid);
	    fprintf(stderr, "%s\n", cnproc->buffer->data);

	    /* Core dump message will follow; clean up for it.  */
	    if (cnproc->buffer != packet_buffer)
		__zoid_release_buffer(cnproc->buffer->data);
	    cnproc->buffer = NULL;
	    cnproc->current_buf = NULL;

	    return 0;
	}

	if(!cioheader && *(int*)cnproc->buffer->data == ZOID_TERMINATING_ID) {
#if 0
	    fprintf(stderr, "Exit request from process %d\n",
		    cn_procs[softheader->pset_cpu_rank].pid);
#endif
	    if (cnproc->buffer != packet_buffer)
		__zoid_release_buffer(cnproc->buffer->data);
	    cnproc->buffer = NULL;
	    cnproc->current_buf = NULL;
	    
	    enter_critical_section(&pending_exit_locks);

	    cnproc->status = PROC_STATUS_EXIT;
	    pending_exit_requests--;

	    leave_critical_section(&pending_exit_locks);
	    
	    return 0;
	}

	/* A buffer is complete.  Queue it.  */

	memcpy(&cnproc->buffer->softheader, softheader, sizeof(*softheader));
	if (cnproc->buffer->ack_sent)
	    currently_receiving_large_message = 0;

	enqueue_zoid_buf(recv_queue, cnproc->buffer);

	cnproc->buffer = NULL;
	cnproc->current_buf = NULL;

	return 1;
    }

    if(softheader->flags & ZOID_SOFTHEADER_INPUT_USERBUF_PACKET) {
	/* We are done reading the first (non-userbuf) part of the input
	   userbuf message.  We will have to invoke the user allocation
	   callback now.  Behave as if the message is complete: we will be
	   able to recognize that it's not by the above flag in the
	   softheader.  */
	memcpy(&cnproc->buffer->softheader, softheader, sizeof(*softheader));

	enqueue_zoid_buf(recv_queue, cnproc->buffer);

	cnproc->buffer = NULL;
	cnproc->current_buf = NULL;

	return 1;
    }

    if (cnproc->buffer->userbuf_in ?
	cnproc->current_buf - cnproc->buffer->userbuf_in >=
	cnproc->buffer->userbuf_in_len :
	cnproc->current_buf - cnproc->buffer->data >= cnproc->msg_length) {
	/* We've read more than we were supposed to without receiving
	   the end-of-message packet!  */

	fprintf(stderr, "Message from %d longer than declared %d\n",
		softheader->pset_cpu_rank, cnproc->msg_length +
		cnproc->buffer->userbuf_in_len);

	/* Nothing better to do than to prevent a buffer overrun...  */
	cnproc->current_buf -= TREE_DATA_SIZE;
    }

    return 0;
}



static BGLQuad* recv_data_cb(void *priv, BGLQuad *softheader) {
    struct CNProc* cnproc = NULL;
    struct CNProc** cnproc_ptr = priv;
    struct ZoidSoftHeader* zsh=(struct ZoidSoftHeader*)softheader;
    int msglen;
    BGLQuad *ret, *dummy;

    if(zsh->zoid_id == ZOID_ID) {
	if (zsh->pset_cpu_rank < pset_size * (vn_mode ? 2 : 1) &&
	    pset_rank_mapping[zsh->pset_cpu_rank] != -1) {
	    zsh->pset_cpu_rank = pset_rank_mapping[zsh->pset_cpu_rank];
	    cnproc = &cn_procs[zsh->pset_cpu_rank];
	    *cnproc_ptr = cnproc;
	} else {
	    /* Invalid message.  */
	    NYI();
	    *cnproc_ptr = NULL;
	    return (BGLQuad*)packet_buffer->data;
	}

	msglen = zsh->msg_length;

	if(zsh->flags & ZOID_SOFTHEADER_ASSERT_PACKET) {
	    /* This is a special one-packet message.  If anything was being
	       read, forget about it and reset the buffer.  */
	    if(cnproc->buffer) {
		if(cnproc->buffer->ack_sent) {
		    currently_receiving_large_message = 0;
		    cnproc->buffer->ack_sent = 0;
		}
		cnproc->current_buf = cnproc->buffer->data;
	    }
	}
    } else {
	/* Assume it's a message from CNK (CIOD protocol).  */
	struct CioHeader* cioheader = (struct CioHeader*)softheader;
	int i;

	for (i = 0; i < pset_proc_count; i++) {
	    if (cn_procs[i].p2p_addr == cioheader->_treeAddress &&
		cn_procs[i].cpu == cioheader->_cpu) {
		cnproc = &cn_procs[i];
		*cnproc_ptr = cnproc;
		break;
	    }
	}

	if(i == pset_proc_count) {
	    if(cioheader->_messageCode == MFC_REQUESTRESET) {
		/* We can receive a REQUESTRESET message for an unknown
		   process, which is the second CPU of a node running in CO
		   node.  We assign the message to the primary CPU.  */
		for(i = 0; i < pset_proc_count; i++) {
		    if(cn_procs[i].p2p_addr == cioheader->_treeAddress) {
			cnproc = &cn_procs[i];
			*cnproc_ptr = cnproc;
			break;
		    }
		}
		if(i == pset_proc_count) {
		    /* Invalid message.  */
		    NYI();
		    *cnproc_ptr = NULL;
		    return (BGLQuad*)packet_buffer->data;
		}
	    } else {
		NYI();
		/* Invalid message.  */
		*cnproc_ptr = NULL;
		return (BGLQuad*)packet_buffer->data;
	    }
	}

	msglen = cioheader->_packetTotal * TREE_DATA_SIZE;
    }

    if (!cnproc->buffer) {
	/* First packet of a new message.  Initialize things.  */
	char* buffer = __zoid_allocate_buffer(msglen);
	if(buffer) {
	    cnproc->buffer = get_zoid_buffer(buffer);
	} else {
	    NYI();
	    /* Either the requested size is too large, or we are out of RAM.
	       In either case, we won't be able to handle the request.
	       We will read the command "whereever" (preferably into a
	       process-specific buffer, otherwise a global one (*not*
	       MT-safe), and flag an error so that the receive_thread knows
	       not to pass this to worker threads.  */
	    struct thread_specific_data* thread_data;
	    thread_data = (struct thread_specific_data*)
		pthread_getspecific(thread_specific_key);

	    if (thread_data->errnum == E2BIG)
		/* Command was too large.  */
		buffer = __zoid_allocate_buffer(TREE_PACKET_SIZE);

	    if (!buffer)
		/* Out of memory.  MT-unsafe!  */
		buffer = packet_buffer->data;

	    cnproc->buffer = get_zoid_buffer(buffer);

	    cnproc->buffer->errnum = thread_data->errnum;
	    if (thread_data->errnum == E2BIG)
		cnproc->buffer->excessive_size =
		    thread_data->last_excessive_size;
	}

	cnproc->current_buf = buffer;
    }

    if (((unsigned int)cnproc->current_buf & 0xf) ||
	(cnproc->buffer->userbuf_in &&
	 cnproc->current_buf + TREE_DATA_SIZE >
	 cnproc->buffer->userbuf_in + cnproc->buffer->userbuf_in_len)) {
	/* Either condition can only take place for input userbuf, because
	   zoid-allocated buffers are always properly aligned and rounded
	   up.  Use an auxiliary buffer; we will copy back from it in
	   receive_zoid_packet().  */
	ret = (BGLQuad*)packet_buffer->data;
    } else {
	ret = (BGLQuad*)cnproc->current_buf;
    }

    /* Allocate L1 cache lines that we are going to write entirely,
       so that they are not needlessly read from main memory */
    dummy = ret;
    asm volatile(
	"\taddi %0,%0,16\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0; addi %0,%0,32\n"
	"\tdcbz 0,%0\n"
	: "+b" (dummy)
    );

    return ret;
}


static int receive_packet(void) {
    struct CNProc* cnproc;
    BGLTreePacketHardHeader hardheader;
    char softheader_buf[sizeof(struct ZoidSoftHeader) + 0xf];
    struct ZoidSoftHeader *softheader = (struct ZoidSoftHeader*)
	(((unsigned int)softheader_buf + 0xf) & ~0xf);

    BGLTreeFIFO_recvF(vc0 + BGL_MEM_TREE_HDROUT_OFFSET,
		      vc0 + BGL_MEM_TREE_DATAOUT_OFFSET,
		      &hardheader, (BGLQuad*)softheader,
		      &recv_data_cb, &cnproc);

    if(!cnproc) {
	/* Invalid message.  */
	int hhdr;
	memcpy(&hhdr, &hardheader, sizeof(hardheader));
	fprintf(stderr, "Invalid packet read, hardheader %08x, "
		"softheader %08x %08x %08x %08x\n",
		hhdr, ((BGLQuad*)softheader)->w0,
		((BGLQuad*)softheader)->w1, ((BGLQuad*)softheader)->w2,
		((BGLQuad*)softheader)->w3);
	return 0;
    }

    if (softheader->zoid_id == ZOID_ID)
	receive_zoid_packet(cnproc, softheader);
    else
	NYI();

    return handle_received_packet(cnproc, softheader);
}




pthread_mutex_t poll_mutex = PTHREAD_MUTEX_INITIALIZER;
extern int *sent_signals, *recv_signals;
pid_t ppid;


void bglco_loop() {
    struct zoid_buffer *buffer;
    BGLTreeStatusRegister status;

    syscall(268);

    ppid = getppid();

    enter_critical_section(&tree_locks);

    while((volatile int)pending_exit_requests > 0) {
	status = *(volatile BGLTreeStatusRegister*)(vc0 + BGL_MEM_TREE_STATUS0_OFFSET);
	
	if(status.recpktcnt > 0)  {
	    if(receive_packet()) {
                //printf("Got a complete msg, wake a worker thread...\n");
		//(*sent_signals)++;
/* 		int r = sigqueue(ppid, SIGRTMIN+1, (union sigval)0); */
/* 		if(r) { */
/* 		    perror("sigqueue failed"); */
/* 		    exit(-1); */
/* 		} */
	    }
	}

	status = *(volatile BGLTreeStatusRegister*)(vc0 + BGL_MEM_TREE_STATUS0_OFFSET);

	if(status.injpktcnt < 8) {
	    if(!currently_receiving_large_message &&
	       ((volatile zoid_buf_pipe*)ack_queue)->first) {
		buffer = dequeue_zoid_buf(ack_queue);
		if(buffer) {
		    high_priority_send(buffer);
		    currently_receiving_large_message = 1;
		}
	    } else if(((volatile zoid_buf_pipe*)high_priority_send_queue)->first) {
		buffer = dequeue_zoid_buf(high_priority_send_queue);
		if(buffer)
		    high_priority_send(buffer);
	    } else if(((volatile zoid_buf_pipe*)send_queue)->first) {
		send_packet();
	    }
	}

	if (sent_kill_packet) {
	    printf("WE NEED TO STOP!!!!!!\n");
	    break;
	}
    } 

    leave_critical_section(&tree_locks);

    syscall(269);
}



/*
 * Sends a single packet to a compute node.
 * WARNING: this function is stateful.  It internally keeps track of
 * the progress of sending the first message from send_queue.
 * This function assumes that it won't be called simultaneously by multiple
 * threads.  Currently, the receive_queue_mutex in handle_messages() ensures
 * of that.
 */
static void send_packet(void) {
    static struct zoid_buffer* buffer = NULL;
    /* The remaining static variables are only valid if buffer != NULL.  */
    static BGLTreePacketHardHeader hardheader;
    static char* result_current;
    static int result_remaining;
    static struct CioHeader* cioheader;
    static int copying_userbuf;
    char* data_buf;

    if (!buffer) {

	enter_critical_section(send_queue->locks);
	buffer = ((volatile zoid_buf_pipe*)send_queue)->first;
	leave_critical_section(send_queue->locks);

	if (!buffer)
	    return;

	if (buffer->softheader.zoid_id == ZOID_ID) {
	    cioheader = NULL;
	    BGLTreePacketHardHeader_InitP2P(&hardheader, PACKET_CLASS_CIO,
					    0, cn_procs[buffer->softheader.
					    pset_cpu_rank].p2p_addr);
	    /* softheader->zoid_id is already filled in correctly.  */
	    buffer->softheader.pset_cpu_rank =
		pset_rank_mapping_rev[buffer->softheader.pset_cpu_rank];
	    buffer->softheader.flags = ZOID_SOFTHEADER_FIRST_PACKET;
	    buffer->softheader.msg_length = buffer->result_len;
	    buffer->softheader.errnum = buffer->errnum;
	} else {
	    /* CIOD message.  */
	    cioheader = (struct CioHeader*)&buffer->softheader;
	    BGLTreePacketHardHeader_InitP2P(&hardheader, PACKET_CLASS_CIO,
					    0, cioheader->_treeAddress);
	    cioheader->_packetIndex = 0;
	}

	result_current = buffer->data;
	result_remaining = (buffer->userbuf_out ? buffer->result_len :
			    buffer->total_len);
	copying_userbuf = 0;
    }

    /* Since we round up the buffers to full packet boundary when allocating,
       we never have to copy to intermediate ones...  */

    /* Is this the last packet?  */
    if (!cioheader && result_remaining <= TREE_DATA_SIZE &&
	(!buffer->userbuf_out || copying_userbuf)) {
	buffer->softheader.flags |= ZOID_SOFTHEADER_LAST_PACKET;
    }

#if 0
    {
	int i, *pkt = (int*)result_current;
	fprintf(stderr, "Sending back ZOID packet to %d, flags %d, len %d, @ %p\n",
		buffer->softheader.pset_cpu_rank, buffer->softheader.flags,
		buffer->softheader.msg_length, buffer);
	for (i = 0; i < 240 / 4; i++)
	    fprintf(stderr, "%08x%c", pkt[i], ((i + 1) % 8 &&
					       i != 240 / 4 - 1 ?
					       ' ' : '\n'));
    }
#endif

    if (((unsigned int)result_current & 0xf) ||
	(copying_userbuf && result_remaining < TREE_DATA_SIZE)) {
	data_buf = packet_buffer->data;
	memcpy(data_buf, result_current, result_remaining < TREE_DATA_SIZE ?
	       result_remaining : TREE_DATA_SIZE);
    } else {
	data_buf = result_current;
    }

    BGLTreeFIFO_sendH(vc0 + BGL_MEM_TREE_HDRIN_OFFSET,
		      vc0 + BGL_MEM_TREE_DATAIN_OFFSET,
		      &hardheader, (BGLQuad*)&buffer->softheader,
		      (BGLQuad*)data_buf);

    result_remaining -= TREE_DATA_SIZE;

    if (!cioheader)
	buffer->softheader.flags = 0;
    else
	cioheader->_packetIndex++;


    if (result_remaining <= 0) {
	if (buffer->userbuf_out && !copying_userbuf) {
	    /* If there is a userbuf output part, we end up here prematurely,
	       as soon as the zoid part of the buffer is sent.  Adjust the
	       pointer and length and send the user buffer next.  */

	    result_current = buffer->userbuf_out;
	    result_remaining += buffer->total_len - buffer->result_len;
	    copying_userbuf = 1;
	} else {
	    dequeue_zoid_buf(send_queue);

	    /* We are done with the current message.  Clean up.  */
	    if (buffer->userbuf_out) {
		enqueue_zoid_buf(recv_queue, buffer);
/* 		int r = sigqueue(ppid, SIGRTMIN+1, (union sigval)0); */
/* 		if(r) { */
/* 		    perror("sigqueue failed"); */
/* 		    exit(-1); */
/* 		} */
	    } else if (buffer != packet_buffer) {
		__zoid_release_buffer(buffer->data);
	    }
	    buffer = NULL;
	}
    } else {
	result_current += TREE_DATA_SIZE;
    }
}


/*
 * Send a single-packet message that bypasses the normal send queue.
 */
static void high_priority_send(struct zoid_buffer *buffer) {
    BGLTreePacketHardHeader hardheader;

    assert(buffer->softheader.zoid_id == ZOID_ID);
    assert(!buffer->userbuf_out);
    assert(buffer->total_len <= TREE_DATA_SIZE);

    BGLTreePacketHardHeader_InitP2P(&hardheader, PACKET_CLASS_CIO,
				    0, cn_procs[buffer->softheader.
				    pset_cpu_rank].p2p_addr);
    /* softheader->zoid_id is already filled in correctly.  */
    buffer->softheader.pset_cpu_rank =
	pset_rank_mapping_rev[buffer->softheader.pset_cpu_rank];
    buffer->softheader.flags = ZOID_SOFTHEADER_FIRST_PACKET |
	ZOID_SOFTHEADER_LAST_PACKET |
	(buffer->softheader.flags & ZOID_SOFTHEADER_ACK_PACKET);
    buffer->softheader.msg_length = buffer->result_len;
    buffer->softheader.errnum = buffer->errnum;

    BGLTreeFIFO_sendH(vc0 + BGL_MEM_TREE_HDRIN_OFFSET,
		      vc0 + BGL_MEM_TREE_DATAIN_OFFSET,
		      &hardheader, (BGLQuad*)&buffer->softheader,
		      (BGLQuad*) buffer->data);

    /* We are done with the current message.  Clean up.  */

    if (buffer != packet_buffer)
	__zoid_release_buffer(buffer->data);
    else 
	NYI();
}



/*
 * Enqueues a message in the send queue, to be sent back to a compute
 * node later (or in another thread), in the handle_messages() function.
 */
static void
queue_message(struct zoid_buffer* buffer)
{
#if 1
    if(buffer->softheader.zoid_id == ZOID_ID && !buffer->userbuf_out &&
       buffer->total_len <= TREE_DATA_SIZE)
	enqueue_zoid_buf(high_priority_send_queue, buffer);
    else
#endif
	enqueue_zoid_buf(send_queue, buffer);
}

/*
 * Body of (multiple) worker threads.  Takes a command from the head of the
 * receive queue, processes it, and puts the reply at the tail of the send
 * queue.
 */
void*
worker_thread_body(void* arg)
{
    struct thread_specific_data thread_data;

    /* Various callbacks from generated and user code can access this data.  */
    if (pthread_setspecific(thread_specific_key, &thread_data))
    {
	perror("setting thread-specific data");
	return NULL;
    }

    for (;;)
    {
	struct zoid_buffer *cmd_buffer, *res_buffer;
	char *result = NULL;
	int result_len, total_len;
	int release_cmd_buffer = 1;
	
 	int sig; 
	pthread_mutex_lock(&poll_mutex);
/*  	sigwait(&sigusr1_set, &sig); */
/* 	pthread_mutex_unlock(&poll_mutex); */


	int poll_cnt = 0;
	while(!((volatile zoid_buf_pipe*)recv_queue)->first) {
	    if((poll_cnt += 1 << 24) == 0)
		pthread_yield();
	}

	cmd_buffer = dequeue_zoid_buf(recv_queue);

 	//printf("Launched worker thread! %d %p\n", sig, cmd_buffer);

	pthread_mutex_unlock(&poll_mutex);
	
	if(cmd_buffer == (struct zoid_buffer*)1) {
	    printf("Stopping worker thread\n");
	    return NULL;
	}

	//assert(cmd_buffer);
	if(!cmd_buffer)
	    continue;

	thread_data.userbuf = NULL;

	if(cmd_buffer->userbuf_out) {
	    thread_data.calling_process_id =
		cmd_buffer->softheader.pset_cpu_rank;

	    cmd_buffer->userbuf_out_cb(cmd_buffer->userbuf_out,
				       cmd_buffer->userbuf_out_priv);
	    cmd_buffer->userbuf_out = NULL;

	    __zoid_release_buffer(cmd_buffer->data);
	    continue;
	}

	if (!cmd_buffer->errnum) {
	    int command_id, header_id, function_id;
	    struct zoid_dispatch_entry* entry;

	    if (cmd_buffer->softheader.zoid_id == ZOID_ID)
	    {
		thread_data.calling_process_id =
		    cmd_buffer->softheader.pset_cpu_rank;

		/* The buffer always begins with a command id.  */
		memcpy(&command_id, cmd_buffer->data, sizeof(command_id));

		header_id = command_id >> 16;
		function_id = command_id & 0xffff;

		for (entry = dispatch_entries; entry; entry = entry->next)
		    if (entry->header_id == header_id)
		    {
			if (function_id >= 0 &&
			    function_id < entry->array_size)
			{
			    if (cmd_buffer->softheader.flags &
				ZOID_SOFTHEADER_INPUT_USERBUF_PACKET)
			    {
				/* We need to invoke a special allocate
				   callback, not (yet) the argument unpacking
				   stub.  */
				int buffer_len;
				struct CNProc* cnproc = &cn_procs[cmd_buffer->
						     softheader.pset_cpu_rank];

				memcpy(&buffer_len, cmd_buffer->data +
				       sizeof(command_id), sizeof(buffer_len));

				cmd_buffer->userbuf_in =
				    entry->dispatch_array[function_id].
				    userbuf_allocate_cb(buffer_len);

				if (!cmd_buffer->userbuf_in)
				{
				    /* User failed to provide us with a buffer.
				       Instead of an ACK, we will return an
				       error message.  */
				    cmd_buffer->errnum = ENOMEM;
				    break;
				}

				cmd_buffer->userbuf_in_len = buffer_len;
				cnproc->buffer = cmd_buffer;
				cnproc->current_buf = cmd_buffer->userbuf_in;

				cmd_buffer->ack_sent = 1;

				flush_zoid_buf(cmd_buffer);

				schedule_ack(cmd_buffer, 
					     &cmd_buffer->softheader);

				/* We are not done with this command yet,
				   so don't release the buffer!  */
				release_cmd_buffer = 0;
				
				break;
			    }

			    if (cmd_buffer->userbuf_in)
			    {
				char* (*function_ptr)(char* command_buffer,
						      int* result_len,
						      int* total_len,
						      char* user_buffer) =
				    entry->dispatch_array[function_id].
					function_ptr;

				result = function_ptr(((char*)cmd_buffer->data)
						      + sizeof(command_id),
						      &result_len, &total_len,
						      cmd_buffer->userbuf_in);
			    }
			    else
			    {
				char* (*function_ptr)(char* command_buffer,
						      int* result_len,
						      int* total_len) =
				    entry->dispatch_array[function_id].
					function_ptr;

				result = function_ptr(((char*)cmd_buffer->data)
						      + sizeof(command_id),
						      &result_len, &total_len);
			    }

			    if (!result)
			    {
				/* An error here is most likely an
				   out-of-memory situation or a too large
				   reply buffer.  */
				cmd_buffer->errnum = thread_data.errnum;
				if (cmd_buffer->errnum == E2BIG)
				    cmd_buffer->excessive_size =
					thread_data.last_excessive_size;
			    }
			    break;
			}
			fprintf(stderr, "Invalid function ID!\n");
			cmd_buffer->errnum = ENOSYS;
			break;
		    }
		if (!entry)
		{
		    fprintf(stderr, "Server stub backend not loaded!\n");
		    cmd_buffer->errnum = ENOSYS;
		}
	    }
	    else
	    {
		NYI();
		/* CIOD message.  */
		struct CioHeader* cioheader =
		    (struct CioHeader*)&cmd_buffer->softheader;
		int i;

		for (i = 0; i < pset_proc_count; i++)
		    if (cn_procs[i].p2p_addr == cioheader->_treeAddress &&
			cn_procs[i].cpu == cioheader->_cpu)
		    {
			thread_data.calling_process_id = i;
			break;
		    }
		if (i == pset_proc_count &&
		    cioheader->_messageCode == MFC_REQUESTRESET)
		{
		    /* We can receive a REQUESTRESET message for an
		       unknown process, which is the second CPU of a
		       node running in CO node.  We assign the message
		       to the primary CPU.  */
		    for (i = 0; i < pset_proc_count; i++)
			if (cn_procs[i].p2p_addr ==
			    cioheader->_treeAddress)
			{
			    thread_data.calling_process_id = i;
			    break;
			}
		}
		assert(i != pset_proc_count);

		result = handle_ciod_message(cioheader, cmd_buffer->data,
					     &result_len, i);
		total_len = result_len;
	    }
	} /* if (!cmd_buffer->errnum) */

	/* An error here can be either inherited from the receiving thread
	   (most likely an out-of-memory condition) or caused by call
	   function (most likely too large result buffer needed).  */
	if (cmd_buffer->errnum)
	{
	    /* Currently, three different error values are possible:
	       ENOMEM -- out of memory when allocating either the
		   command or result buffer.
	       E2BIG -- the command sent was too large (shouldn't happen,
		   since client checks that as well) or the result
		   would have been too large. cnproc->excessive_size
		   contains the excessive size value.
	       ENOSYS -- the stub backend for the requested function is
		   not loaded.
	       We manufacture an error reply here and it gets sent by
	       the standard reply sending code below.  */
	    result = __zoid_allocate_buffer(TREE_PACKET_SIZE);
	    if (!result)
		/* Out of memory.  MT-unsafe!  */
		result = packet_buffer->data;
	    if (cmd_buffer->errnum == E2BIG)
	    {
		result_len = sizeof(int);
		memcpy(result, &cmd_buffer->excessive_size, sizeof(int));
	    }
	    else
		result_len = 0;
	    total_len = result_len;
	}

	/* There are currently two cases where result == NULL without there
	   being any error:
	   - CIOD command that requires no response,
	   - partially received input userbuf, so we invoked the allocation
	   callback, not the actual argument-unpacking one.  */
	if (result)
	{
	    res_buffer = get_zoid_buffer(result);

	    res_buffer->result_len = result_len;
	    res_buffer->total_len = total_len;

	    res_buffer->errnum = cmd_buffer->errnum;
	    res_buffer->excessive_size = cmd_buffer->excessive_size;

	    if ((res_buffer->userbuf_out = thread_data.userbuf))
	    {
		res_buffer->userbuf_out_cb = thread_data.userbuf_cb;
		res_buffer->userbuf_out_priv = thread_data.userbuf_priv;
	    }

	    memcpy(&res_buffer->softheader, &cmd_buffer->softheader,
		   sizeof(res_buffer->softheader));

	    queue_message(res_buffer);
	}

	if (cmd_buffer != packet_buffer && release_cmd_buffer)
	    __zoid_release_buffer(cmd_buffer->data);

    } /* for (;;) */
}

int unix_open(const char *pathname /* in:str */,
	      int flags /* in:obj */,
	      mode_t mode /* in:obj */) __attribute__((weak));
int unix_close(int fd /* in:obj */) __attribute__((weak));
ssize_t unix_write(int fd /* in:obj */,
		   const void *buf /* in:arr:size=+1:zerocopy */,
		   size_t count /* in:obj */) __attribute__((weak));

/*
 * Processes the CIOD-specific messages.  These are generated by the kernel,
 * on application exit or crash.
 */
static char*
handle_ciod_message(struct CioHeader* cioheader, char* data, int* result_len,
		    int pset_cpu_rank)
{
    struct MTC_ReplyWriteCore reply;
    char* result;

    switch (cioheader->_messageCode)
    {
	case MFC_REQUESTEXIT:
	{
	    struct MFC_RequestExit* request = (struct MFC_RequestExit*)data;
	    fprintf(stderr, "ABNORMAL exit request from process %d, "
		    "code %d (%s)\n", cn_procs[pset_cpu_rank].pid,
		    request->s.status,
		    (request->s.reason == EXITED ? "exited" : "killed"));

	    enter_critical_section(&pending_exit_locks);

	    if (cn_procs[pset_cpu_rank].status == PROC_STATUS_RUNNING)
		pending_exit_requests--;
	    cn_procs[pset_cpu_rank].status = PROC_STATUS_EXIT_ABNORMAL;
	    abnormal_msg_received = 1;

	    leave_critical_section(&pending_exit_locks);
#if 0
	    cioheader->_messageCode = MTC_REPLYEXIT;
	    *result_len = 0;
	    break;
#endif
	    return NULL;
	}

	case MFC_REQUESTRESET:
	    fprintf(stderr, "ABNORMAL reset request from node %d cpu %d\n",
		    cioheader->_treeAddress, cioheader->_cpu);

	    enter_critical_section(&pending_exit_locks);

	    if (cn_procs[pset_cpu_rank].status == PROC_STATUS_RUNNING)
		pending_exit_requests--;
	    cn_procs[pset_cpu_rank].status = PROC_STATUS_EXIT_ABNORMAL;
	    abnormal_msg_received = 1;

	    leave_critical_section(&pending_exit_locks);
#if 0
	    cioheader->_messageCode = MTC_REPLYRESET;
	    *result_len = 0;
	    break;
#endif
	    return NULL;

	case MFC_REQUESTWRITECORE:
	{
	    struct MFC_RequestWriteCore* request =
		(struct MFC_RequestWriteCore*)data;
	    char buffer[20];
	    int fd;

	    fprintf(stderr, "ABNORMAL core dump request from process %d\n",
		    cn_procs[pset_cpu_rank].pid);

	    sprintf(buffer, "core.%d", cn_procs[pset_cpu_rank].pid);

	    if (&unix_open)
		fd = unix_open(buffer, O_CREAT | O_WRONLY | O_TRUNC, 0600);
	    else
	    {
		if ((fd = open(buffer, O_CREAT | O_WRONLY | O_TRUNC, 0600)) < 0)
		    fd = -errno;
	    }

	    if (fd < 0)
		fprintf(stderr, "Error opening file %s: %s\n", buffer,
			strerror(-fd));
	    else
	    {
		if (&unix_write)
		    unix_write(fd, data + request->s.offset, request->s.size);
		else
		    write(fd, data + request->s.offset, request->s.size);

		if (&unix_close)
		    unix_close(fd);
		else
		    close(fd);
	    }

	    enter_critical_section(&pending_exit_locks);

	    if (cn_procs[pset_cpu_rank].status == PROC_STATUS_RUNNING)
		pending_exit_requests--;
	    cn_procs[pset_cpu_rank].status = PROC_STATUS_EXIT_ABNORMAL;
	    abnormal_msg_received = 1;

	    leave_critical_section(&pending_exit_locks);

	    if (fd >= 0)
	    {
		reply.s.rc = 0;
		reply.s.errnum = 0;
	    }
	    else
	    {
		reply.s.rc = -1;
		reply.s.errnum = EACCES; /* Who cares what it was anyway.  */
	    }
	    cioheader->_messageCode = MFC_REPLYWRITECORE;
	    *result_len = sizeof(reply);
	    break;
	}
    }

    cioheader->_packetIndex = 0;
    cioheader->_packetTotal = 1;

    result = __zoid_allocate_buffer(*result_len);
    if (!result)
	/* Out of memory.  MT-unsafe!  */
	result = packet_buffer->data;
    memcpy(result, &reply, *result_len);

    return result;
}

/*
 * Part of user API.  Returns a (thread-local) pset-local process id.
 */
int
__zoid_calling_process_id(void)
{
    struct thread_specific_data* thread_data =
	(struct thread_specific_data*)pthread_getspecific(thread_specific_key);
    return thread_data->calling_process_id;
}

/*
 * Part of user API.  Registers an output userbuf and a callback to
 * be called when the buffer is no longer needed.
 */
void
__zoid_register_userbuf(void* userbuf,
			void (*callback)(void* userbuf, void* priv),
			void* priv)
{
    struct thread_specific_data* thread_data =
	(struct thread_specific_data*)
	pthread_getspecific(thread_specific_key);

    thread_data->userbuf = userbuf;
    thread_data->userbuf_cb = callback;
    thread_data->userbuf_priv = priv;
}

/*
 * Part of user API.  Sends standard output/error message to the right place.
 */
int
__zoid_send_output(int pid, int fd, const char* buffer, int len)
{
    struct CiodOutputHeader header;
    int ret;

    if (pthread_mutex_lock(&output_mutex))
	return -1;

    if (ciod_streams_socket == -1)
	ret = write(fd, buffer, len);
    else
    {
	header.fd = fd + 1;
	header.cpu = cn_procs[pid].cpu;
	header.node = cn_procs[pid].p2p_addr;
	header.rank = cn_procs[pid].pid;
	header.len = len;

	if (write(ciod_streams_socket, &header, sizeof(header)) !=
	    sizeof(header))
	{
	    ret = -1;
	}
	else
	    ret = write(ciod_streams_socket, buffer, len);
    }

    if (pthread_mutex_unlock(&output_mutex))
	return -1;

    return ret;
}

void *ciod_thread_body(void* arg) {
    int signum;
    struct timeval tv;
    double last_packet_received;
    struct CioHeader cio;
    int i;

    pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, NULL);
    pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, NULL);

    /* We only support one command at the moment: kill.  */
    for (;;)
    {
	int cmd;

	if (read(ciod_control_socket, &cmd, sizeof(cmd)) != sizeof(cmd))
	    perror("read");
	else if (cmd != 4)
	    fprintf(stderr, "Unknown command read: 0x%x\n", cmd);
	else
	{
	    if (read(ciod_control_socket, &signum, sizeof(signum)) !=
		sizeof(signum))
	    {
		perror("read");
	    }
	    else
		break;
	}
    }

    /* Disable thread cancellation before we try to acquire any mutexes.  */
    pthread_setcancelstate(PTHREAD_CANCEL_DISABLE, NULL);

    /* We set this flag even before we send anything, for one simple reason:
       so that pthread_mutex_lock below can succeed.  worker threads check
       for this flag and release the receive_queue_mutex if it is set, so that
       we can get it.  */
    sent_kill_packet = 1;

    enter_critical_section(&tree_locks);

    /* We are going to kill the job.  We do that by sending a special kill
       packet that wakes up the CNK, which takes over and kills the processes.
       There is one but: there might be some ZOID messages in the network, and
       if CNK or CIOD see them, they might act weird.
       There is no fool-proof solution to this problem.  What we do is to
       read all the messages that we can on this side.  We throw these away;
       we are killing the job anyway, so who cares.  Hopefully this action
       will quiesce the network (every sender waits for a reply before it can
       continue), so that we can then wake up the CNK relatively safely.  */

    /* Suck all the packets from the network first.  */
    gettimeofday(&tv, NULL);
    last_packet_received = tv.tv_sec + tv.tv_usec * 1e-6;
    for (;;)
    {
	BGLTreeStatusRegister status;

	status = *(volatile BGLTreeStatusRegister*)
	    (vc0 + BGL_MEM_TREE_STATUS0_OFFSET);

	if (status.recpktcnt == 0)
	{
	    gettimeofday(&tv, NULL);
	    /* If we haven't received anything in five seconds, assume we
	       are done.  */
	    if (tv.tv_sec + tv.tv_usec * 1e-6 - last_packet_received > 5)
		break;
	}
	else
	{
	    while (status.recpktcnt--)
	    {
		BGLTreePacketHardHeader hardheader;
		BGLTreeFIFO_recv(vc0 + BGL_MEM_TREE_HDROUT_OFFSET,
				 vc0 + BGL_MEM_TREE_DATAOUT_OFFSET,
				 &hardheader, (BGLQuad*)packet_buffer->data);
	    }
	    gettimeofday(&tv, NULL);
	    last_packet_received = tv.tv_sec + tv.tv_usec * 1e-6;
	}
    }

    /* Now, kill the processes.  */

    cio._dataSize = 0;
    cio._messageCode = MTC_KILL;
    cio._packetTotal = 1;
    cio._packetIndex = 0;

    ((struct MTC_Kill*)packet_buffer->data)->s.signum = signum;

    enter_critical_section(&pending_exit_locks);

    for (i = 0; i < pset_proc_count; i++)
	if (cn_procs[i].status == PROC_STATUS_RUNNING)
	{
	    BGLTreeStatusRegister status;
	    BGLTreePacketHardHeader hardheader;

	    BGLTreePacketHardHeader_InitP2P(&hardheader, PACKET_CLASS_CIO,
					    1, cn_procs[i].p2p_addr);
	    cio._cpu = cn_procs[i].cpu;
	    cio._rankInCnodes = cn_procs[i].pset_rank;
	    cio._treeAddress = cn_procs[i].p2p_addr;

	    do
	    {
		status = *(volatile BGLTreeStatusRegister*)
		    (vc0 + BGL_MEM_TREE_STATUS0_OFFSET);
	    } while (status.injpktcnt > 7);

	    BGLTreeFIFO_sendH(vc0 + BGL_MEM_TREE_HDRIN_OFFSET,
			      vc0 + BGL_MEM_TREE_DATAIN_OFFSET,
			      &hardheader, (BGLQuad*)&cio,
			      (BGLQuad*)packet_buffer->data);

	    pending_exit_requests--;
	    cn_procs[i].status = PROC_STATUS_EXIT;
	}

    leave_critical_section(&pending_exit_locks);

    leave_critical_section(&tree_locks);

    return NULL;
}

/*
 * Called after a job has finished to clean up any allocated buffers,
 * reset state variables, etc.
 */
void cleanup_traffic(void)
{
#if 1 
   NYI();
#else
    struct zoid_buffer* buf;

    while (send_queue_first)
    {
	buf = send_queue_first;
	send_queue_first = send_queue_first->next;
	__zoid_release_buffer(buf->data);
    }

    while (high_priority_send_queue_first)
    {
	buf = high_priority_send_queue_first;
	high_priority_send_queue_first = high_priority_send_queue_first->next;
	__zoid_release_buffer(buf->data);
    }

    while (receive_queue_first)
    {
	buf = *receive_queue_first;
	*receive_queue_first = (*receive_queue_first)->next;
	__zoid_release_buffer(buf->data);
    }

    while (ack_queue_first)
    {
	buf = ack_queue_first;
	ack_queue_first = ack_queue_first->next;
	__zoid_release_buffer(buf->data);
    }

    currently_receiving_large_message = 0;
#endif
}
