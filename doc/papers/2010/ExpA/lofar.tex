%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2006/03/15
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallextended]{svjour3}     % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}         % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage[usenames]{color}
\usepackage{citesort}
\usepackage{dcolumn}
\usepackage{graphicx}
\usepackage{mathptmx}      % use Times fonts if available on your TeX system
\usepackage{wrapfig}
\usepackage{txfonts}
\usepackage{xspace}
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
\journalname{Experimental Astronomy}
%

%\newcounter{theparagraph}
\setcounter{secnumdepth}{4}
\renewcommand{\paragraph}[1]{%
%\addtocounter{\theparagraph}{1}%
%\vspace{2mm}\noindent\emph{\textbf{\theparagraph #1}} --- }
\vspace{2mm}\noindent\emph{\textbf{#1}} --- }

\newcommand{\us}{\,$\muup$s\xspace}

\definecolor{Gold}{rgb}{1,0.84,0} 
\newcommand{\circlenumber}[1]{%
\begin{picture}(10,10)%
\put(5,2.5){\circle*{11}}%
\color{Gold}%
\put(-.3,0){\makebox[9pt]{\fontfamily{phv}\fontseries{b}\fontshape{sl}\selectfont\small#1}}%
\end{picture}%
}

\newcolumntype{.}{D{.}{.}{-1}}

\begin{document}

\title{The LOFAR Correlator%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{Short form of title}        % if too long for running head

\author{\hbox{John W. Romein \and P. Chris Broekema \and Jan David Mol \and Rob V. van Nieuwpoort}}

%\authorrunning{Short form of author list} % if too long for running head

\institute{John W. Romein \and P. Chris Broekema \and Jan David Mol \and Rob V. van Nieuwpoort \at
              ASTRON (Netherlands Institute for Radio Astronomy) \\
	      Dwingeloo, The Netherlands \\
%              Tel.: +123-45-678910\\
%              Fax: +123-45-678910\\
              \email{\{romein,broekema,mol,nieuwpoort\}@astron.nl}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
%           \and
%           S. Author \at
%              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
This article describes the LOFAR correlator.
The LOFAR correlator uses a novel approach to process real-time, streaming
telescope data, by using \emph{software}.
Traditional telescopes typically use custom-built hardware, but the need
for a flexible and reconfigurable instrument that supports multiple processing
pipelines demands a software solution.
However, the high data rates and processing requirements compel the use of a
supercomputer.
We use an IBM Blue Gene/P to centrally combine the data from the LOFAR stations.
The correlator is highly optimized and achieves very good computational
performance and high bandwidths.

In this article, we focus on the functionality of the correlator.
We describe the standard imaging pipeline and similar pipelines, several pulsar
pipelines, and the Ultra-High Energy Particles pipeline.
The correlator supports multiple concurrent observations, even of different
types.


\keywords{LOFAR \and Correlator \and Blue Gene/P}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}

This article describes the central, real-time part of the LOFAR data
processing.
The data is processed in software, by an application commonly called
``the correlator''.
Actually, this term is a misnomer, since the correlator can perform many
more operations than correlating only, and will support a wide range of
observation types.

The LOFAR correlator is the first large-scale correlator that processes
data in \emph{software}, in real time.
Traditionally, correlators use custom-built hardware to process data, because
of the high data rates and processing requirements.
However, the desire to support different observation types demands a more
flexible software solution.
The availability of a sufficiently powerful supercomputer, an IBM Blue Gene/P,
allows this.
We perform all processing in real time, since the data streams
that come from the stations are simply too large to store.

The correlator supports a number of processing pipelines, with shared pipeline
components.
The \emph{standard imaging pipeline\/} filters and correlates station data.
The \emph{Epoch of Reionization (EoR)\/} and \emph{Survey\/} pipelines are,
at least in the real-time part, similar to the standard imaging pipeline.
There is a collection of \emph{pulsar pipelines}, that coherently or
incoherently filter and beam form station data.
The \emph{Ultra-High Energy Particles (UHEP)\/} pipeline will beam form data as
well, and will search for peaks that should trigger the Transient Buffer Boards.
Multiple, concurrent observations, even of different types, are supported.
The software is still in development: the standard imaging pipeline is fully
functional; the pulsar pipelines are mostly implemented and have been used for
pulsar observations, and the UHEP pipeline is in its design phase.
Additional pipelines, such as the \emph{Transient Detection\/} pipeline and
the \emph{High-Energy Cosmic Ray\/} mode will be added later.
Since they have not been designed in detail yet, we do not discuss them in
this paper.

This paper focuses on the functionality of the correlator, not on the
implementation.
Other papers describe the implementation~\cite{Romein:06,Romein:10a} or
particular aspects thereof~\cite{Iskra:08,Romein:09a,Kazutomo:10}.


The remainder of this article is structured as follows.
In Section~\ref{sec:BG/P}, we describe the Blue Gene/P hardware.
Section~\ref{sec:processing} describes all pipeline components for the
imaging, pulsar, and UHEP modes.
Section~\ref{sec:results} shows correlator (performance) results.
Finally, Section~\ref{sec:conclusions} concludes.


\section{The IBM Blue Gene/P}
\label{sec:BG/P}

The Blue Gene/P (BG/P) is a supercomputer that combines powerful computational
capabilities with a fast interconnect.
The computational power comes from a large number of relatively simple
processor cores (4,096 per rack) that run at a low clock speed, to reduce
power consumption and allow a dense packaging.
Each chip consists of four PowerPC cores, and each core is extended by
two custom-built double-precision Floating-Point Units (FPU), that provide
remarkably good support for operations on complex numbers.
Although the support for double precision is overkill for our application,
the good support for complex numbers is handy for signal-processing purposes.

The nodes are connected by \emph{five\/} different types of networks,
all integrated on the same chip.
The most important network is the \emph{three-dimensional torus}, that
connects all compute nodes.
The \emph{collective network\/} is used for MPI collective operations,
but also for external communication, as explained below.
Additional networks exist for fast barriers, initialization, diagnostics, and
debugging.
More information on the BG/P can be found elsewhere~\cite{IBM:08}.

\begin{wrapfigure}{r}{62mm}
\vspace{-9mm}
\begin{center}
\includegraphics[width=60mm]{pset.pdf}
\end{center}
\caption{I/O function forwarding on the Blue Gene.}
\vspace{-5mm}
\label{fig:pset}
\end{wrapfigure}
Compute nodes are not connected directly to external systems, but use
\emph{I/O~nodes\/} for external communication.
Each I/O-related system call on a compute node (e.g., \texttt{open()},
\texttt{read()}, \texttt{socket()}) is forwarded by the operating system to
a daemon process on the I/O~node that performs the real operation (see
Figure~\ref{fig:pset}).
Despite the fact that I/O~nodes were not designed to run application software,
we found that it is much more efficient to run part of the correlator
software on the I/O~node~\cite{Iskra:08}.
In Section~\ref{sec:processing}, we will elaborate on this.
%An I/O~node uses the same hardware as a compute node, but has its
%10~Gb/s Ethernet interface connected and runs another operating system
%(a modified Linux kernel).
%The group of one I/O~node and its associated compute nodes is called a
%\emph{pset}.
%Our system has 192~psets in total, 64 per rack.

In 2005, LOFAR used a six-rack IBM Blue Gene/L (BG/L) supercomputer for
real-time processing of the station data, which was, at that time, the sixth
fastest supercomputer in the world.
In 2008, the system was replaced by its successor, a three-rack BG/P.
The biggest improvements of the BG/P were the added L1-cache coherency,
improved reliability, the ability to overlap computation and communication,
reduced power consumption, and much more mature system software.
Especially the latter is appreciated: for the BG/L we had to rewrite major
parts of the operating system and network system software to achieve acceptable
I/O performance~\cite{Iskra:08,Boonstoppel:08} (which was done in close
cooperation with Argonne National Laboratory, Chicago).
In contrast, on the BG/P we only need a kernel patch to avoid slow TLB-miss
exceptions~\cite{Kazutomo:10} and a fast, user-space network protocol between
the compute nodes and the I/O~nodes~\cite{Romein:09a}.

%Although the architecture of the BG/P does not differ much from that
%of the BG/L, there were several advantages.
%First, the system software of the BG/P is much more mature than that of the
%BG/L.
%For the BG/L, we had to rewrite major parts of the operating system and network
%system software to achieve acceptable I/O
%performance~\cite{Iskra:08,Boonstoppel:08} (which was done in close cooperation
%with Argonne National Laboratory, Chicago).
%Second, the L1 caches of the processor cores are coherent in the BG/P, unlike
%those of the BG/L, where shared-memory processing was virtually impossible.
%Third, provisions were made to transparently recover from L1-cache faults,
%making the system much more reliable.
%Fourth, asynchronous communication (i.e., overlapping of computations and
%communication) on the 3-D~torus was enabled by adding a DMA (Direct Memory
%Access) engine.
%Furthermore, the use of 10-Gb/s Ethernet interfaces (as opposed to 1-GbE on
%the BG/L) simplified the connections to the stations.
%However, the relatively slow processor cores of I/O~nodes limit the data rate
%to a sustained speed of roughly four Gb/s in practice.



\section{Processing pipelines}
\label{sec:processing}

\begin{wrapfigure}{r}{62mm}
\vspace{-9mm}
\begin{center}
\includegraphics[width=60mm]{overview.pdf}
\end{center}
\caption{Data flow through the correlator.}
\vspace{-5mm}
\label{fig:overview}
\end{wrapfigure}
The LOFAR stations send data via the Wide-Area Network to the correlator
(see Figure~\ref{fig:overview}).
The Blue Gene I/O nodes receive the data, and forward the data to the
compute nodes, where most of the processing is done.
The resulting data is sent back via the I/O~nodes to a cluster of storage
nodes, where the data is written to disk.
After an observation, these data are postprocessed (not shown).
Correlated data is flagged, calibrated, and imaged~\cite{Nijboer:10};
pulsar data is dedispersed and folded~\cite{Hessels:10}.

Note that the HBAs of a core station can be split into two separate
substations~\cite{?}.
The correlator treats a split station as if it were two independent stations,
hence both substations are correlated to all other stations, as well as
mutually.

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{processing.pdf}
\end{center}
\caption{Overview of the processing pipeline components.
At the places where the data flow arrows diverge, either path can be chosen,
with the exception of the correlator and the beam forming pipelines, that can
run concurrently on the same data.
Many components are optional, indicated by a bypass path around or beneath the
component box.
The main text uses the circled numbers to refer to the pipeline components.
}
\label{fig:processing}
\end{figure}

In the remainder of this section, we explain all pipeline components of
the correlator.
Figure~\ref{fig:processing} shows how the data flows through the entire
processing chain.
The circled numbers below refer to the pipeline components in the figure.


\paragraph{Data Receipt \circlenumber{1}}
The stations send packets with subband data over the Wide-Area Network to the
Blue Gene.
The observer selects up to 248 subbands (frequency/beam combinations) of
195~KHz (assuming a 200~MHz station clock), corresponding to 48.4~MHz
observation bandwidth.
Each (sub)station sends 48,828 packets per second, at a rate of 3.1~Gb/s.

We use the UDP datagram protocol for data transport, which is simple, but
unreliable.
A reliable protocol like TCP would significantly complicate the programming of
the station FPGAs, due to buffering, flow control, retransmissions, real-time
issues, and the necessity to communicate in two directions.
The correlator takes care of out-of-order, duplicated, and lost UDP packets.
Missing data is administrated throughout the remainder of the processing
pipeline, but in practice, hardly any data is lost.


\paragraph{The Circular Buffer \circlenumber{2}}
A Blue Gene I/O~node receives data from one (sub)station, and stores the
data into a circular buffer.
Due to the high data rate and the small amount of available memory,
the circular buffer holds only the most recent 2.5~seconds of data, after
which old data is overwritten by new data.
In practice, 2.5~seconds is sufficient, due to the good real-time behavior of
the correlator.


\begin{wrapfigure}{r}{37mm}
\vspace{-5mm}
\begin{center}
\includegraphics[width=35mm]{delay.pdf}
\end{center}
\caption{The left antenna receives the wave later.\vspace{-4mm}}
\label{fig:delay}
\end{wrapfigure}
\paragraph{Delay Compensation \circlenumber{3}}
Due to the finite speed of electromagnetic waves, the wavefront from a
celestial source hits stations at different times
(see Figure~\ref{fig:delay}).
The time difference depends on the direction of the observed source and on the
station positions, and is continuously altered by the rotation of the earth.
Therefore, all station streams must be aligned before the signals can be
correlated.

Since delays can be larger than the sample period, we perform delay
compensation in two steps.
First, we delay the stream of station samples by an integer amount.
For example, a 22\us delay can be achieved by shifting four 5.12\us
samples; an error of 1.52\us remains.
The shift is performed on the I/O~node, by adjusting the read pointer of the
circular buffer.
The remaining error is corrected by rotating the phase of the signal later on
in the pipeline (see \circlenumber{7}).


\paragraph{I/O Node to Compute Node transport \circlenumber{4}}
The I/O~node sends the data directly from the circular buffer to the compute
nodes for further processing, in chunks of up to one second.
We developed a new network protocol for this purpose, called
\emph{FCNP\/}~\cite{Romein:09a}, since existing protocols did not provide
enough bandwidth and consumed too many CPU cycles.
FCNP communicates at link speed for large transfers.
Timely transport to the compute node is important, since the circular buffer
is small.
In the exceptional case that (part of the) data is lost due to a missed
real-time deadline, the missing data is flagged.


\paragraph{Round-Robin distribution}
A processor core is too slow to process one second of data within one second of
real time (e.g., filtering and correlating one second of data from 64~stations
requires 5.6~seconds processing time~\cite{Romein:10a}).
The basic idea is that the I/O~node sends subsequent chunks to different
compute nodes in round-robin order (i.e., one after another, in a cyclic
manner).
A compute node first receives data from the I/O~node, processes them, sends
back the results, and idles until the I/O~node sends new data.


\paragraph{Data Transpose \circlenumber{5}}
Unfortunately, an I/O~node cannot send the data directly to the compute node
that must correlate the data, since an I/O~node is connected to only a few
compute nodes.
Also, an I/O~node contains data from \emph{all subbands\/} of a \emph{single
station}.
However, to correlate, a compute node needs data from a \emph{single subband\/}
of \emph{all stations}.
Hence, the sampled data must be exchanged with other compute nodes.
We use the 3-D torus to switch hundreds of gigabits per second.
These are challenging data rates, but the torus is well capable of doing so.
We overlap communication with the computational steps described below,
to hide the costs of sending and receiving data.
The program code that performs the transpose is highly efficient but extremely
complicated; due to the round-robin scheduling of work, the group of compute
nodes that must exchange data continuously changes over time.


\paragraph{Data Conversion}
We convert the 4-bit, 8-bit, or 16-bit integer samples to 32-bit
floating point numbers (the stations currently support only 16-bit samples,
but the correlator is ready for future modes).
We convert the data to floating point, because the Blue Gene is much better at
floating-point processing than integer processing.
Since the conversion increases the data size, we perform it \emph{after\/} the
data transpose.


\paragraph{The Poly-Phase Filter Bank \circlenumber{6}}
Next, the subband data is processed by a Poly-Phase Filter (PPF) bank that
splits a frequency subband into a number of narrower frequency channels.
In this step, we trade time resolution for frequency resolution: we split a
subband into $N$ separate channels, but with an $N$-times lower sampling rate
per channel.
With the higher frequency resolution, we can remove RFI artifacts with a
higher accuracy later in the pipeline.
Typically, a 195~KHz subband is split into 256~channels of 763~Hz, but the
filter supports any reasonable of channels.

The PPF bank consists of two parts.
First, the data is filtered using Finite Impulse Response (FIR) filters,
to avoid leakage between channels.
A FIR filter simply multiplies a sample with a real weight factor, and
also adds a number of weighted samples from the past.
Since we have to support different numbers of channels, our software
automatically designs a filter bank with the desired properties and number
of channels at run time, generating the FIR filter weights on the fly.
This again demonstrates the flexibility of a software solution.
For performance reasons, the implementation of the filter is done in assembly.
Second, the filtered data is Fourier Transformed.
We use the Blue Gene ``Vienna'' version of FFTW~\cite{Lorenz:05} to do this.
Since the most common observation mode uses 256 channels, we optimized this
case a bit further, and manually wrote a more efficient assembly
implementation for the 256-point FFT.

In priciple, any reasonable number of channels per subband is supported.
However, powers of two are more efficient, since power-of-two FFT sizes
are more efficient.
Also, the number of FIR filter taps is fixed to~16, so the quality of the
filter becomes pretty bad when the number of channels is below 16.
Unfortunately, adding more taps would make the (assembly) implementation
of the FIR filter unacceptably less efficient, so we do not support this.
Finally, channel~0 of each subband is always marked as flagged, since both
the lowest and the highest frequencies of the subband are folded into this
channel, and the different frequencies within this channel cannot be
disambiguated~\cite{Romein:08}.


\paragraph{Phase Shift Correction \circlenumber{7}}
Next, the remaining fraction of the delays are compensated (see
\circlenumber{3}), by rotating the phase of the signal.
We apply the correction after creating channels, since the correction factor
depends on the frequency.
The phase rotation itself requires a complex multiplication per sample.
The exact delays are computed for the begin time and end time of a chunk,
and interpolated in frequency and time for each individual sample (with
another complex multiplication), so the sky source is followed very accurately.


\begin{wrapfigure}{r}{51mm}
\vspace{-6mm}
\includegraphics[width=51mm]{bandpass.pdf}
\caption{Channels have different signal powers.}
\vspace{-5mm}
\label{fig:bandpass}
\end{wrapfigure}
\paragraph{Bandpass Correction \circlenumber{8}}
The bandpass correction step compensates for an artifact introduced by a
filter back that runs on the FPGAs in the stations~\cite{Kooistra:10}.
This filter bank performed the initial division of the antenna signals into
subbands.
Without correction, some channels have a stronger signal than others
(see Figure~\ref{fig:bandpass}).
The correction is performed by multiplying each complex sample by a constant,
real, channel-dependent weight.
A station cannot correct for this artifact itself, since it is only visible
in channels, not in subbands.
We describe how the correction factors are computed in~\cite{Romein:08}.


\paragraph{Finalizing the Asynchronous Transpose \circlenumber{5}}
Up to this point in the pipeline, chunks of data from different stations are
processed independently, in any order, as soon as the data is received.
However, from here on, the data from all stations is needed together.
Therefore, the processor waits until the data from all stations is received
and processed before proceeding with the next step.


\paragraph{Superstations \circlenumber{9}}
Optionally, any group of stations can be beam formed to create a more sensitive
\emph{superstation}.
Such a superstation is treated as a single station in the remainder of the
pipeline.
A typical use case is the addition of all superterp stations in long-baseline
observations, so that the superterp forms a very sensitive virtual station.
Grouping stations into superstations significantly reduces the number of
baselines and hence the output data rate of the correlator.


\paragraph{The Correlator \circlenumber{10}}
The samples from each pair of individual or grouped stations are
correlated, by multiplying the sample of one station with the complex conjugate
of the sample of the other station.
To reduce the output size, the products are integrated, by accumulating all
products (\circlenumber{11}).
We typically accumulate 768~correlations at 763~Hz, so that the integration
time is approximately one second, the size of a chunk.
The correlator is the most time-consuming operation in the signal
processing path, because its cost grows quadratically with the number of
stations.
All other steps have a lower time complexity.


\paragraph{Beam Forming}
\label{sec:beam-forming}
We implemented a variety of beam-forming modes, to support the pulsar pipelines
and transient pipelines.
We briefly describe the modes here; more information can be found in the
paper on pulsar modes elsewhere in this issue~\cite{Hessels:10}.

The beamforming pipelines fall into two catagories: the coherent modes and
incoherent modes.
The coherent modes add the complex samples from all stations, resulting in a
complex voltage.
The beam of the coherent sum of all stations is much smaller than the station
beams themselves, and is inversely proportional to the distance of the stations.
The incoherent modes first compute the power of each sample, and add the
powers from the different stations.
The beam of incoherently added stations covers the beam of the individual
stations.


\begin{wrapfigure}{r}{35mm}
\vspace{-9mm}
\begin{center}
\includegraphics[width=25mm]{pencil-beam.pdf}
\end{center}
\caption{Multiple pencil beams cover (part of) the station beam.}
\vspace{-5mm}
\label{fig:pencil-beams}
\end{wrapfigure}
\paragraph{Coherent Beam Forming}
Since the beam of the coherent sum of the stations is narrow, only a small
part of the sky is covered.
However, the coherent modes can form multiple
\emph{pencil beams\/}~(\circlenumber{12}), to cover more of the much larger
station beam (see Figure~\ref{fig:pencil-beams}).
The direction of a (pencil) beam is established by applying appropriate
delays to each station before adding their complex samples, and a delay is
established by applying an appropriate phase rotation (c.f., \circlenumber{7}).
In other words, a sample is first multiplied by a (station and
direction-dependent) complex weight, and then added to the weighted samples
of the other stations.

To create multiple pencil beams with different directions, different delays
are appied to a station before addition.
Note that the center beam needs no correction, as the phase correction for
this direction was already performed at high precision (see \circlenumber{7}).
The other pencil beams are corrected with respect to the center beam.

The directions of the pencil beam can be choosen freely (of course, observing
outside the station beams makes no sense).
However, there is provision for specifing a honeycomb structure around the
center beam (see Figure~\ref{fig:pencil-beams}).
This simplifies the specification of an observation, since a honeycomb structure
requires only two parameters: the diameter of a pencil beam and the number
of rings.

Each pencil beam creates its own stream of complex voltages.
Optionally, the complex voltages can be converted to Stokes
I,Q,U,V~(\circlenumber{13}), or to Stokes~I only~(\circlenumber{14}).


\paragraph{Incoherent Beam Forming}
Unlike the coherent modes, the incoherent modes add the powers of the signals
from the different stations, rather than the signals themselves.
Since the beam of the incoherent sum covers the station beams, there is no
need to create multiple pencil beams.
As a consequence, the output data rate of this mode is much smaller.
Either Stokes I,Q,U,V~(\circlenumber{15}) or to Stokes~I
only~(\circlenumber{16}) can be choosen.


\paragraph{Integration of Beam-Formed Products \circlenumber{17}}
The number of pencil beams that can be formed is likely to be limited by the
output data rate that can be handled (specified at 50~Gb/s).
The data rate can be reduced by integrating multiple samples.
We also plan to support conversion from single-precision (32-bit) floating
point to 16-bit or even smaller bit-count values, to reduce the data rate
even further.


\paragraph{The Second Transpose \circlenumber{18}}
A Blue Gene compute node computes all beams of (all channels within) a
single subband; different subbands are necessarily computed by different 
compute nodes.
However, the next steps of the pulsar and transient processing pipelines
need the data of all subbands close together; different beams can further be
processed independently on different processors.
Hence, a second transpose is necessary to reorder the beam-formed data.

We are currently implementing this transpose.
For the pulsar pipelines, the transpose can be done offline, but it is more
convenient and efficient to do it online on the fast torus network on the
Blue Gene.
For the transient detection pipeline, the transpose \emph{must\/} be done
in real time.


\paragraph{Data Transport to the I/O~Node \circlenumber{19}}
The beam-formed and correlated data is sent back to the I/O~nodes, again
using FCNP.

\paragraph{Further Integration \circlenumber{20}}
The I/O~node can optionally integrate correlated data over multiple seconds.
The reason that this is done on the I/O~node and not on the compute node is
historical, and has to do with the absence of asynchronous communication
on the BG/L.


\paragraph{The Best-Effort Queues \circlenumber{21}}
At the very end of the Blue Gene pipelines, the output data is queued for
sending to the storage nodes.
The queue improves real-time behavior and increases fault tolerance, since
it handles data on a best-effort basis.
If, for any reason, the data is not sent quickly enough to the storage node,
the queue fills up and subsequent data is simply discarded until space is
available.
This way, we can tolerate intermittent or permanent disk and network failures
with minimal loss of data.
The mechanism is important to keep the correlator running in real
time: it is much better to lose a small part of the data than to stall the
entire correlator and lose \emph{all\/} data.
Under normal circumstances, no data is lost here.


\paragraph{Data Transport to the Storage Node \circlenumber{22}}
The data from the best-effort queues is dequeued and sent to the storage nodes.
Unlike the station input, we use TCP here, since a reliable connection here
is more important than real-time behavior (possible hiccups are handled by
the best-effort queues).


\paragraph{Storing the Data \circlenumber{23}}
The correlated data is written as CasaCore Measurement Set~\cite{Kemball:00}.
Initially, we used the CasaCore ``tables'' library to write data, but we
found that the library consumed far too many CPU cycles to sustain write
speeds of more than a few tens of megabytes per second per storage node.
To improve this speed by more than an order of magnitude, we now write the
raw streams of correlations \emph{uninterpretedly\/} to disk, and changed
the CasaCore library so that it supports the raw data format as well,
without changing the Application Programming Interface of the library.
This way, legacy applications can read (and write) Measurement Sets that
internally use the raw data format, by relinking against the new CasaCore
library, either dynamically or statically.
A versioning mechanism is provided to support future data formats.

Only the correlations and flagging information are written in the raw
format, bypassing the CasaCore library.
All metadata is written in the ``old'' format, using the normal CasaCore
library primitives; writing the metadata is not on the time-critical path.

Beamformed data is written in the HDF5 format~\cite{?}.
There is a large number of tools available that supports this file format.



\subsection{The Ultra-High Energy Particle pipeline}

The Ultra-High Energy Particle pipeline is currently in its design phase.
This pipeline coherently beamforms the stations and reverts back to the 200~Mhz
time-domain, to detect peaks caused by ultra-high-energy particles.
It largely relies on the pulsar pipeline, since it will use the beam former
to create many tens of beams.
However, the second PPF~(\circlenumber{6}) and bandpass
correction~(\circlenumber{8}) are bypassed.
The second transpose, used for the pulsar pipeline will be used to spread the
beams over different compute nodes and join all frequencies, so that a compute
node has all frequencies of a single beam.
This way, the first PPF (at the stations) can be undone, by an inverse PPF bank
(\circlenumber{24} and \circlenumber{25}).
Of course, some of the signal is lost, since only 248 out of 512~subbands can
be sent by the stations.
A peak-detection algorithm (\circlenumber{26}) inspects the time-series data
to generate a trigger to freeze the Transient Buffer Boards~\cite{Kooistra:10}.



\subsection{Multiple Observations}

The correlator is very flexible: multiple, independent observations can
run concurrently.
For example, the core stations can be set in the 200~MHz mode and used for
a pulsar observation, while the remote stations and international stations
can be set in the 160~MHz mode and used for an imaging observation.

Also, multiple (types of) observations can be done using a single group of
stations.
In this case, the available observation bandwidth is divided over the
observations; the total bandwidth cannot exceed 48.4~MHz.
Another restriction in the HBA mode is that all observed sources must be in the
beam of the tiles, since the analogue tile beam former can only point at one
direction~\cite{?}.

Additionally, an observation can produce multiple types of outputs, e.g.,
the station data can both be correlated \emph{and\/} beam formed, so that
an imaging pipeline and pulsar pipeline can piggy-back each other.
In this case, the available observation bandwidth is not divided, but shared
by the different observation types.
Some restrictions apply, since the pipelines share common components through
which the data is processed only once.
For example, the Poly-Phase Filterbank generates the same amount of channels
for both the pulsar pipeline and imaging pipeline (although the beam-formed
data can be compressed in frequency direction afterward).
Also, the maximum output data rate must not be exceeded.

The ability to run multiple pipelines simultaneously does not only increase
the scientific output of the instrument, in the future, it also offers the
possibility to improve the data quality.
For example, when the data is both correlated and beam formed, the correlated
data can be used to calibrate the input data stream in real time (with a few
seconds delay), using a feed-back loop that applies corrections to the station
samples.
This way, the quality of the beam-formed data improves as well.


\subsection{Control}

The correlator can both be run under control of MAC~\cite{Overeem:10} and
using a series of Python scripts.
The former method is primarily used for production observations by the
LOFAR Observatory, while the latter is mostly used for development.
The correlator reads a \emph{parameter set\/} with observation-specific
parameters, one per observation, provided by MAC or by the Python environment.
MAC also monitors the log output of the correlator.


\subsection{Future Work}

Although the correlator is in production use for daily imaging and pulsar
observations, we still extend its functionality.
In addition to the observation modes that are not implemented yet, we list
some ideas for future work below.

We plan to implement some form of real-time flagging, to discard bad data
as soon as possible.
This has several advantages, for example in the beam-forming pipelines,
where a bad signal from a single station now corrupts the entire sum.
Also, if a real-time flagger would perform really well, there is opportunity
to average the visibility data in frequency direction, since RFI is typically
narrow banded.
We have not yet determined a good real-time flagging algorithm, and are not
even sure if it is feasible to flag in real time.
The correlator can hold at most a few seconds of data in main memory, and
this is rather restrictive.
Also, it is not clear what is the best place (or places) in the pipeline to
insert a flagger; for example, the properties of a flagger that will flag
on correlated data will be quite different from the properties of a flagger
that will flag on filtered station data.

We would also like to implement some form of real-time calibration.
The idea is to analyze the visibilities and estimate new gains and phases
that are applied to the station input stream (somewhere between
\circlenumber{6} and \circlenumber{9}) in a feed-back loop.
Since the correlated output is produced with a latency of multiple seconds,
gain and phase corrections can only be applied to data of a couple of seconds
later.

Also, the latency of the correlator output may be too high to generate
triggers for the Transient Buffer Boards within time; they must be triggered
within roughly four seconds, otherwise the interesting data on the TBBs is
overwritten.
There are several ways to decrease the latency, some of which require quite
some programming effort.
We have not yet determined how much the latency should be reduced.

Another idea that we might implement in the future is to reduce the output
data rate by using hybrid integration times: short baselines can be integrated
longer than long baselines.
Especially for international observations with very different baseline lengths,
where short integration times are desired, this can significantly reduce the
amount of output data.
However, there are other options to reduce the output data rate with the
functionality that is already present, e.g., (groups of) core stations can be 
beam formed prior to correlation, and the channel bandwidth can be increased.
Hybrid integration times can only be used if the offline software support
this as well.
Although this is not the case, there are no fundamental reasons that make it
impossible to implement.
It would also be a good demonstrator for the Square Kilometer Array.


\section{Results}
\label{sec:results}

\begin{figure}[ht]
\includegraphics[width=\columnwidth,height=15mm]{fringe.jpg}
\caption{Fringes from a 9-hour observation.}
\label{fig:fringe}
\end{figure}

The system we described is used on a daily basis for observations, using the
currently available stations.
A graphical representation of the correlator output is depicted in
Figure~\ref{fig:fringe}.
The figure shows the cross-correlations from two of the stations used during a
9-hour observation.
The horizontal axis represents time;
the vertical axis represents the 256~channels of one frequency subband.
Each pixel corresponds to a correlation, where the color represents
the phase of the signal; the intensity matches the amplitude (power).
%The phase changes over time, due to the earth rotation that alters the
%relative position of the observed sources and thus the time difference
%between the two stations.
The white spots are caused by RFI.


\subsection{Computational Performance and I/O capabilities}

The application is highly optimized to achieve extremely high computational
performance and to sustain the high station data rates.
For example, all compute-intensive program parts are written in assembly
that optimally use the Blue Gene hardware.
C++ reference code is maintained as well, but is much slower.
The correlator achieves 96\% of the theoretical peak performance
of the floating-point units during the computational phase.
Other pipeline components also achieve very high performance.
A separate paper elaborates on the high-performance computing and scaling
aspects of the correlator~\cite{Romein:10a}.

\begin{table}
\begin{center}
\begin{tabular}{|l|...l|}
\hline
%bits/sample		& \multicolumn{2}{|c|}{2x16} & \multicolumn{2}{|c|}{2x8} & \multicolumn{2}{|c|}{2x4} \\ 
bits/sample		& \multicolumn{1}{c}{2x16} & \multicolumn{1}{c}{2x8} & \multicolumn{1}{c}{2x4} & \\
\hline
max.\ nr.\ stations	&   64 &  64 &  48 & \\
max.\ nr.\ subbands	&  248 & 496 & 992 & \\
obs.\ bandwidth * beams & 48.4 & 96.9 & 194 & MHz \\
nr.\ channels/subband   & \multicolumn{1}{c}{1,16--4096} & \multicolumn{1}{c}{1,16--4096} & \multicolumn{1}{c}{1,16--4096} & \\
min.\ integration time	& 1.0\ensuremath{^{1)}} & 1.0 & 1.0 & s \\
\hline
operations per second	& 3.91 & 7.81 & 9.34 & TFLOPS \\
total input bandwidth	&  198 & 198 & 149 & Gb/s \\
total output bandwidth	&   35.9 &  71.8\ensuremath{^{2)}} & 81.2\ensuremath{^{2)}} & Gb/s\\
\hline
\multicolumn{5}{l}{$^{1)}$ Integration times as low as 0.25~s are supported for smaller numbers of stations.} \\
\multicolumn{5}{l}{$^{2)}$ Exceeds current storage capacity.} \\
\end{tabular}
\end{center}
\caption{Correlator specifications, assuming one BG/P rack, 1.007~s integration
time, 256~channels, if not stated otherwise.
Maximum beam-forming capabilities still have to be determined.}
\label{tab:correlator-specifications}
\end{table}

The capabilities of the correlator are listed in
Table~\ref{tab:correlator-specifications}.
There are sufficient spare CPU cycles available to run additional observation
modes concurrently, but the output data rate is quickly exhausted.
The very good efficiency and high bandwidths obtained on the Blue Gene led
to the decision to increase the bandwidth from 32~MHz to 48.4~MHz, the
maximum data rate that the station FPGAs can handle.
This does not only increase the efficiency of the entire LOFAR telescope
by 50\%, it also allows full coverage of the interesting 30--78~MHz range
in a single observation, rather than two.


\section{Conclusions}
\label{sec:conclusions}

%The LOFAR telescope is essentially a distributed sensor network consisting
%of tens of thousands of simple receivers.
%Unlike dishes, the receivers monitor the entire sky, and multiple sources
%can be observed concurrently, while beams can be switched instantaneously.
%These abilities provide opportunities for new science in radio astronomy and
%particle physics.
%Multiple processing pipelines are required for the different observation types.

%To support the full potential of 
%The LOFAR correlator is evolving to a flexible and versatile application.

%Unlike previous generations of telescopes that typically used special-purpose
%hardware, the LOFAR correlator is a \emph{software\/} correlator.
The LOFAR correlator is a \emph{software\/} correlator, unlike previous
generations of telescopes that typically used special-purpose hardware.
The use of software increases flexibility, and flexibility is highly needed
to support the different observation types that make LOFAR a versatile
instrument.
However, the high data rates and processing requirements demand the use of
a supercomputer.
We use an IBM Blue Gene/P to filter, beam form, and correlate the data.
Meanwhile, the functionality is still enhancing, as more pipelines and
pipeline components are developed.
The standard imaging pipeline is fully functional; several pulsar pipelines
are mostly implemented and have been successfully used for observations; other
pipelines are being designed.
The software supports multiple concurrent observations, even of different
types.

The application is highly optimized and achieves extremely good performance.
This comes at a price, as some optimizations complicate the implementation
considerably.
In particular, the work distribution scheme and the transpose that
redistributes the data among the processor cores are complex.
But the good performance results allow processing of up to 64~stations on a
\emph{single\/} rack only, and allow observations of 48.4~MHz rather than the
required 32~MHz, significantly increasing the performance of the
\emph{entire\/} instrument.


\begin{acknowledgements}
We thank Ger van Diepen, Martin Gels, Marcel Loose, and Ruud Overeem
for their contributions to the LOFAR software, and many other colleagues
for their work on the LOFAR telescope.
We also thank Kamil Iskra and Kazutomo Yoshii from Argonne National Laboratory
for their work on the BG/P system software.
Bruce Elmegreen, Todd Inglett, Tom Liebsch, and Andrew Taufener from IBM
provided the support to optimally use the BG/P.

LOFAR is funded by the Dutch government in the BSIK program for
interdisciplinary research for improvements of the knowledge
infrastructure.  Additional funding is provided by the European Union,
European Regional Development Fund (EFRO), and by the
``Samenwerkingsverband Noord-Nederland,'' EZ/KOMPAS. Part of this work was
performed in the context of the NWO STARE AstroStream project.
\end{acknowledgements}


%\bibliographystyle{spbasic}
\bibliographystyle{plain}
\bibliography{this-issue,lofar}


\end{document}
