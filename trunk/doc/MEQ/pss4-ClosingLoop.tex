\documentclass[12pt]{article}

\oddsidemargin=-5mm
\oddsidemargin=-5mm
\textwidth=170mm

\title{PSS4: Closing the Loop}
\author{O.M. Smirnov}

\begin{document}
\sloppy

\newcommand{\url}[1]{{\tt #1}}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

  PSS4 is targeted to be the first ``full'' or ``real'' PSS, in the sense that
  it should provide a complete reduction cycle, and be accessible by users
  (i.e. users outside the development team). The motivation for this is
  outlined in JEN's PSS4 document \cite{PSS4}. In this document we will
  concentrate on the ``missing links'' (compared to PSS3), and try to build a
  model of how all the pieces will fit together.

\section{Overview of functional modules}

  This section provides a list of semi-decoupled modules that need to be
  present in PSS4, looks at their status as of PSS3, and suggests scope of
  development for PSS4. Note that I use the term ``module'' in a very loose
  sense here --  something like ``work sub-package'' would be more accurate but
  less wieldy.

\subsection{Data Sources [OMS, GvD]}

  The AIPS++ MS remains the current storage format for visibility data. The
  Octopussy/AppAgent version of PSS3 provides facilities for converting an MS
  into a stream of VisTile objects. This allows VisTile-aware modules to be
  plugged directly into an MS, or into an Octopussy data stream.

  \paragraph{PSS4 plans:} A MeqSpigot class needs to be developed, to plug
  VisTiles into MeqTrees.
  
  \paragraph{Coordination:} VisTile and related interfaces are maintained by
  OMS. Primary client is GvD, and to a lesser extent MAB. The current
  interface has stabilized; GvD and MAB have already been using it in their
  code, so little coordination is required.

\subsection{Global/Local Sky Model [OMS]}

  \paragraph{Current status:} none as such. Current code uses an AIPS++
  table containing a simple list of sources, which is created ``by hand'' for
  any given test MS. 

  \paragraph{PSS4 plans:} refer to \cite{GSM} for details. A Glish layer
  will be developed to provide region search, and access source lists as {\em
  defrecs} from which sub-trees representing source parameters can be directly
  created. All sources will have a fixed set of parameters a-la NEWSTAR, and
  their sub-tree representation will be hardwired into the GSM/LSM code.
  
  A quick-and-dirty initial version [OMS] will use AIPS++ tables for storage.
  In parallel, the D-team [KvdS, RO] will develop a database back-end, with a
  Glish/C++ interface. This ``production'' back-end will be swapped in at a
  later date. 
  
  \paragraph{Coordination:} Glish interface will be implemented by OMS.  All
  GSM-related code is seen from Glish only, so primary clients are JEN and MAB.
  This is a completely new interface, so a lot of coordination will be
  required. Developing the storage back-end requires coordination between OMS
  and the D-team.

\subsection{MEP Database [GvD]}

  \paragraph{Current status:} similar to GSM. Current code uses an AIPS++
  table containing a list of parameters, which is created ``by hand'' for
  any given test MS. 

  \paragraph{PSS4 plans:} Keep using AIPS++ tables for now [GvD],
  eventually develop a database back-end [KvdS, RO]. 
  
  \paragraph{Coordination:} GvD--KvdS--RO.

\subsection{The ``Solver'' [GvD]}

  (NB: JEN insists on a narrow definition of a {\em solver}; for lack of a
  better term, I'll use ``Solver'' (capitalized \& double-quoted) here to refer
  to that whole module which takes input data, builds trees, predicts, solves,
  and generates parameters and residuals.)

  \paragraph{Current status \& PSS4 plans:} This module is where the biggest
  overhaul will take place in PSS4. JEN's PSS4 paper \cite{PSS4} covers this
  whole subject in great detail, which I won't repeat here.

  \paragraph{Coordination:} Lots required. Most of the work will be done by
  GvD. JEN/MAB will be primary clients. OMS will provide data and control
  structure.

\subsection{Imaging [OMS]}

  \paragraph{Current status:} AIPS++ imager tool is called from Glish. 

  \paragraph{PSS4 plans:} minimal refinements.

  \paragraph{Coordination:} none.

\subsection{Automatic source finder [OMS]}

  \paragraph{Current status:} none.

  \paragraph{PSS4 plans:} Evaluate the {\tt image.findsources()} function in
  AIPS++. Write a Glish wrapper around it to manage point sources (see below).
  If findsources() is not sufficient, investigate more elaborate methods.

  \paragraph{Coordination:} OMS--JEN/MAB. New sources need to be added
  to MeqTrees and stored in the LSM/GSM. 

\subsection{UV Data Visualizer [MAB]}

  \paragraph{Current status:} working version can be plugged into any
  data source supported by AppAgents. 

  \paragraph{PSS4 plans:} Some refinement required, but no major new
  functionality. 

  \paragraph{Coordination:} OMS--MAB, minimal.

\subsection{Control, GUIs, Glue [JEN, OMS]}

  \paragraph{Current status:} In PSS3, Octopussy/AppAgents is used to
  control system execution from Glish. Any combination of modules may be
  started (possibly remotely, i.e., over several nodes), reinitialized, paused,
  restarted, etc. by using Glish calls. Changes in module status (status
  records) and processing events are propagated via publish/subscribe. A
  prototype control GUI encompassing these capabilities has been written [OMS].

  A version of the {\tt calibrater} Distributed Object is still supported. This
  maps a number of ``Solver'' functions to Glish methods.

  \paragraph{PSS4 plans:} discussed in detail below.

  \paragraph{Coordination:} more or less everyone.
  
\section{Operations \& Usage Patterns}

  Let's look at typical usage patterns for PSS4. These will be listed somewhat 
  chronologically (i.e. in typical order of execution).

  Note that the term ``user'' is used rather loosely here. All these operations
  will be available directly to the user via the Glish prompt (and optional
  GUIs), but in most cases, it will be some sort Glish script or GUI that's the
  ``user'' of the interface.

\subsection{Using the GSM/LSM}

  GSM/LSM operations are somewhat independent of the rest of the system, since
  the databases will be accessed entirely from within the end-user's Glish
  session. This is a brief overview; for full details see \cite{GSM}.

\subsubsection{Creating initial LSM}

  The first step is to extract a subset of the GSM pertinent to the observation
  being calibrated. See \cite{GSM} for details; following is some Glish code
  representing the operation.

\begin{verbatim}
include 'GSM.g'
GSM := attach_gsm([table_name]);   
query := [ ra=ra,dec=dec,radius=radius,other optional fields ];
lsm := GSM.extract_region(query,...);
\end{verbatim}

  This will create an LSM database and populate it with a region extracted from
  the GSM according to the query parameters.\footnote{Note that we could also
  provide utility functions for automatically forming a query record based on a
  given MS, etc.} The {\tt lsm} object can be used for all further LSM
  operations.

\subsubsection{Reusing an LSM}

  In future sessions with the same MS, a user could attach an {\tt lsm} object
  directly to an existing database:

\begin{verbatim}
include 'lsm.g'
# if the LSM exists in its own table
lsm := attach_lsm(tablename)
# ... or if the LSM lives inside the MEPdb
lsm := attach_lsm([mepdb]);
\end{verbatim}

\subsubsection{Extracting sources}

  Once an {\tt lsm} object is available, it can be used to obtain a {\em
  sourcelist}:

\begin{verbatim}
sourcelist := lsm.select_sources(sort_by='pol_stokes_i',subset=query); 
\end{verbatim}

  The sourcelist is list of records (record of records, in Glish), with each
  one representing a source. See \cite{GSM} for details. 

\subsubsection{Finding sources automatically}

  The imaging tools can be used to make a dirty map from the original MS. The
  automatic source finder can then be run on this dirty map to come up with a
  sourcelist:

\begin{verbatim}
include 'sourcefinder.g'
finder := sourcefinder(image);
sourcelist := finder.find_sources(...);
\end{verbatim}

  This sourcelist can be added to the LSM...
  
\begin{verbatim}
lsm.add(sourcelist,commit=T);
\end{verbatim}

  ...but the user shouldn't be forced to do it at this stage. The user may want
  to solve for the sources first, and only add those for which there is a 
  converging solution, or something. Hence, it should be possible to use a
  sourcelist for calibration on-the-fly, without going to the LSM.

\subsection{System Startup \& Control}
\label{sec:startup}

  PSS3 already implements a remote system startup \& control mechanism. On
  the C++ side, a generic binary called the applauncher (application launcher)
  is started on every node involved in the operation. Upon startup, the
  applaunchers contact each other and establish an OCTOPUSSY network, which
  handles publish/subscribe within and across nodes. Glish sessions can then
  start on any node, and plug into the network by including the {\tt
  octopussy.g} closure. In the simplest case, this just one {\em control
  session} started on the user's machine. 

  A Glish session can then start up various processing tasks on any node in the
  network by publishing the appropriate init messages. These tasks can be
  implemented in C++ (PSS3 tasks: solver, data repeater) or Glish (imager,
  solution visualizer). The configuration of each task is determined by its
  init record (part of the init message) sent from Glish. In particular, the
  init record provides message names for each task's input and output. This
  allows for great flexibility in tying tasks together into ``virtual
  pipelines''. The user can run a pre-fabricated script to start the system in
  some common configuration, or do it interactively (from the prompt or a GUI)
  to set up some specialized session.
  
  The tasks in the system can be controlled by publishing the appropriate
  messages. Normally, this is done from the Glish session, but it is also
  possible to ``glue'' output events from one task to control messages  for
  another task. (Useful examples will be presented below). Note that the user
  need not be exposed to the details of this mechanism. Instead, local {\em
  interface objects} are created in the Glish session. Internally, these
  objects translate their methods calls into control messages. This has already
  been implemented in PSS3, see, specifically, the {\tt app\_proxy} object and
  the {\tt solver} object derived from it. For example, calling

\begin{verbatim}
app_proxy.pause()
\end{verbatim}

  publishes a Pause message to the appropriate task. The task then receives
  this message and enters paused state, responding with a state message. The
  state message is caught by the app proxy object, and its internal state is
  updated accordingly. The app proxies can also automatically construct GUIs to
  show the task state, and provide GUI elements for task control.

  Note that this approach already supports functional parallelization. For
  example, a user may set up multiple solver sessions using one data source
  (MS), one or more imagers and visualizers to image the solutions on-the-fly,
  and control all of this from a single GUI.

  With a few minor refinements, this mechanism should serve us well in PSS4. 

\subsection{Running a Solve}
  
  This process is covered in great detail in \cite{PSS4}. Here we'll only
  concern ourselves with specifics of the data flow. 

\subsubsection{Starting up}

  The first step is to start up a ``Solver'' task. This is covered in section
  \ref{sec:startup}, above. Once this is done, we have a C++ module running
  somewhere on the network (perhaps the same node), which will respond to
  control messages published by interface object(s) defined in our Glish
  session. These objects will implement an interface along the lines of
  \cite{PSS4}. 

\subsubsection{Specifying inputs and outputs}

  The AppAgent framework developed for PSS3 provides an abstract mechanism for
  specifying a task's inputs and outputs. Processing tasks have VisInput 
  streams (DataRecord header, VisTile data, DataRecord footer), VisOutput 
  streams (similarly structured), and a control message stream. Different kinds
  of streams can be attached by specifying different {\em event sinks} in a
  tasks's init record. Available sinks in PSS3 include:

  \begin{itemize}
  
  \item An OCTOPUSSY sink (OctoSink). This subscribes to input messages, and
  publishes output messages. The message names are set up in the init record,
  which allows tasks to be tied into ``virtual pipelines''.

  \item MS sinks (MSInputSink, MSOutputSink). When used for input, this sink
  reads an AIPS++ MS, and turns it into a header record -- VisTiles -- footer
  record sequence. When used for output, this sink writes VisTiles into an MS,
  using information passed via the header record. The sink allows arbitrary
  mappings between VisTile columns and MS columns -- for example, the predict
  and residual columns of a VisTile (if available), could be mapped to
  MODEL\_DATA and CORRECTED\_DATA of an MS, or even written to a new column in
  the MS. 

  \item A BOIO sink. BOIO is a simple mechanism for reading and writing events
  to or from a file. A BOIO sink allows one to easily record an event stream,
  and play it back later. 

  \end{itemize}
  
  The abstraction provided by event sinks allows for a lot of flexibility in
  specifying the data streams in a system. Some examples:
  
  \begin{enumerate}
  
  \item Simple solver test: MSInputSink $\rightarrow$ Solver $\rightarrow$
  MSOutputSink.
  
  \item Simple parallel pipeline: MSInputSink $\rightarrow$ Solver
  $\rightarrow$ OctoSink $\rightarrow$ OCTOPUSSY $\rightarrow$ OctoSink 
  $\rightarrow$ DataRepeater $\rightarrow$ MSOutputSink. The Solver and
  DataRepeater tasks can run in parallel, on different CPUs or nodes.

  \item On-line imaging: same as the simple pipeline, but an extra OctoSink
  $\rightarrow$ DataRepeater $\rightarrow$ MSOutputSink component is added.
  This writes the output of the solver into a ``scratch'' MS. (OCTOPUSSY takes
  care of splitting the output of the solver into two duplicate streams to the
  two repeaters.) A Glish script can wait for end-of-data events from the
  second repeater, and run the AIPS++ imager on the scratch MS.

  \end{enumerate}
  
  For PSS4, the ``Solver'' task needs to be extended with a glue layer that
  connects input/output sinks to MeqSpigots and MeqSinks.

\subsubsection{Data sorting issues}

  In off-line calibration mode (where the complete MS is available on disk
  before reduction starts), if the input data set is small enough to completely
  fit in memory, then data access becomes a trivial exercise. A preloading loop
  could be executed; this would loop over the input data, and distribute it to
  appropriate MeqSpigots in all the trees. MeqSpigots would then retain all
  data in memory, and provide it a many times as requested; the data needs
  reading from disk only once. In principle, this is similar to the way it's
  currently being done in PSS3 (all visibility data is preloaded prior to any
  calibration).

  However, we should also design in support for larger MSs, and on-line
  calibration mode. In both cases, there is too much data to fit into memory at
  any one time, and random access is unavailable or prohibitively expensive (as
  is the case of large MSs). Trees must cope with input data just the way it
  comes in. We can, however, specify a particular sort order for the data, and
  assume that, within certain limits, it can be resorted for us somewhere up
  the pipeline (or, in the case of a MS, pre-sorted prior to operation). One
  ``hard'' limit is time interval -- time is always the outer sort axis in one
  way or another, because a pipeline can't accumulate time intervals of an
  arbitrary length. Therefore, we should be able to deal with short chunks of
  time (up to a single time slot, or shorter than the domain length, in any
  case).

  On the other hand, we are allowed to make several passes through the data (at
  least one pass per iteration). When working with an MS, this is trivially
  accomplished (by re-reading the file); in on-line mode, a separate task in
  the pipeline can work as a cache, and re-run through the data when needed. 

  Note that the PSS3 AppAgent framework already allows this, without
  implementing any sort of policy in the Solver task itself. The Solver
  publishes Start.Iteration and End.Iteration events at the start and end of
  any iteration. These events can be ``glued'' to control messages of the data
  source, causing it to automatically rewind and do another pass over the data.
  This allows all data handling policy to be specified from the Glish control
  layer.

\subsubsection{Data-driven trees}

  On-line processing requires trees to be data-driven. Since we can no longer
  assume that all MeqSpigots will be equally available (pre-loaded) in advance,
  we must allow the processing to be driven by the availability of data.

  In other words, MeqSpigots should load the data as it becomes available,
  notify their parent nodes, and flush the data once it has been processed (or
  at some later point in time -- see below). The data thus propagates up the
  tree (from leaves to root nodes), driven by availability. On the surface,
  this makes the tree implementation a lot more complicated, since it requires
  that child nodes have knowledge of their parents (and a child can have many
  parents). A more elegant solution involves moving this functionality into the
  data structure itself:

  \begin{itemize}
  
  \item Each node keeps a DMI DataRecord of its state. When a node becomes
  valid (for a leaf node, this means being loaded with data. For a node with
  children, this means all children becoming valid), it sets a ``valid'' flag
  in its state record.

  \item DataRecords (actually, DMI containers in general) are extended with 
  {\em data watchpoint} functionality. An object can set a data watchpoint --
  that is, ask to be notified (by having some method called) whenever some
  specific item in a container changes.

  \item Parents have access to their children's state records.
  
  \item A parent can then set watchpoints on its childrens' ``valid'' flags. 
  Thus, it will be notified when a child becomes valid. (It may prove to be
  easier to dispense with the ``valid'' flag and just have a data field in the
  state record, setting watchpoints on that data field.)

  \end{itemize}
  
  This moves some complexity from MeqTrees into DMI (since containers will have
  to maintain lists of callbacks, etc.) There is a big advantage in doing it
  this way, since instead of implementing specific functionality in MeqNodes
  (the child $\rightarrow$ parent notification mechanism), we'd be making a
  generic, system-wide data watchpoint mechanism. Note that data watchpoints
  are a well-developed concept that pops up in many high-level languages and
  software frameworks. They would enable many other capabilities elsewhere in
  the system:

  \begin{itemize}
  
  \item Monitoring \& control from Glish level. Rather than simply subscribing
  to events from tasks (which have to be explicitly published in the task --
  i.e., explicit publish() calls would need to be implemented in the task),
  control code could subscribe to specific changes in a task's data structure.

  \item Debugging. The hardest bugs to find are those that only pop up with
  specific data!\footnote{While all modern debuggers support watchpoints, these
  are limited to low-level variables, and have very limited utility with
  complex data structures.}

  \end{itemize}

  Another aspect of this problem is data caching. It may be sensible for a
  MeqSpigot (and MeqNodes in general) to cache its data, rather than flushing
  it as soon as it has been requested by a parent. Ideally, this should be
  driven by memory availability. This is a complex problem, and it remains to
  be seen how elaborate we want get in PSS4.

\subsubsection{Building trees}

  The user can create nodes and construct trees (on the C++ side) by calling
  methods in the interface object. See \cite{PSS4} for details. The nodes are
  defined via defrecs created in Glish. It is assumed that we'll produce
  pre-fab scripts to set up trees for various calibration modes and strategies,
  but the user can also customize the scripts, or even build trees ``by hand''.
  Note that LSM sources will be handled in the same manner \cite{GSM}, since
  the LSM will automatically provide the defrecs representing source
  parameters. 

\subsubsection{Running a solution}

  A Start.Solution message (initiated by calling a method in the interface
  object, or by using a GUI control) is published to start the proceedings. From
  that point on, the following things happen:

  \begin{itemize}
  
  \item The Solver task loads various solution control parameters from the
  Start message. These could optionally include end-criteria.

  \item The data source receives a Start message (this could be the same
  Start.Solution message, or an event generated by the Solver -- depending on
  how the system is configured from Glish) and begins to spit out visibility 
  data.
  
  \item The data is loaded into MeqSpigots and processed by trees. If someone
  (Glish control code \& GUIs, most likely) has subscribed to data structures
  in the Solver task, then an appropriate events are generated and passed to
  Glish (GUIs updated, etc.)

  \item At some point, a new solution becomes available. The Solver task
  publishes an End.Iteration message, containing various information on the
  solution (it could even include the entire set of solvable MeqParms, but not
  necessarily). If requested, the Solver could also spit out intermediate
  residuals and/or predict data, to be picked up by a visualizer elsewhere in
  the system (in fact, in PSS3 the Solver always posts intermediate data to its
  output AppAgent, but when no-one is subscribed to this data, the output agent
  does nothing. In this case, no data is copied around, so the performance
  impact is miniscule.) 

  \item Depending on the solution control parameters, the Solver task could end
  the solution \& stop (i.e. end criteria reached), or pause and wait for a
  Resume event, or automatically proceed to the next iteration. The user may
  always issue an End.Solution command (from Glish prompt or GUI) manually,
  causing the Solver task to stop at the next opportunity. 

  \item Various visualization tools (see, e.g., the Glish GUIs in PSS3) could
  pick up the End.Iteration message, and visualize intermediate solutions.

  \item If a new iteration starts, and another pass through the data is
  required, the data source receives the appropriate message (which could be a
  Start.Iteration event from the solver), and proceeds to spit out the data
  again. 

  \item If a solution ends (whether automatically, or in response to an
  End.Solution request from the user), the Solver task may also be told to
  either keep the solution (i.e. save modified MeqParms) or discard it. It may
  also be told to peel some sources and generate new visibility data, and to
  publish new residuals. In batch processing mode, this would be determined by
  fields in the {\em end-record}, which could be sent with the End.Iteration
  event, or specified as part of the end-criteria in the Start.Solution
  message. In interactive mode, the user could choose to do all this manually
  (after looking at the current solution), while the Solver is paused at the
  end of an iteration.

  \end{itemize}
  
  At the end of a solution, the Solver may produce a stream of residual data.
  This is sent to the output sink, and, depending on system configuration (as
  specified by the user and/or startup scripts), it may be discarded, or
  forwarded up the pipeline and/or written to an MS. If written to an MS, an
  End.Data message is published once all data has been written. 

\subsection{Imaging}

  Imaging is handled by the AIPS++ imager, coupled with some Glish control
  code. Once residual data has been written to an MS (by whomever), a message
  is published. The imager control script catches this event, and proceeds to
  run the imager on the MS. The result of this is an AIPS++ image file. An
  Image.Ready message is published once the process is complete.

  PSS3 already handles this quite competently, and no major changes are
  expected for PSS4. Note that functional parallelization can already be
  achived here. This has been demonstrated in the PSS3 demos:
  
  \begin{itemize}
  
  \item The Solver task is attached to an OCTOPUSSY output sink. The residual
  data is not written to an MS by the Solver task, but published to OCTOPUSSY. 

  \item Elsewhere on the network (could be the same node, but could be another
  node), a DataRepeater task catches this stream (via its own OCTOPUSSY input
  sink), and forwards it to its output sink, which is an MS. Once the full data
  stream has been written, the DataRepeater publishes an End.Data message and
  automatically pauses.

  \item The Glish control script catches the End.Data event, and proceeds to
  run the imager. Once the imager is done, the script sends a Resume message to
  the DataRepeater. The Pause/Resume mechanism protects the MS from being
  overwritten by a new batch of data while the imager is still reading it.

  \end{itemize}
  
  Note that the DataRepeater/imager tasks are running in parallel with the
  Solver. There is little added value when just imaging the results of a single
  solution (since the imager has to wait for the Solver to finish anyway).
  Things get a lot more interesting with ``online imaging'': the Solver can
  publish the residuals of one iteration, and proceed with the next. Meanwhile,
  the residuals are imaged and presented to the user. 

  If the Solver is spitting out data faster the imager can handle, OCTOPUSSY
  automatically acts as a buffer. In PSS3, it is up to the user to make sure
  that this buffer is not overextended (if your solve is too fast, maybe you
  shouldn't be imaging at every iteration). If found necessary for PSS4,
  automatic load control could also be implemented.

\subsection{Source finding}

  Once an image has been produced by the imager, we can run source finding on
  it. This could be done manually by the user, or automatically from a control
  script (either triggerred by the Image.Ready message, or as an intergral part
  of the imaging loop).

  Regardless of implementation details (AIPS++ source finder, or our own C++ 
  task), the interface to the finder is a Glish object. The input is an AIPS++
  image, and, optionally, a list of known sources; the output is a sourcelist
  (see above). In batch mode, the sourcelist would be published in a
  Sources.Found message.

  The sourcelist could then be:
  
  \begin{itemize}
  
  \item Processed in any way imaginable (that's what Glish is for, after all).
        
  \item Added to MeqTrees directly, for running another solution.
  
  \item Written to the LSM.
  
  \end{itemize}
  
  Since the sourcelist is published in a message, we have a lot of flexibility
  in deciding what to do with it. For example, if the user is running a GUI,
  the GUI could display that sourcelist, and provide controls for adding it to
  trees or committing it to the LSM. If running in batch mode, the global
  control script could catch the message and automatically initiate another
  solution. 
  
\section{Some Conclusions}

  Some design conclusions emerging from this discussion:
  
  \begin{itemize}
  
  \item All individual processing tasks should be controllable via Glish
  interface objects, instantiated in the ``control session'' (the user's Glish
  session, or a batch control script).

  \item Distributed tasks will have a C++ (or Glish!) implementation which
  could be running in another process or on another node. OCTOPUSSY is the
  communications medium. The distribution of tasks over the network is
  specified when starting the task, but from that point on, the user of the
  interface object does not need to know where the task is running. Note also
  that it should be possible to create an interface object and attach it to an
  already running task.

  \item The user can do everything from the Glish prompt, by calling individual
  methods in the interface object. So can a control script.

  \item Interface objects could optionally present their own generic GUI. 
  Alternatively, scripts could be written to build big integrated GUIs for
  control of multiple objects. 

  \item For all ``interesting'' occurences in a processing task, a message is
  published. All these messages could be subscribed to by the user via the
  interface object. The interface object will implicitly use some messages to
  update its internal state and/or its GUI -- but in any case all messages from
  the task should be available to the user of the object. (That ``user'' can
  also be a batch control script). 

  \item A task's output messages (``events'') could be glued to input messages
  (``controls'') of another task. This is specified when creating (or
  reinitializing) the tasks. Of particular interest here are various
  Start-Stop-Pause-Resume-End Iteration, etc. messages, as discussed above.
  This gives the user a lot of freedom in making individual modules work
  together. At the same time, we can provide pre-fab scripts for starting tasks
  in some common useful configurations.

  \end{itemize}

\begin{thebibliography}{99}

% alias for bibitem used here
\newcommand{\bibref}[1]{\bibitem{#1}}

\newcommand{\biburl}[1]{ {\tt\mbox{#1}}}

% creates a reference to an ADASS paper
% arguments: 1:label 2:author 3:year 4:title 5:volume 6:adass_roman_num
%            7:page 8:url
\newcommand{\adassref}[8]{\bibref{#1} #2 #3, {\em #4}, 
    in ASP Conf. Ser., Vol. #5 {\em(ADASS #6)}, #7,\biburl{#8}}

% shortcut for a references to specific ADASSes
% arguments: 1:label 2:author 3:title 4:page 5:url
\newcommand{\adassvii}[5]{\adassref{#1}{#2}{1998}{#3}{145}{VII}{#4}{#5}} 
\newcommand{\adassviii}[5]{\adassref{#1}{#2}{1999}{#3}{172}{VIII}{#4}{#5}} 
\newcommand{\adassix}[5]{\adassref{#1}{#2}{2000}{#3}{216}{IX}{#4}{#5}} 
\newcommand{\adassx}[5]{\adassref{#1}{#2}{2001}{#3}{238}{X}{#4}{#5}} 
\newcommand{\adassxii}[5]{\adassref{#1}{#2}{2003}{#3}{295}{XII}{#4}{#5}} 

\bibref{PSS4}Noordam, J.E.\ 2003, {\em Prototype Selfcal System 4 (PSS4)}, 
LOFAR-ASTRON-DOC-?????

\bibref{GSM}Smirnov, O.M. \ 2003, {\em Developing GSM Concepts in PSS4}, 
LOFAR-ASTRON-DOC-?????

\end{thebibliography}

\end{document}

