%
%  Copyright (C) 2007
%  ASTRON (Netherlands Foundation for Research in Astronomy)
%  P.O.Box 2, 7990 AA Dwingeloo, The Netherlands, seg@astron.nl
%
%  $Id$
%
\documentclass[10pt]{lofar}
%
%% I prefer Bookman over Palatino, but I need Kerkis for math support
%\usepackage{bookman} 
\usepackage{mathpazo}      %% Palatino with matching math fonts.
\usepackage{layout}
\usepackage{color}
\usepackage{xspace}
%\usepackage[colorlinks=false]{hyperref}
%
\newcounter{decision}
\newenvironment{decision}[1][Decision]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1 \refstepcounter{decision}\thedecision}]}{\end{trivlist}}
%
\newcommand{\todo}[1]{\begin{center}\fbox{\parbox{0.9\textwidth}{\textbf{!!FIXME!! #1}}}\end{center}}
\newcommand{\cs}[1]{\textsc{cs}\begin{footnotesize}#1\end{footnotesize}\xspace}
\newcommand{\bbs}{\textsc{bbs}\xspace}
\newcommand{\lofar}{\textsc{lofar}\xspace}
\newcommand{\ms}{\textsc{ms}\xspace}
\newcommand{\me}{\textsc{me}\xspace}
\newcommand{\lsm}{\textsc{lsm}\xspace}
\newcommand{\aips}{\textsc{aips}\begin{footnotesize}++\end{footnotesize}\xspace}
\newcommand{\olap}{\textsc{olap}\xspace}
\newcommand{\meqtree}{\textsc{m}\begin{footnotesize}eq\end{footnotesize}\textsc{t}\begin{footnotesize}ree\end{footnotesize}\xspace}
%\newcommand{\meqtree}{MeqTree\xspace}
\newcommand{\predict}{\textsc{predict}\xspace}
\newcommand{\solve}{\textsc{solve}\xspace}
\newcommand{\subtract}{\textsc{subtract}\xspace}
\newcommand{\correct}{\textsc{correct}\xspace}
%
\include{definitions}
%
\begin{document}
\maketitle
\newpage
%
\theDistributionList
\vspace{1cm}
\theDocumentRevision
\newpage
%
\begin{abstract}
\centering
\includegraphics[width=0.8\textwidth]{images/dt20070317}
\end{abstract}
\newpage
%
\tableofcontents
\newpage
%
\bibliographystyle{unsrt}

\section{Introduction}
\label{sec:introduction}

\subsection{Purpose of This Document}
\label{subsec:purpose}
%
\textsc{aips++}
\aips
\meqtree
This document provides a detailed description of the architectural and
software design of the Blackboard Selfcal System (BBS) that will be used for
the off-line calibration of the LOFAR observations. The primary goal of this
document is to provide information that is detailed enough to help the reader
understand the design considerations, choice of software architecture and
global design. We will not delve into the level of detailed software design,
since this will likely cause discrepancies between the actual code and this
document. For this level of detail, the reader is suggested to consult the
online code documentation.

\subsection{Executive Summary}
\label{subsec:summary}

\subsection{Abbreviations}
\label{subsec:abbrev}
\begin{description}
\item [BBS] BlackBoard Selfcal System
\end{description}

\pagebreak

\section{Architectural Design}
\label{sec:architectural-design}

\subsection{Design Considerations}
\label{subsec:considerations}
The BlackBoard SelfCal (BBS) system is designed to do the calibration of LOFAR
in an efficient way. Although BBS is mainly developed for LOFAR, it may also be
used to calibrate other instruments as soon as their specific algorithms are
plugged in.

\subsubsection{Data Volume}
\label{subsubsec:data-volume}
The volume of the data coming from the LOFAR correlator is \emph{very} large.
During initial operation (mid 2008) the amount of data generated during an
average observation will be in the order of several terabytes. Once LOFAR is
fully operational, this number will have increased to almost a hundred
terabytes.  Given the output data rate of the correlator and the storage
capacity of harddisks, it is obvious that the data cannot be stored on a
single system, not even if an array of harddisks were used. The only feasible
way to handle these large data sets is to distribute them to multiple
computers. Each computer will have to store and manipulate part of the data.

\subsubsection{Distributed Processing}
\label{subsubsec:distributed-processing}

The Selfcal application will be running on the off-line and auxiliary
processing clusters of the central processing facility (see
\cite{LOFAR-ASTRON-ADD-012}). These clusters consist of Linux PCs in a high
bandwidth network. The BBS application will typically run on 50 to 500 of these
nodes. Data stored in the CEP intermediate storage facility will be distributed
over multiple disks and will be accessed by multiple nodes concurrently. 
Reordering tens of terabytes of data takes a lot of time and should be avoided.
Therefore the data should be distributed such that the various applications
(e.g., calibration and imaging) can operate well without reordering. The
distribution should be such that large chunks of data can be
processed locally and only small amounts of data need to be sent to other
machines. There are a few axes along which the data may be distributed:
\begin{description}
\item [Time] is a bad candidate, because a time slot contains a lot of data (up
to 0.7~GBytes during initial operation). This may lead to problems in the
online system when all data of a time slot are sent to a single machine and
written there. Another problem is that parallelization of imaging gets hard
because the data of all time slots have to be combined.
\item [Baseline] seems a better candidate, but will lead to imaging problems as
well. This is because a single image needs data from different machines, so
large amounts of gridded or FFT-ed data have to be sent around.
\item [Frequency] seems to be the best candidate. Creating an image is usually
done per channel or for a few channels, so in principle the whole imaging
process can be done locally. It will result in an image cube distributed over
many machines, so the image display and analysis software have to be able to
handle this. The image cube can be very large (e.g., 256~GBytes for
1000~channels of $4000 \times 4000$ images for the 4~Stokes parameters). \\
Distribution in frequency means that each subband is stored on a separate
machine.  If needed, each subband can be distributed further. Of course, each
machine should contain about the same amount of data to get good load
balancing. \\ 
Note that this distribution matches well with the way the correlator and
online system is designed.
\end{description}
The BBS calibration software is not dependent on a specific distribution, so
in the future other distributions can be used when applicable. However, it has
not been decided yet if that is also true for the imaging software.

\subsubsection{Scalable Architecture}
\label{subsubsec:scalable-architecture}
One important requirement is scalability. In order to avoid any performance
bottlenecks, unnecessary coupling between the different computing nodes should
be avoided as much as possible. When distributing data over frequency, we can
almost completely decouple the computing nodes, as we saw in the previous
section. Another way to reduce coupling is to make communication indirect as
well. Computing nodes should communicate through some kind of global shared
memory. There are several architectural patterns that describe this approach. 
One of the oldest and best known is the Blackboard pattern, which we will 
describe briefly below.

Computing nodes should communicate through some kind of global shared
memory. One obvious candidate for such shared memory is a database system. It
provides locking and notification (trigger) mechanisms, and sometimes even
command queueing.

\subsection{Blackboard Pattern}
\label{subsec:blackboard}
The idea behind the Blackboard architecture is a collection of independent
processes that work cooperatively on a common data structure. Each program is
specialized in handling a particular part of the overall task, and all
programs work together on the solution. These specialized programs work
independent of each other. They do not call each other, nor is there a
predetermined sequence for their activation. Instead, the direction taken by
the system is mainly determined by the current state of progress. A central
control component evaluates the current state of processing and coordinates
the specialized programs. This data-directed control regime makes
experimentation with different algorithms possible, and allows experimentally
derived heuristics to control processing. This architecture is described in
\cite{Buschmann1996} and
\cite{LOFAR-ASTRON-SDD-002}.

The Blackboard architecture is ideal for solving problems for which no
predetermined algorithm or solve strategy is known. However, for the design of
the BBS system, we've come to the conclusion that the operational system will
benefit in terms of performance when using a predefined solving strategy. The
"best" algorithm to perform a self-calibration run can be chosen from a
relatively short list of calibration strategies in advance (based, e.g., on
heuristics, or suggested by research done with the \meqtree system).

\begin{figure}[!ht]
\centering
\fbox{
  \begin{picture}(440,110)(-20,-3)
  \footnotesize
  \thicklines
  \put(0,50){Shared Repository pattern}
  \put(100,61){\vector(2,1){40}}
  \put(100,53){\vector(1,0){40}}
  \put(100,45){\vector(2,-1){40}}
  \put(150,80){Controller pattern}
  \put(150,50){Blackboard pattern}
  \put(150,20){Repository Manager pattern}
  \put(230,57){\vector(3,1){40}}
  \put(230,49){\vector(3,-1){40}}
  \put(280,68){Blackboard-based Control pattern}
  \put(280,32){Hierarchical Blackboard pattern}
  \end{picture}
}
\caption{Patterns based on the Shared Repository pattern, after Lalanda~\cite{Lalanda1998}.}
\label{fig:shared-repository-pattern}
\end{figure}

In fact, the Shared Respository pattern~\cite{Lalanda1998}, which can be seen
as a generalization of the Blackboard pattern, is probably a better match for
the BBS system. It realizes indirect communication using a repository as
shared memory. Figure~\ref{fig:shared-repository-pattern} show the
specialization hierarchy of patterns based on the Shared Repository pattern.

For BBS, we will need a global controller, which could be implemented using
the Controller pattern; and a notification or trigger mechanism to inform the
computing nodes of changes to the shared memory, which could be implemented
using the Repository Manager pattern. The shared memory is used as the common
knowledge base for the self-calibration process, and will be implemented as a
database. Using a database system has the advantage that locking, notification
(trigger) and sometimes even command queueing mechanisms are provided
out-of-the-box.

The database will be separated into two parts. One part will contain a list of
commands (or work orders) to be sent to each computing node. The other part
will contain the values and quality of the (partial) solutions calculated by
each computing node. The database can be used as an external source for
various assessments of the solutions.

%
%\subsubsection{Controller}
%\label{subsubsec:controller}
%
%\subsubsection{Knowledge Sources}
%\label{subsubsec:ks}
%
%\subsubsection{Blackboard}
%\label{subsubsec:bb}
%

\pagebreak

\section{System Overview}
\label{sec:overview}

\subsection{Subsystems}
\label{subsec:subsystems}
BBS is split into two parts which are described in detail in other
chapters. The BBS Control takes care of the distributed processing by means of
the Blackboard pattern. The BBS Kernel does the actual processing; it executes
a series of steps where each step consists of an operation like solve or
correct.

\subsubsection{BBS Control}
\label{subsubsec:sys-control}

The BBS Control subsystem is responsible for controlling the execution of a
self calibration strategy. A strategy consists of an ordered list of commands,
which will be executed by the BBS Kernel subsystem.

The key idea is that a subset of the data (the so-called "work domain") is
kept in memory; as many commands as possible are executed on these data before
the next data chunk is accessed. A strategy defines the size of the work
domain (in time and frequency) and optionally which stations and correlations
are contained in the work domain. It is also possible to define an integration
interval in time and frequency to achieve that, say, a longer time interval
can be used. The basic concept is that on each machine the data contained in
the work domain have to fit in memory. The BBS Kernel iterates over the work
domains to process all the data.  For each strategy a number of steps can be
defined. For instance, when peeling 10 Cat I sources, 20 steps can be
defined. For each source step 1 is solving for the gain in the direction of
the source and step 2 is subtracting the source. Note that only after the last
subtraction the residual data need to be written.  In this way the data are
read and/or written only once per strategy.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.6\textwidth]{images/bbs-control-global-design}
\caption{Global design of the BBS Control system}
\label{fig:bbs-control-global-design}
\end{figure}

The calibration process is controlled by the BBS Control
subsystem. Figure~\ref{fig:bbs-control-global-design} depicts the general
control structure. The BBS Control subsystem consists of one global
controller, which acts as the main process, and multiple local controller,
which control the BBS Kernel subsystem. The global controller posts one or
more commands (steps) to the Command Queue. Each local controller fetches the
next command from the Command Queue and forwards the command to the BBS Kernel
subsystem. The kernel returns parameter solutions and their quality metrics to
the local controller, which, in turn, posts the results to the Parameter
Solutions database. The global controller inspects the results and decides
which action should be taken next.

Since all communication takes places via the Blackboard, there is no need for
a direct connection between the BBS Control and the BBS Kernel subsystems.
The Blackboard contains all the relevant information about the current state
of the self calibration process. This information that can be used by other
(external) processes to monitor the calibration process and to plot results.
See~\cite{LOFAR-ASTRON-SDD-002} for more details on the Blackboard
architecture and roles of the controller.


\subsubsection{BBS Kernel}
\label{subsubsec:sys-kernel}

\subsubsection{BBS Database}
\label{subsubsec:sys-database}

\subsection{Interfaces}
\label{subsec:sys-interfaces}

\subsubsection{Context Diagram}
\label{subsubsec:context}

\subsubsection{BBS Control}
\label{subsubsec:interf-control}

\subsubsection{BBS Kernel}
\label{subsubsec:interf-kernel}
\begin{itemize}
\item MS
\item ParmDB and LSM
\item ACC
\end{itemize}

\subsubsection{BBS Database}
\label{subsubsec:interf-database}

\pagebreak

\section{Software Design}
\label{sec:software-design}

\subsection{BBS Control}
\label{subsec:design-control}

\subsubsection{BBS Strategy}
\label{subsubsec:design-strategy}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.6\textwidth]{images/bbs-strategy-class-diagram}
\end{figure}

\subsubsection{BBS Step}
\label{subsubsec:design-step}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.6\textwidth]{images/bbs-step-class-diagram}
\end{figure}

\subsubsection{Global Control}
\label{subsubsec:design-global-control}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{images/bbs-global-control-activity-diagram}
\end{figure}

\subsubsection{Local Control}
\label{subsubsec:design-local-control}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.9\textwidth]{images/bbs-local-control-activity-diagram}
\end{figure}


\subsection{BBS Kernel}
\label{subsec:design-kernel}
\subsubsection{Some basic principles and the relation with MeqTrees}
\label{subsubsec:design-principles}
BBS and the MeqTree system are based on the same PSS (Prototype Selfcal
System) implementation, but have gone their own way with their own specific
goal in mind. MeqTree has grown to a very flexible system making it possible
to try out all kinds of calibration algorithms. It is meant as a learning
environment to develop new algorithms that will eventually be implemented in
BBS. BBS has grown to a system where great attention has been paid to
performance at the cost of some flexibility. Distributed and parallel
processing have been designed in.

Although BBS and MeqTrees have gone their own way, they still share the same
principles:
\begin{enumerate}
\item The model (MeasurementEquation) is represented by an expression tree (in
fact, a separate tree for each baseline). BBS and MeqTrees differ in the way
the trees are defined. In MeqTrees the user has a lot of freedom by the
ability to define the trees at the scripting level. In BBS only some
predefined, highly tuned trees can be used.
\item Parameters form the leafs of the expression trees. They are represented by
funklets (2-dimensional functions, usually polynomials) that are valid for a
given frequency-time domain. A constant value is regarded as a zeroeth-order
polynomial. Both instrumental and sky parameters can be used.  In MeqTrees
functions with more than 2 dimensions can be used. For example, beam shapes as
a function of frequency, time, l, and m. It is not clear if that is needed in
BBS.
\item The calibration process consists of adapting the model parameters to the
observed UV data by improving the values of the solvable model parameters (in
fact, the coefficients of the parameter functions). This is done using a
solver (least squares fitter). It needs the derivatives of the solvable
parameter coefficients which are calculated in a numerical way.
\item Because the problem is non-linear a non-linear solver is used requiring
multiple iterations. The iteration process is stopped when the solution has
converged sufficiently or when a maximum number of iterations is done.  The
user can choose to solve for any combination of parameters. The solve is done
in frequency-time domains of a given size and resolution. For example, one
could solve ionospheric phases for each minute and solve source fluxes for the
entire time range.  Note that another way of iteration is alternating between
groups of parameters to solve for.
\item Peeling and phase shifting the data is a means to reduce the number of
calculations needed in the predict of the sources. It makes it possible to
predict the sources near the new phase center on a coarser grid.
\end{enumerate}

\subsubsection{Measurement Equation Evaluation}
\label{subsubsec:design-me-evaluation}
The Measurement Equation (\me) \cite{LOFAR-ASTRON-ADD-015} is a parameterized model of an interferometer in terms of a model of the sky, environmental effects (e.g. ionosphere), and antenna based instrumental effects.

Both \bbs and \meqtree represent the \me as a directed acyclic graph, although it is commonly referred to as \emph{tree}. We will stick with this tradition. The nodes of the tree represent atomic expressions, while the branches represent dependencies between atomic expressions. Compound expression can be build by combining several nodes into a (sub)tree. The leaf nodes represent the parameters of the expression, e.g. source positions and Stokes vectors. As an example, figure \ref{fig:expr_tree} shows a subtree for computing the visibilities produced by two linearly polarized point sources at different positions.

\begin{figure}[!ht]
\label{fig:expr_tree}
\centering
\includegraphics[width=\textwidth]{images/expr_graph_sdd.ps}
\caption{This figure shows a subtree for computing the visibilities produced by two linearly polarized point sources at different positions. The leaf nodes, i.e. the parameters of the expression, have been colored grey.}
\end{figure}

\meqtree is very flexible in the expression trees that a user can specify. The flexibility is due to a powerful Python language extension (\textsc{TDL}) that allows easy construction of a tree from an arbitrary expression, and to the high granularity of the nodes. The granularity of the nodes used in \bbs tends to be lower. For example, in \meqtree a typical node implements the sine function, whereas in \bbs a single node might implement a complete beamshape model. Because of the granularity of the nodes used in \bbs, the expression trees are generally small. Therefore, the overhead of traversing the trees is small as well. Note that, if necessary, \bbs could be made more flexible by adding nodes that are less specialized. By the same reasoning, the overhead of tree traversal in \meqtree could be reduced by creating specialized nodes.

Specification by the user is limited to selecting predefined subtrees to include in the \me. Currently implemented subtrees are:
\begin{itemize}
\item Prediction of a (possibly polarized) point source
\item Complex gain per station (G-jones)
\item Complex gain in the direction of a source, per station (E-jones)
\item Bandpass (B-jones)
\end{itemize}

\bbs will automatically generate a complete expression tree based on the selected predefined subtrees. For example, it will instantiate a subtree for each selected source in the local sky model and will include these at the appropriate place in the \me tree for each selected interferometer (baseline).

\todo{ref. Kjeld's document for a description of the various models}

\subsubsection{Model Parameters}
In order to evaluate an expression tree, the kernel needs to know the values of the model parameters (leaf nodes). Typical examples of model parameters are: source position and Stokes vector, complex station gain, ionospheric phase shift. The total number of parameters in the expression trees of the entire \lofar array will depend on the number of sources that are included in the sky model. Several thousands of parameters seems to be a realistic estimate \cite{LOFAR-ASTRON-SDD-052}.

Of course, most parameters are variable in e.g. frequency, time, and/or direction. Therefore, each parameter is represented by a set of values with an associated support (or validity domain). Figure \ref{fig:parameters} shows an example. The current implementation is limited to 2-D supports (frequency, time). Variability in direction can be achieved by creating a separate parameter for each direction of interest. This is done for the E-jones, where a separate parameter is used for each combination of station and source. True 4-D supports (frequency, time, l, m) may be required in the future to allow modelling of the beam shape.

A parameter value with an associated support is also called a \emph{funklet}. In the current implementation, funklets are 2-D polynomials of arbitrary degree. This allows some variability when solving for the value of a parameter on a large support. For example, station phase typically varies on time scales of a few seconds or less. This suggests solving for constant funklets with small support. Yet station amplitude typically varies on much coarser time scales, so we may try to solve for second degree funklets with a larger support. Of course, it this case we make the implicit assumption that station amplitude can be approximated as a second degree polynomial in time. The quality of the solution will depend on the extent to which this assumption is valid. The use of alternatives to polynomials, such as splines and shapelets is currently being investigated by the \meqtree team.

\todo{insert figure to clearify supports, parameters, funklets}

\paragraph{Naming scheme}
Because of the large number of parameters, keeping track of them all becomes a challenge. A naming scheme can be used to simplify this task. The naming scheme achieves two goals:
\begin{enumerate}
\item Identification of groups of related parameters
\item Specification of initial values
\end{enumerate}

Each parameter is assigned a unique name, consisting of several parts separated by colons. For example:
\begin{table}[htbp]
\centering
\begin{tabular}{lp{0.60\textwidth}}
\hline
\textbf{Identifier} & \textbf{Referenced parameter(s)} \\
\hline
\texttt{ra:3C343} & Right ascention of source 3C343 \\
\hline
\texttt{dec:3C343} & Declination of source 3C343 \\
\hline
\texttt{gain:11:phase:CS10:3C343.1} & Phase of the x-polarized signal from station CS10 in the direction of source 3C343.1 \\
\hline
\texttt{gain:11:phase:*:3C343.1} & Phase of the x-polarized signal of all stations in the direction of source 3C343.1 \\
\hline
\texttt{gain:\{11,22\}:phase:*:3C343.1} & Phase of both the x- and the y-polarized signal from station CS10 in the direction of source 3C343.1. \\
\hline
\end{tabular}
\end{table}


The current Prediffer implementation can handle parameters as follows:
At the first iteration it reads the parameters from an NFS-visible Berkeley DB (BDB) database or AIPS++ table.
The Prediffer can handle the updated parameter values in 3 ways:
Read from an NFS-visible database or AIPS++ table.
Read from a replicated BDB database.
Receive from the Controller.
Given the amount of parameters and domains, searching parameters and domains in the database can take some time. Optimization of the parameter data structure and evaluation of high-performance distributed embedded databases are ongoing.

The parameters are used in three places:
Each Prediffer needs parameter values to calculate the model.
Each Solver calculates new values for solvable parameters.
The controller writes the new parameter values to the distributed database and on the blackboard.

Before the first solve iteration a Solver receives the current values of the solvable parameters from the Prediffers. After each iteration it sends the newly calculated values to the Controller. So a Solver does not need access to the parameter database itself. In other words, the Prediffer nodes access the parameter database and sent self-contained inputs to the solver(s).
The AIPS++ LSQFit class that is used in the solver, keeps data from previous solutions within the same iteration series to speed up the convergence process. This is used to minimize the data size that is sent from Prediffer nodes to the solver(s).

At the start of the first iteration in solving a group of parameters, a Prediffer needs the values of all model parameters, not only the solvable ones. It needs them for the domain being solved. There are two approaches to achieve this:
1.A Prediffer reads the parameters from the database. This requires that the database is visible on the Prediffer machine.
2.The Controller reads the parameters and sends them to the Prediffers.
The problem with this approach is that the Controller does not know which parameters are needed by the Prediffers. A possible solution for this is that all Prediffers tell the Controller which parameters they need and get the values back. Because most Prediffers will need the same parameters, the Controller does not need to read too many parameters.
In the second and later iterations a Prediffer only needs the new values of the solvable parameters. This can be done similarly; either the Controller sends the new values to the Prediffers or the Controller updates the database after which a Prediffer can read the new values.

If a Prediffer reads the parameter values itself, the following database options are possible:
1.An AIPS++ table or embedded database on an NFS-mounted file system.
2.An embedded database that is (automatically) replicated on all machines.
3.A database server that is visible from all machines.

The parameter handling classes have been designed such that they hide the parameter handling from the other code. In this way it is rather easy to choose the parameter handling scheme that suites best. E.g., on Blue Gene/L another scheme might be used than on a Linux cluster.

The parameters form the leafs of the expression trees. They can be 2-dimensional functions of any degree valid for a given frequency-time domain. The domain is scaled to the domain [0,1] to avoid precision errors during the evaluation of the polynomials.
BBS can solve for any combination of the parameter coefficients used in the algorithms.

Each parameter has a unique name. The name is made up of a few parts separated by dots.
For example,
RA.CP1          the RA of point source 1
gain11.SR1.SG2      the XX gain of station 1 in the direction of source group 2
This naming scheme is used to make it easily possible to group parameters (using UNIX-like wildcarding) when specifying solvable parameters.
For example,
gain11.*.SG2        the XX gain of all stations in the direction of source group 2

%It is hard to say how many parameters the entire LOFAR calibration model will consist of as it depends heavily on the number of sources used. It is estimated to be several thousands.

Filling the parameter database
To be able to start a calibration run, the parameter database has to be filled with initial values. In principle, a value has to be present each parameter and frequency-time domain. However, it is possible to have default values for all parameters. The naming scheme is used to allow for defaults for groups of parameters.
E.g. a default value for parameter ‘gain11’ will be used for all parameters starting with gain11 if no more specific value is found.
Of course, the calibration process will generate specific entries when a solution is found for a specific parameter.

Cross calibration
Cross calibration can in principle easily be done by copying the parameters found for, say, a calibration observation to the parameter table of the other observation. If a calibration observation before and after the actual observation is used, it is easy to convert the parameters to a polynomial in order to use interpolation when calibrating the actual observation.
Currently there is no support for these operations; it is all handwork.


\subsubsection{Solver}
\label{subsubsec:design-solver}
Solvable parameters
In a BBS run it is possible to do a joint solve for any combination of parameters. There is a limitation that the frequency-time domains of the solvable parameters have to be the same.
The number of solvable parameters in a run can vary a lot. For two reasons it should be kept as low as possible:
1.A derivative has to be calculated for each solvable parameter. It means it is better to solve for constant values in small domains than for polynomial coefficients in larger domains. Later it is always possible to fit the constant parameter values to a polynomial (or any other function) on a larger domain.
2.Inverting the solver matrix is a $O(N^3)$ problem. As the size of the fitter matrix is determined by the number of solvable parameters, it is clear that the number should be kept as low as possible.
It means that it makes no sense to combine multiple domains in a single solve, because each domain has its own set of parameter values.

Some examples of parameters to solve are:
Ionospheric phases for a patch in the sky require 74 parameters per patch (for 37 stations).
To keep the number of parameters low it is best to solve patch by patch. In  it is argued that this approach is also best in peeling the visibility onion.
Point source fluxes and/or positions require 1-6 parameters per source assuming that no polynomial or so is used to represent the fluxes.

\subsection{BBS Database}
\label{subsec:design-database}

\subsubsection{Data Model}
\label{subsubsec:design-data-model}
\begin{figure}[!ht]
\includegraphics[width=\textwidth]{images/blackboard-datamodel}
\caption{Data model of the Blackboard database}
\end{figure}

\subsubsection{Work Orders}
\label{subsubsec:design-work-orders}

\subsubsection{Parameter Solutions}
\label{subsubsec:design-parmsolutions}

\pagebreak

% References
\bibliography{lofar}

\pagebreak

\appendix
\section{Configuration Syntax}

This appendix describes the syntax of the BBS configuration file (a.k.a.
parset). Its goal is to foster a common understanding and terminology. At the
moment this page is still under construction. I've added
\textcolor{red}{questions in red} to things that were not clear to me while
creating this page. Thing to do are \textcolor{green}{stated in green}.

\subsection*{Global Settings}
\begin{description}
\item [DataSet] : \emph{string} \\
    Path to the input measurement set.
\item [BBDB] : \emph{BBDB} (see page \pageref{app-bbdb}) \\
    Information about the black board database.
\item [ParmDB] : \emph{ParmDB} (see page \pageref{app-parmdb}) \\
    Information about the parameter databases (e.g. instrument parameters,
    local sky model).
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
DataSet                  = "test.ms"    # name of Measurement Set

BBDB.Host                = "127.0.0.1"  # hostname/ipaddr of BB DBMS
BBDB.Port                = 12345        # port used by BB DBMS
BBDB.DBName              = "blackboard" # name of the BB database
BBDB.UserName            = "postgres"   # username for accessing the DBMS
BBDB.PassWord            = ""           # password for accessing the DBMS

ParmDB.Instrument        = "test.mep"   # instrument parameters (MS table)
ParmDB.LocalSky          = "test.gsm"   # local sky parameters (MS table)
\end{verbatim}
}

\subsection*{Strategy}
A strategy consists of one or more (multi-)steps with an associated work domain
size and optional data integration.
\begin{description}
\item [Steps] : \emph{vector$<$string$>$} \\
    The names of the steps that compose the strategy. It is an error to leave
    this field empty.
\item [Stations] : \emph{vector$<$string$>$} \\
    Names of the participating stations. All stations will be used if this
    field is left empty.
\item [InputData] : \emph{string} \\
    Name of the column in the measurement set that contains the input data.
\item [Correlation] : \emph{Correlation} (see page \pageref{app-correlation}) \\
    Specifies which correlations to use.
\item [WorkDomainSize] : \emph{DomainSize} (see page \pageref{app-domainsize}) \\
    Size of the work domain in frequency and time. A work domain represents an
    amount of input data that is loaded into memory and processed as a single
    block.  A large work domain size should reduce the overhead due to disk
    access.
\item [Integration] : \emph{DomainSize} (see page \pageref{app-domainsize}) \\
    Cell size for integration. Allows the user to perform operations on a
    lower resolution, which should be faster in most cases.
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
Strategy.Steps                 = ["MultiStep", "SingleStep2"] \
                                                # (multi-)steps that compose this strategy
Strategy.Stations              = [ 0, 1, 2, 3 ] # ID's of stations to use
Strategy.InputData             = "INDATA"       # MS input data column
Strategy.Correlation.Selection = ALL            # one of AUTO, CROSS, ALL
Strategy.Correlation.Type      = ["XX", "YY"]   # which (cross)correlations to use
Strategy.WorkDomainSize.Freq   = 1e+6           # work domain size: f(Hz)
Strategy.WorkDomainSize.Time   = 10             # work domain size: t(s)
Strategy.Integration.Freq      = 1              # integration interval: f(Hz)
Strategy.Integration.Time      = 0.1            # integration interval: t(s)
\end{verbatim}
}

\subsection*{Step}
A \emph{single-step} describes one unit of work of the strategy. A step that
is defined in terms of a number of other steps is known as a multi-step. The
attributes of a \emph{multi-step} should be interpreted as default values for
the steps that compose the multi-step. These default values can always be
overridden.
\begin{description}
\item [Steps] : \emph{vector$<$string$>$} \\
    The names of the steps that compose this step (for multi-steps), or absent
    (for single steps).
\item [Baselines] : \emph{Baselines} (see page \pageref{app-baselines}) \\
    Baselines to use.
\item [Sources] : \emph{vector$<$string$>$} \\
    Sources to use. All sources will be used if this field is left empty.
\item [ExtraSources] : \emph{vector$<$string$>$} \\
    Additional sources to include when predicting visibilities. If this field
    is left empty, no extra sources will be included.
\item [Correlation] : \emph{Correlation}  (see page \pageref{app-correlation}) \\
    Specifies which correlations to use.
\item [Integration] : \emph{DomainSize}  (see page \pageref{app-domainsize}) \\
    Cell size for integration. Allows the user to perform operations on a
    lower resolution, which should be faster in most cases.
\item [InstrumentModel] : \emph{vector$<$string$>$} \\
    The parts of the measurement equation that should be included. \par
    \textcolor{green}{TODO: add descriptions for the various parts of the ME.}
\item [Operation] : \emph{string} \\
    The operation to be performed in this step. One of SOLVE, SUBTRACT,
    CORRECT, PREDICT, SHIFT, or REFIT. Only relevant for single steps, should
    be absent for multi-steps. \par\
    SOLVE : Find values for the parameters that minimize the difference
    between the predicted and the measured (u,v) values. \par
    \textcolor{green}{TODO: add descriptions for other values.}
\item [OutputData] : \emph{string} \\
    Column in the measurement set wherein the output values of this step
    should be written. If left empty, no data will be written.
\end{description}

\emph{Single steps should define one of the following fields, depending on the
value of \textbf{Operation}} :
\begin{description}
\item [Solve] : \emph{Solve} (see page \pageref{app-solve}) \\
    Arguments of the SOLVE operation. \par
    \textcolor{green}{TODO: specify arguments for the other operations.}
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
Step.MultiStep.Steps                   = ["SingleStep1", "SingleStep2"] \
                                                                   # steps that compose this multi-step
Step.MultiStep.Baselines.Station1      = [0, 0, 0, 1, 1, 2]        # baselines to use
Step.MultiStep.Baselines.Station2      = [0, 1, 2, 1, 2, 2]        # (all if empty)
Step.MultiStep.Sources                 = ["3C343"]                 # list of sources
Step.MultiStep.ExtraSources            = ["M81"]                   # list of sources outside patch
Step.MultiStep.InstrumentModel         = ["BANDPASS", "TOTALGAIN", "PATCHGAIN"] \ 
                                                                   # instrument model
Step.MultiStep.Integration.Freq        = 2                         # integration interval: f(Hz)
Step.MultiStep.Integration.Time        = 0.5                       # integration interval: t(s)
Step.MultiStep.Correlation.Selection   = CROSS                     # one of AUTO, CROSS, ALL
Step.MultiStep.Correlation.Type        = ["XX", "XY", "YX", "YY"]  # which (cross) correlations to use

Step.SingleStep1.Baselines.Station1    = [0, 1]                    # baselines to use
Step.SingleStep1.Baselines.Station2    = [1, 2]                    # (all if empty)
Step.SingleStep1.Sources               = []                        # list of sources
Step.SingleStep1.InstrumentModel       = ["BANDPASS", "TOTALGAIN"] # instrument model
Step.SingleStep1.Operation             = SOLVE                     # one of SOLVE, SUBTRACT, CORRECT, \
                                                                   # PREDICT, SHIFT, REFIT
Step.SingleStep1.OutputData            = "OUTDATA1"                # MS output data column
Step.SingleStep1.Solve.MaxIter         = 10                        # maximum number of iterations
Step.SingleStep1.Solve.Epsilon         = 1e-7                      # convergence threshold
Step.SingleStep1.Solve.MinConverged    = 0.95                      # fraction that must have converged
Step.SingleStep1.Solve.Parms           = ["PHASE:*"]               # names of solvable parameters
Step.SingleStep1.Solve.ExclParms       = [""]                      # parameters excluded from solve
Step.SingleStep1.Solve.DomainSize.Freq = 1000                      # f(Hz)
Step.SingleStep1.Solve.DomainSize.Time = 1                         # t(s)

Step.SingleStep2.Baselines.Station1    = []                        # baselines to use
Step.SingleStep2.Baselines.Station2    = []                        # (all if empty)
Step.SingleStep2.Sources               = []                        # list of sources
Step.SingleStep2.InstrumentModel       = ["DirGain", "Phase"]      # instrument model
Step.SingleStep2.Operation             = CORRECT                   # one of SOLVE, SUBTRACT, CORRECT, \
                                                                   # PREDICT, SHIFT, REFIT
Step.SingleStep2.OutputData            = "OUTDATA2"                # MS output data column
\end{verbatim}
}

\subsection*{BBDB}
\label{app-bbdb}
This contains information on how the blackboard database and the parameter
databases can be reached.
\begin{description}
\item [Host] : \emph{string} \\
    Hostname or IP address of the host on which the black board database and
    the parameter databases reside.
\item [Port] : \emph{int} \\
    Port number on which the blackboard database server is listening.
\item [DBName] : \emph{string} \\
    Name of the black board database.
\item [UserName] : \emph{string} \\
    Username to access the black board database.
\item [PassWord] : \emph{string} \\
    Password to access the black board database.
\end{description}

\subsection*{ParmDB}
\label{app-parmdb}
\begin{description}
\item [Instrument] : \emph{string} \\
    Path to the AIPS++ table containing the instrument parameters.
\item [LocalSky] : \emph{string} \\
    Path to the AIPS++ table containing the local sky model parameters.
\item [History] : \emph{string}
    Path to the AIPS++ table containing the solve history.
\end{description}

\subsection*{Correlation}
\label{app-correlation}
\begin{description}
\item [Selection] : \emph{string} \\
    Station correlations to use. Should be one of 'AUTO', 'CROSS', or 'ALL'.
    \par
    AUTO: Use only correlations of each station with itself (i.e. no base
    lines). \textcolor{red}{Not yet implemented.} \\ CROSS: Use only
    correlations between stations (i.e. base lines). \\ ALL: Use auto and
    cross correlations both.
\item [Type] : \emph{string} \\
    Correlations of which polarizations to use, one or more of 'XX', 'XY',
    'YX', 'YY'. As an example, suppose we select 'XX' here and set Selection
    to 'AUTO', then the X polarization signal of each station is correlated
    with itself.  However, if we set Selection to 'CROSS' then the X
    polarization of station A is correlated with the X polarization of station
    B for each base line (A,B)
\end{description}

\subsection*{DomainSize}
\label{app-domainsize}
\begin{description}
\item [Freq] : \emph{double} \\
    The size of the domain in frequency (Hz).
\item [Time] : \emph{double} \\
    The size of the domain in time (s).
\end{description}

\subsection*{Baselines}
\label{app-baselines}
The selected baselines. A baseline is a pair of stations. The first station of
the pair is contained in Station1, the second in Station2. For example,
suppose we have six baselines: (A, B), (A, C), (A, D), (B, C), (B, D), (C,
D). Then Station1 would contain [A, A, A, B, B, C] and Station2 would contain
[B, C, D, C, D, D]. The lengths of Station1 and Station2 should always be
equal. If both fields are left empty, all baselines are used.
\begin{description}
\item [Station1] : \emph{vector$<$string$>$} \\
    One name for each baseline: the first station in the pair that forms the
    baseline.
\item [Station2] : \emph{vector$<$string$>$} \\
    One name for each baseline: the second station in the pair that forms the
    baseline.
\end{description}

\subsection*{Solve}
\label{app-solve}
\begin{description}
\item [MaxIter] : \emph{int} \\
    Maximum number of iterations.
\item [Epsilon] : \emph{double} \\
    Minimal difference between the old and the new parameter values after each
    iteration. When the difference falls below this threshold, the solver will
    stop iterating.
\item [MinConverged] : \emph{double} \\
    Minimal fraction of solve domains that must have converged to declare
    overall convergence.
\item [Parms] : \emph{vector$<$string$>$} \\
    Parameters to solve for. Wildcards are allowed, e.g. BANDPASS:*.
\item [ExclParms] : \emph{vector$<$string$>$} \\
    Subset of the parameters selected by Parms that should not be solved for.
    For example, if we would like to solve for the gain (amplitude, phase) of
    each station, but we would also like to fix the phase of the first station
    (STATION0) this can be specified as follows: {\footnotesize
\begin{verbatim}
Solve.Parms = ["gain:*"]
Solve.ExclParms = ["gain:*:phase:STATION0"]
\end{verbatim}
}
\item [DomainSize] : \emph{DomainSize} \\
    Size of the solve domain. The work domain is divided in solve domains and
    a solution is computed for each solve domain independently.
\end{description}



\end{document}
