%
%  Copyright (C) 2007
%  ASTRON (Netherlands Foundation for Research in Astronomy)
%  P.O.Box 2, 7990 AA Dwingeloo, The Netherlands, seg@astron.nl
%
%  $Id$
%
\documentclass[10pt]{lofar}
%
%% I prefer Bookman over Palatino, but I need Kerkis for math support
%\usepackage{bookman} 
\usepackage{mathpazo}      %% Palatino with matching math fonts.
\usepackage{layout}
\usepackage{color}
\usepackage{xspace}
\usepackage{url}           %% could also use package hyperref.
%\usepackage[colorlinks=false]{hyperref}
%
\newcounter{decision}
\newenvironment{decision}[1][Decision]{\begin{trivlist}\item[\hskip \labelsep {\bfseries #1 \refstepcounter{decision}\thedecision}]}{\end{trivlist}}
%
\newcommand{\todo}[1]{\begin{center}\fbox{\parbox{0.9\textwidth}{\textbf{!!FIXME!! #1}}}\end{center}}
\newcommand{\cs}[1]{\textsc{cs}\begin{footnotesize}#1\end{footnotesize}\xspace}
\newcommand{\bbs}{\textsc{bbs}\xspace}
\newcommand{\lofar}{\textsc{lofar}\xspace}
\newcommand{\ms}{\textsc{ms}\xspace}
\newcommand{\me}{\textsc{me}\xspace}
\newcommand{\lsm}{\textsc{lsm}\xspace}
\newcommand{\aips}{\textsc{aips}\begin{footnotesize}++\end{footnotesize}\xspace}
\newcommand{\olap}{\textsc{olap}\xspace}
\newcommand{\meqtree}{\textsc{m}\begin{footnotesize}eq\end{footnotesize}\textsc{t}\begin{footnotesize}ree\end{footnotesize}\xspace}
%\newcommand{\meqtree}{MeqTree\xspace}
\newcommand{\predict}{\textsc{predict}\xspace}
\newcommand{\solve}{\textsc{solve}\xspace}
\newcommand{\subtract}{\textsc{subtract}\xspace}
\newcommand{\correct}{\textsc{correct}\xspace}
%
\include{definitions}
%
\begin{document}
\maketitle
\newpage
%
\theDistributionList
\vspace{1cm}
\theDocumentRevision
\newpage
%
\begin{abstract}
\centering
\includegraphics[width=0.8\textwidth]{images/dt20070317}
\end{abstract}
\newpage
%
\tableofcontents
\newpage
%

\section{Introduction}
\label{sec:introduction}

\subsection{Purpose of This Document}
\label{subsec:purpose}
This document provides a detailed description of the architectural and
software design of the Blackboard Selfcal System (BBS) that will be used for
the calibration of the LOFAR observations. The primary goal of this document
is to provide information that is detailed enough to help the reader
understand the design considerations, choice of software architecture and
global design. We will not delve into the level of detailed software design,
since this will likely cause discrepancies between the actual code and this
document. For this level of detail, the reader is suggested to consult the
online code documentation. This document supersedes the previous version of
the BBS SDD~\cite{LOFAR-ASTRON-SDD-052}.

\subsection{Executive Summary}
\label{subsec:summary}

\subsection{Abbreviations}
\label{subsec:abbrev}
\begin{description}
\item [BBS] BlackBoard Selfcal System
\end{description}

\cleardoublepage

\section{Architectural Design}
\label{sec:architectural-design}

\todo{see SDD-050 section 4.2 for requirements}

\subsection{Design Considerations}
\label{subsec:considerations}
The BlackBoard SelfCal (BBS) system is designed to do the calibration of LOFAR
in an efficient way. Although BBS is mainly developed for LOFAR, it may also
be used to calibrate other instruments as soon as their specific algorithms
are plugged in.

\subsubsection{Data Volume}
\label{subsubsec:data-volume}
The volume of the data coming from the LOFAR correlator is \emph{very} large.
During initial operation (mid 2008) the amount of data generated during an
average observation will be in the order of several terabytes. Once LOFAR is
fully operational, this number will have increased to almost a hundred
terabytes.  Given the output data rate of the correlator and the storage
capacity of harddisks, it is obvious that the data cannot be stored on a
single system, not even if an array of harddisks were used. The only feasible
way to handle these large data sets is to distribute them to multiple
computers. Each computer will have to store and manipulate part of the data.

\subsubsection{Distributed Processing}
\label{subsubsec:distributed-processing}

The Selfcal application will be running on the off-line and auxiliary
processing clusters of the central processing facility (see
\cite{LOFAR-ASTRON-ADD-012}). These clusters consist of Linux PCs in a high
bandwidth network. The BBS application will run on a large cluster, typically
consisting of several hundred nodes. Data stored in the CEP intermediate
storage facility will be distributed over multiple disks and will be accessed
by multiple nodes concurrently. Reordering tens of terabytes of data takes too
much time and should be avoided.  Therefore the data should be distributed
such that the various applications (e.g., calibration and imaging) can operate
well without reordering. The distribution should be such that large chunks of
data can be processed locally and only small amounts of data need to be sent
to other machines. There are a few axes along which the data may be
distributed:
\begin{description}
\item [Time] is probably not a good candidate, because a time slot contains a
lot of data (up to 0.7~GBytes during initial operation). This may lead to
problems in the online system when all data of a time slot are sent to a
single machine and written there. Another problem is that parallelization of
imaging gets hard because the data of all time slots have to be combined.
\item [Baseline] seems a better candidate, but will lead to imaging problems. 
This is because a single image needs data from different machines, so large
amounts of gridded or FFT-ed data have to be sent around.
\item [Frequency] seems to be the best candidate. Creating an image is usually
done per channel or for a few channels, so in principle the whole imaging
process can be done locally. It will result in an image cube distributed over
many machines, so the image display and analysis software have to be able to
handle this. The image cube can be very large (e.g., 256~GBytes for
1000~channels of $4000 \times 4000$ pixels for the 4~Stokes parameters). \\
Distribution in frequency means that, e.g., each subband is stored on a
separate machine.  If needed, each subband can be distributed further. Of
course, each machine should contain about the same amount of data to get good
load balancing. \\ 
Note that this distribution matches well with the way the
correlator and online system is designed.
\end{description}
The BBS calibration software is not dependent on a specific distribution, so
in the future other distributions can be used when applicable. However, it has
not been evaluated yet if that is also true for the imaging software.

\subsubsection{Scalable Architecture}
\label{subsubsec:scalable-architecture}
One important requirement is scalability. In order to avoid any performance
bottlenecks, unnecessary coupling between the different computing nodes should
be avoided as much as possible. When distributing data over frequency, we can
almost completely decouple the computing nodes, as we saw in the previous
section. Another way to reduce coupling is to make communication indirect as
well. Computing nodes should communicate through some kind of global shared
memory. There are several architectural patterns that describe this
approach. One of the oldest and best known is the Blackboard pattern, which we
will describe briefly below.

Computing nodes should communicate through some kind of global shared
memory. One obvious candidate for such shared memory is a database system. It
provides locking and notification (trigger) mechanisms, and sometimes even
command queueing. We have to be careful, though, that the database will not
become a bottleneck.

\subsection{Blackboard Pattern}
\label{subsec:blackboard}
The idea behind the Blackboard architecture is a collection of independent
processes that work cooperatively on a common data structure. Each program is
specialized in handling a particular part of the overall task, and all
programs work together on the solution. These specialized programs work
independent of each other. They do not call each other, nor is there a
predetermined sequence for their activation. Instead, the direction taken by
the system is mainly determined by the current state of progress. A central
control component evaluates the current state of processing and coordinates
the specialized programs. This data-directed control regime makes
experimentation with different algorithms possible, and allows experimentally
derived heuristics to control processing. This architecture is described in
\cite{Buschmann1996} and \cite{LOFAR-ASTRON-SDD-002}.

The Blackboard architecture is ideal for solving problems for which no
predetermined algorithm or solve strategy is known. However, for the design of
the BBS system, we've come to the conclusion that the operational system will
benefit in terms of performance when using a predefined solving strategy. The
"best" algorithm to perform a self-calibration run can be chosen from a
relatively short list of calibration strategies in advance (based, e.g., on
heuristics, or suggested by research done with the \meqtree system).  In fact,
the Shared Respository pattern~\cite{Lalanda1998}, which can be seen as a
generalization of the Blackboard pattern, is probably a better match for the
BBS system. It realizes indirect communication using a repository as shared
memory. Figure~\ref{fig:shared-repository-pattern} show the specialization
hierarchy of patterns based on the Shared Repository pattern.

\begin{figure}[!ht]
\centering
\fbox{
  \begin{picture}(440,110)(-20,-3)
  \footnotesize
  \thicklines
  \put(0,50){Shared Repository pattern}
  \put(100,61){\vector(2,1){40}}
  \put(100,53){\vector(1,0){40}}
  \put(100,45){\vector(2,-1){40}}
  \put(150,80){Controller pattern}
  \put(150,50){Blackboard pattern}
  \put(150,20){Repository Manager pattern}
  \put(230,57){\vector(3,1){40}}
  \put(230,49){\vector(3,-1){40}}
  \put(280,68){Blackboard-based Control pattern}
  \put(280,32){Hierarchical Blackboard pattern}
  \end{picture}
}
\caption{Patterns based on the Shared Repository pattern, after Lalanda~\cite{Lalanda1998}.}
\label{fig:shared-repository-pattern}
\end{figure}

For BBS, we will need a global controller, which could be implemented using
the Controller pattern; and a notification or trigger mechanism to inform the
computing nodes of changes to the shared memory, which could be implemented
using the Repository Manager pattern. The shared memory is used as the common
knowledge base for the self-calibration process, and will be implemented as a
database. Using a database system has the advantage that locking, notification
(trigger) and sometimes even command queueing mechanisms are provided
out-of-the-box.

The database will be separated into two parts. One part will contain a list of
commands (or work orders) to be sent to each computing node. The other part
will contain the values and quality of the (partial) solutions calculated by
each computing node. The database can be used as an external source for
various assessments of the solutions.

\cleardoublepage

\section{System Overview}
\label{sec:overview}

\subsection{Subsystems}
\label{subsec:subsystems}
BBS is split into two parts. BBS Control takes care of the distributed
processing by means of the Blackboard pattern. BBS Kernel does the actual
processing; it executes a series of steps where each step consists of an
operation like solve or correct.


\subsubsection{BBS Control}
\label{subsubsec:sys-control}

The BBS Control subsystem is responsible for controlling the execution of a
self calibration strategy. A strategy consists of an ordered list of commands,
which will be executed by the BBS Kernel subsystem.

The key idea is that a subset of the data (the so-called \emph{work domain})
is kept in memory; as many commands as possible are executed on these data
before the next data chunk is accessed. A strategy defines the size of the
work domain (in time and frequency) and optionally which stations and
correlations are contained in the work domain. It is also possible to define
an integration interval in time and frequency to achieve that, say, a longer
time interval can be used. The basic concept is that on each machine the data
contained in the work domain have to fit in memory. The BBS Kernel iterates
over the work domains to process all the data.  For each strategy a number of
steps can be defined. For instance, when peeling 10 Cat I sources, at least 30
steps can be defined. For each source, step~1 is solving for the gain in the
direction of the source, step~2 is subtracting the source, and step~3 shifts
to the next source. Note that only after the last subtraction the residual
data need to be written.  In this way the data are read and/or written only
once per strategy.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.6\textwidth]{images/bbs-control-global-design}
\caption{Global design of the BBS Control system}
\label{fig:bbs-control-global-design}
\end{figure}

The calibration process is controlled by the BBS Control
subsystem. Figure~\ref{fig:bbs-control-global-design} depicts the general
control structure. The BBS Control subsystem consists of one global
controller, which acts as the main process, and multiple local controllers,
each controlling one BBS Kernel subsystem. The global controller posts one or
more commands (steps) to the Command Queue. Each local controller fetches the
next command from the Command Queue and forwards the command to the BBS Kernel
subsystem. The kernel returns parameter solutions and their quality metrics to
the local controller, which, in turn, posts the results to the Parameter
Solutions database. The global controller inspects the results and decides
which action should be taken next.

Since all communication takes places via the Blackboard, there is no need for
a direct connection between the BBS Control and the BBS Kernel subsystems.
The Blackboard contains all the relevant information about the current state
of the self calibration process. This information can be used by other
(external) processes to monitor the calibration process and to plot results.
See~\cite{LOFAR-ASTRON-SDD-002} for more details on the Blackboard
architecture and roles of the controller.


\subsubsection{BBS Kernel}
\label{subsubsec:sys-kernel}

\subsubsection{BBS Database}
\label{subsubsec:sys-database}

\subsection{Interfaces}
\label{subsec:sys-interfaces}

\subsubsection{Context Diagram}
\label{subsubsec:context}

\subsubsection{BBS Control}
\label{subsubsec:interf-control}

\subsubsection{BBS Kernel}
\label{subsubsec:interf-kernel}

\begin{itemize}
\item MS
\item ParmDB and LSM
\item ACC
\item kernel -- solver
\item kernel -- black board
\end{itemize}

\subsubsection{BBS Database}
\label{subsubsec:interf-database}

\cleardoublepage

\section{Software Design}
\label{sec:software-design}

\subsection{BBS Control}
\label{subsec:design-control}

\subsubsection{BBS Strategy}
\label{subsubsec:design-strategy}
One iteration in the so-called \emph{Major
Cycle}~\cite[sec.~4.1]{LOFAR-ASTRON-SDD-050} can be described by a BBS
Strategy. A strategy defines a relationship between the data set of a given
observation, which is stored in a measurement set, and the parameter database
holding (intermediate) values of the model parameters that will be estimated
as part of the self calibration process. At least two models are used in the
current self calibration setup: the Local Sky Model (LSM) and the Instrument
Model. The Data Selection associated with a BBS Strategy defines the selection
of the observed data that will be used for the complete strategy. Here you
can, for example, specify which frequency bands, time intervals, and baselines
should be used during this self calibration run. A strategy is defined in
terms of one or more BBS Steps (see section~\ref{subsubsec:design-step}
below).

\begin{figure}[!ht]
\centering
\includegraphics[width=0.5\textwidth]{images/bbs-strategy-class-diagram}
\caption{The BBS Strategy class defines the strategy to be used for the current self calibration run.}
\label{fig:bbsstrategy}
\end{figure}

\subsubsection{BBS Step}
\label{subsubsec:design-step}
A BBS Strategy is defined in terms of one or more BBS Steps. The BBS Step
class is designed as a Composite pattern~\cite{Gamma1995}, which means that
each BBS Step can itself be made up of one or more BBS Steps. The Composite
pattern provides an easy way to define a tree-like structure. Leaf classes,
like BBS SolveStep cannot be further subdivided; they describe one single
piece of work that can be handed over to the BBS Kernel. Currently, there is a
total of six leaf classes, each defining one single piece of work.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{images/bbs-step-class-diagram}
\caption{The BBS Step class family defines single pieces of work that can be
executed by the BBS Kernel as part of the current self calibration run.}
\label{fig:bbsstep}
\end{figure}

\subsubsection{Global Control}
\label{subsubsec:design-global-control}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{images/bbs-global-control-activity-diagram}
\end{figure}

\subsubsection{Local Control}
\label{subsubsec:design-local-control}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.9\textwidth]{images/bbs-local-control-activity-diagram}
\end{figure}


\subsection{BBS Kernel}
\label{subsec:design-kernel}
The kernel is the part of \bbs responsible for number crunching. It is
implemented as a stand-alone library that is used by the local controller
(BBSKernel executable) and the solver (BBSSolver executable). This simplifies
reuse of kernel code in projects that require a different control
implementation.

\todo{create a simple figure in a more general part of the document that
explains the different packages/libraries/executables/... used in \bbs (see for
example the \olap document)}

The kernel supports four primitive commands:
\begin{itemize}
\item \textsc{predict}\\
Predict (simulate) visibilities based on a model that describes the sky, the
environment (e.g. ionosphere), and the instrument.
\item \textsc{subtract}\\
Predict visibilities for one or more source(s) and subtract the result from the
observed visibilities.
\item \textsc{correct}\\
Correct visibilities for a given reference (source) direction.
\item \textsc{generate equations}\\
Generate condition equations in a form that can be fed to the solver. Condition
equations relate the model parameters to the difference between the observed
visibilities and the predicted visibilities (based on the model).
\end{itemize}


Both \bbs and \meqtree are based on \textsc{pss} (Prototype Selfcal System), but
have since gone their own separate ways. \meqtree has become a very flexible
system, making it easy to experiment with different calibration algorithms. \bbs
development focussed more on performance and robustness at the cost of some
flexibility. Distribution and parallel processing are an integral part of the
design. Using \meqtree to learn which approaches work and which do not, \bbs
will implement the succesful approaches in an operational environment.

Although BBS and MeqTrees have gone their own way, they still share the same
principles:
\begin{enumerate}
\item The model (MeasurementEquation) is represented by an expression tree (in
fact, a separate tree for each baseline). BBS and MeqTrees differ in the way
the trees are defined. In MeqTrees the user has a lot of freedom by the
ability to define the trees at the scripting level. In BBS only some
predefined, highly tuned trees can be used.
\item Parameters form the leafs of the expression trees. They are represented by
funklets (2-dimensional functions, usually polynomials) that are valid for a
given frequency-time domain. A constant value is regarded as a zeroeth-order
polynomial. Both instrumental and sky parameters can be used.  In MeqTrees
functions with more than 2 dimensions can be used. For example, beam shapes as
a function of frequency, time, l, and m. It is not clear if that is needed in
BBS.
\item The calibration process consists of adapting the model parameters to the
observed UV data by improving the values of the solvable model parameters (in
fact, the coefficients of the parameter functions). This is done using a
solver (least squares fitter). It needs the derivatives of the solvable
parameter coefficients which are calculated in a numerical way.
\item Because the problem is non-linear a non-linear solver is used requiring
multiple iterations. The iteration process is stopped when the solution has
converged sufficiently or when a maximum number of iterations is done.  The
user can choose to solve for any combination of parameters. The solve is done
in frequency-time domains of a given size and resolution. For example, one
could solve ionospheric phases for each minute and solve source fluxes for the
entire time range.  Note that another way of iteration is alternating between
groups of parameters to solve for.
\item Peeling and phase shifting the data is a means to reduce the number of
calculations needed in the predict of the sources. It makes it possible to
predict the sources near the new phase center on a coarser grid.
\end{enumerate}

\todo{Move the above to a more general part of the document}

\subsubsection{Terminology}
\label{subsubsec:terminology}

\paragraph{Domains}
A central concept in \bbs is the \emph{domain}: A 2-D rectangular region in
$frequency$ and $time$. In table \ref{tab:domains} we define seven different
domain types that are useful for discussing the design of \bbs. Figure
\ref{fig:domains} illustrates the different domain types and how they relate to
eachother.

\begin{table}[htb!]
\centering
\begin{tabular}{lp{0.60\textwidth}}
\hline
\textbf{Name} & \textbf{Description}\\
\hline
\texttt{data domain} & The domain covered by a single observation\\
\hline
\texttt{local data domain} & Part of the data domain that is local to (i.e.
stored at) a given compute node\\
\hline
\texttt{work domain} & The union of all local work domains\\
\hline
\texttt{local work domain} & Part of the local data domain that can be kept in
main memory\\
\hline
\texttt{solve domain} & Part of the data domain that is used to solve for a set
of unknowns\\
\hline
\texttt{local solve domain} & A solve domain that \emph{intersects} the local
data domain\\
\hline
\texttt{validity domain} & Domain on which a \emph{funklet} is considered
valid\\
\end{tabular}
\caption{An overview of the domain types used in \bbs.}
\label{tab:domains}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.5\textwidth]{images/domains.eps}
%\parbox{0.75\textwidth}{
\caption{Data distribution during the execution of a typical \textsc{solve}
command that shows the domain types described in Table~\ref{tab:domains}}
%}
\label{fig:domains}
\end{figure}

\todo{Fix the labels in figure \ref{fig:domains}.}


\paragraph{Models, parameters, funklets, and coefficients} Self calibration
revolves around fitting a \emph{model} to the observed data by adjusting the
parameters of the model. On the coarsest scale a model for the calibration of
\lofar describes the instrument, the environment, and the sky. This model can be
decomposed into smaller (sub)models, such as a model for the beamshape, the
bandpass, or the ionosphere (see also \cite[sec. 2]{LOFAR-ASTRON-SDD-050}).

In general, a \emph{parameter} can be a constant or a continuous function of one
or more variables such as $frequency$, $time$, and $direction$. At the moment,
\bbs only supports functions of $frequency$ and/or $time$. Variability in
direction can be achieved by creating a separate parameter for each direction of
interest. This is done for the E-jones, where a separate parameter is used for
each combination of station and source. True 4-D supports (frequency, time, l,
m) may be required in the future to allow modelling of the beam shape.

Both \bbs and \meqtree represent the value of a parameter by a set of
\emph{funklets}. A funklet (in \bbs) is a polynomial of arbitrary degree in
$frequency$ and/or $time$ with an associated \emph{validity domain}. Figure
\ref{fig:funklet} illustrates these concepts.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{images/funklet.eps}
\caption{This figure shows a set of funklets with associated validity domains
that together represent the value of a parameter on a larger domain.}
\label{fig:funklet}
\end{figure}

Validity domains and solve domains are related concepts. The validity domains of
a set of funklets is equal to the solve domains used to fit the parameter
represented by the funklets. For example, suppose we fit station gain as
$0^{th}$-degree funklets using solve domains of one channel width times 5
integration periods. Then the validity domains of the funklets that represent
station gain will match the solve domains exactly. However, during fitting the
values of other parameters will be needed as well. The validity domains of the
funklets that represent these parameters will most likely be different, because
they were fitted at some earlier time using potentially different solve domains.
In summary: The validity domain of a funklet determines in what region the value
of the funklet is considered valid; A solve domain determines which part of the
observed data is used to fit the coefficients of a funklet.

The problem of fitting a single parameter can now be reformulated as fitting the
coefficients of a \emph{set} of funklets defined on a set of (non-overlapping)
solve domains. Because the solve domains do not overlap, each funklet may be
treated as an independent fitting problem. For each $n^{th}$-degree funklet,
$n+1$ coefficients need to be determined.

\subsubsection{Measurement Equation Evaluation}
\label{subsubsec:design-me-evaluation}
The Measurement Equation (\me) \cite{LOFAR-ASTRON-ADD-015} is a parameterized
model of an interferometer in terms of a model of the sky, environmental effects
(e.g. ionosphere), and antenna based instrumental effects.

Both \bbs and \meqtree represent the \me as a directed acyclic graph, although
it is commonly referred to as \emph{tree}. The nodes of the tree represent
atomic expressions, while the branches represent dependencies between atomic
expressions. Compound expressions can be build by combining several nodes into a
(sub)tree. The leaf nodes represent the parameters of the expression, e.g.
source positions and Stokes vectors. As an example, figure \ref{fig:expr_tree}
shows a subtree for computing the visibilities produced by two linearly
polarized point sources at different positions.
\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{images/expr_graph_sdd.ps}
\caption{This figure shows a subtree for computing the visibilities produced by
two linearly polarized point sources at different positions. The leaf nodes,
i.e. the parameters of the expression, have been colored grey.}
\label{fig:expr_tree}
\end{figure}

\meqtree is very flexible in the expression trees that a user can specify. The
flexibility is due to a powerful Python language extension (\textsc{tdl}) that
allows easy construction of a tree from an arbitrary expression, and to the high
granularity of the nodes. The granularity of the nodes used in \bbs tends to be
lower. For example, in \meqtree a typical node implements the sine function,
whereas in \bbs a single node may implement a bandpass model. Because of the
granularity of the nodes used in \bbs, the expression trees are generally small.
Therefore, the overhead of traversing the trees is small as well. Note that, if
necessary, \bbs could be made more flexible by adding nodes that are less
specialized. By the same reasoning, the overhead of tree traversal in \meqtree
could be reduced by creating specialized nodes.

Specification by the user is limited to selecting predefined subtrees to include
in the \me. Currently implemented subtrees are:
\begin{itemize}
\item Prediction of a (possibly polarized) point source
\item Complex gain per station (G-jones)
\item Complex gain in the direction of a source, per station (E-jones)
\item Bandpass (B-jones)
\end{itemize}

\bbs will automatically generate a complete expression tree based on the
selected predefined subtrees. For example, it will instantiate a subtree for
each selected source in the local sky model and will include these at the
appropriate place in the \me tree for each selected interferometer (baseline).

In \cite[sec. 2]{LOFAR-ASTRON-SDD-050} a more in depth discussion of the various
models, such as beamshape, bandpass, and ionosphere is given. 

\subsubsection{Model Parameters}
To evaluate an expression tree, the kernel needs to know the values of
the model parameters (leaf nodes). Typical examples of model parameters are:
source position and Stokes vector, complex station gain, ionospheric phase
shift. The total number of parameters in the expression trees of the entire
\lofar array will depend on the number of sources that are included in the sky
model. Several thousands of parameters seems to be a realistic estimate
\cite{LOFAR-ASTRON-SDD-052}.

\paragraph{Naming scheme}
Because of the large number of parameters, keeping track of them all becomes a
challenge. A naming scheme can be used to simplify this task. The naming scheme
achieves two goals:
\begin{enumerate}
\item Identification of groups of related parameters
\item Specification of default values
\end{enumerate}

Each parameter is assigned a unique name, which can be made up of several parts
separated by colons. Grouping can be achieved with \textsc{unix}-like wildcards
(*, \{\}), see table \ref{tab:naming_scheme}.
\begin{table}[htb!]
\centering
\begin{tabular}{lp{0.60\textwidth}}
\hline
\textbf{Identifier} & \textbf{Referenced parameter(s)} \\
\hline
\texttt{ra:3C343} & Right ascention of source 3C343 \\
\hline
\texttt{dec:3C343} & Declination of source 3C343 \\
\hline
\texttt{gain:11:phase:CS10:3C343.1} & Phase of the x-polarized signal from
station CS10 in the direction of source 3C343.1 \\
\hline
\texttt{gain:11:phase:*:3C343.1} & Phase of the x-polarized signal of all
stations in the direction of source 3C343.1 \\
\hline
\texttt{gain:\{11,22\}:phase:*:3C343.1} & Phase of both the x- and y-polarized
signal from station CS10 in the direction of source 3C343.1. \\
\hline
\end{tabular}
\caption{Examples of typical parameter names and the use of wildcards to
identify a group of related parameters.}
\label{tab:naming_scheme}
\end{table}
If the kernel cannot find an exact match when searching for a certain parameter,
it will try to find a default value. Default values are specified according to
the same naming scheme as parameters. However, if no default value can be found
that matches the name of a parameter exactly, the kernel will strip off the last
part of the name and retry. This process continues until either a match is found
or the name becomes empty. Thus, one can specify for example a default flux for
every source by specifying \texttt{StokesI}: If the kernel needs the value of
\texttt{StokesI:3C343} but that parameter does not exist as a regular parameter,
it will try to find a default value called \texttt{StokesI:3C343}. If that
default value also does not exist, it will strip off the last part of the name
and search for a default value called \texttt{StokesI}, which does exist in our
example.

\paragraph{Reusing parameter values}
It often makes sense to reuse the parameter values determined during
calibration of an observation $A$ for calibration of an observation $B$. For
instance, to provide a good initial values for the calibration of $B$ and
hopefully save a few iterations.

Instead of straightforward copying of parameter values, interpolation could be
considered. For example, suppose we have already calibrated two observations,
$A$ and $C$, one taken before and one after the time at which the observation to
be calibrated, $B$, was recorded.

Reuse of parameter values can also be applied \emph{during} calibration of an
observation, when moving from one work domain to the next. One could even
consider something like the following algorithm:
\begin{enumerate}
\item Solve a small number of work domains spaced relatively far apart
\item Determine initial values for the intermediate work domains by
interpolation
\item Solve for the intermediate work domains.
\end{enumerate}

The current implementation does not provide support for this kind of operations
yet.

\paragraph{Refitting}
Refitting is useful in a multi-resolution approach. A typical example would be:
\begin{enumerate}
\item Solve for a parameter using funklets of a low polynomial degree on
relatively large solve domains to get the global trend
\item Refine the solution by using funklets of a higher degree on relatively
small solve domains, using the values found in 1. as a starting point
\end{enumerate}

Funklets are defined on a normalized domain. The normalization depends on the
validity domain. Therefore, the coefficients of funklets defined on a coarse
scale cannot be copied directly to funklets defined on a fine scale. For
polynomials the required transformation is easy to derive. In more complicated
cases, the solver could be used to find the new values.

Refitting is not yet supported in the current implementation.

\todo{Check of bovenstaande juist is.}

\paragraph{Storage}
The current Prediffer implementation can handle parameters as follows:
At the first iteration it reads the parameters from an NFS-visible Berkeley DB (BDB) database or AIPS++ table.
The Prediffer can handle the updated parameter values in 3 ways:
Read from an NFS-visible database or AIPS++ table.
Read from a replicated BDB database.
Receive from the Controller.
Given the amount of parameters and domains, searching parameters and domains in the database can take some time. Optimization of the parameter data structure and evaluation of high-performance distributed embedded databases are ongoing.

The parameters are used in three places:
Each Prediffer needs parameter values to calculate the model.
Each Solver calculates new values for solvable parameters.
The controller writes the new parameter values to the distributed database and on the blackboard.

Before the first solve iteration a Solver receives the current values of the solvable parameters from the Prediffers. After each iteration it sends the newly calculated values to the Controller. So a Solver does not need access to the parameter database itself. In other words, the Prediffer nodes access the parameter database and sent self-contained inputs to the solver(s).
The AIPS++ LSQFit class that is used in the solver, keeps data from previous solutions within the same iteration series to speed up the convergence process. This is used to minimize the data size that is sent from Prediffer nodes to the solver(s).

At the start of the first iteration in solving a group of parameters, a Prediffer needs the values of all model parameters, not only the solvable ones. It needs them for the domain being solved. There are two approaches to achieve this:
1.A Prediffer reads the parameters from the database. This requires that the database is visible on the Prediffer machine.
2.The Controller reads the parameters and sends them to the Prediffers.
The problem with this approach is that the Controller does not know which parameters are needed by the Prediffers. A possible solution for this is that all Prediffers tell the Controller which parameters they need and get the values back. Because most Prediffers will need the same parameters, the Controller does not need to read too many parameters.
In the second and later iterations a Prediffer only needs the new values of the solvable parameters. This can be done similarly; either the Controller sends the new values to the Prediffers or the Controller updates the database after which a Prediffer can read the new values.

If a Prediffer reads the parameter values itself, the following database options are possible:
1.An AIPS++ table or embedded database on an NFS-mounted file system.
2.An embedded database that is (automatically) replicated on all machines.
3.A database server that is visible from all machines.

The parameter handling classes have been designed such that they hide the parameter handling from the other code. In this way it is rather easy to choose the parameter handling scheme that suites best. E.g., on Blue Gene/L another scheme might be used than on a Linux cluster.

\subsubsection{Solver}
\label{subsubsec:design-solver}
BBS can solve for any combination of the parameter coefficients used in the algorithms.

Solvable parameters
In a BBS run it is possible to do a joint solve for any combination of parameters. There is a limitation that the frequency-time domains of the solvable parameters have to be the same.
The number of solvable parameters in a run can vary a lot. For two reasons it should be kept as low as possible:
1.A derivative has to be calculated for each solvable parameter. It means it is better to solve for constant values in small domains than for polynomial coefficients in larger domains. Later it is always possible to fit the constant parameter values to a polynomial (or any other function) on a larger domain.
2.Inverting the solver matrix is a $O(N^3)$ problem. As the size of the fitter matrix is determined by the number of solvable parameters, it is clear that the number should be kept as low as possible.
It means that it makes no sense to combine multiple domains in a single solve, because each domain has its own set of parameter values.

Some examples of parameters to solve are:
Ionospheric phases for a patch in the sky require 74 parameters per patch (for 37 stations).
To keep the number of parameters low it is best to solve patch by patch. In  it is argued that this approach is also best in peeling the visibility onion.
Point source fluxes and/or positions require 1-6 parameters per source assuming that no polynomial or so is used to represent the fluxes.


It follows that solving for $0^{th}$-degree funklets using one channel width times one integration period solve domains is equivalent to solving for a sampled version of the parameter value. A larger solve domain may be used to increase the signal to noise, or to get enough condition equations when solving for multiple parameters simultaneously. From a practical point of view, a large solve/validity domain also decreases the number of parameter values that need to be stored. Storing a set of funklet coefficients per visibility will most likely require too much storage space.

\todo{Klopt de bovenstaande alinea en is ie duidelijk genoeg?}

An important consequence of using funklets is that, given a polynomial degree $n$ and irrespective of the size of the validity domain, it is implicitly assumed that the parameter value is well approximated by a $n^{th}$-degree polynomial on the validity domain. To make this assumption more explicit, it may be useful to interpret the $n^{th}$-degree polynomial as a model, instead of an integral part of parameter handling.

\subsection{BBS Database}
\label{subsec:design-database}

\subsubsection{Data Model}
\label{subsubsec:design-data-model}
%\begin{figure}[!ht]
%\includegraphics[width=\textwidth]{images/blackboard-datamodel}
%\caption{Data model of the Blackboard database}
%\end{figure}

\subsubsection{Work Orders}
\label{subsubsec:design-work-orders}

\subsubsection{Parameter Solutions}
\label{subsubsec:design-parmsolutions}

\subsection{Performance considerations}
\label{subsec:performance-considerations}

\subsubsection{Double vs. single precision}
\subsubsection{Other vis. format / mmap}
Currently mmap is used to read and write the data in the MeasurementSet. Although it works fast and nice, it has the problem that it makes it impossible to use a large work domain where data are selected and/or integrated. It also makes it hard to use a MeasurementSet containing tiled data.
It would be better to have a separate IO thread (or process) that can read, write, select, and integrate the data asynchronously and transports the data as MeqMatrix objects.

The description of each MeasurementSet is currently stored in a blob in a .des file. It would be better to let the IO thread retrieve that info from the MeasurementSet and send it as a blob to the Prediffers.

\todo{Possibility of mistakes is also large!}


\subsubsection{Parameter spatial queries}
\subsubsection{Process / code level parallelism}
Apart from the distributed processing described before, the BBS system also exploits parallelization in the Prediffers as much as possible. This form of parallelization is for nodes with multiple CPUs and CPU-cores which are both envisaged in the CEP implementation. This parallelization is done by using multi-threading in the calculation of the result of the expression tree. Because a node in the tree can be used by multiple parents, a straightforward parallel execution of the tree is not possible. Instead the tree is analyzed to find the dependency level of each node. Starting at the lowest level the nodes at each level are calculated in parallel and the results are cached. In this way all calculations can be done in parallel without the need of explicit locking and waiting.

Because most calculations are done for a vector of frequencies, it is possible to use vectorized instructions on modern processors. A special code generator has been developed to assist in using SSE2 instructions for this purpose.

\subsubsection{Co-processors}
We have analysed the possibilities of off-loading Prediffer calculations from the host node CPUs to additional hardware attached to the local machine or available via the networks. The main problem is that low-latency communication is required to keep up with the processing on the host node itself and integrated evaluated values with the “main” processing flow on the host processor CPUs.
We have tested with an external “DFT server” available on the network which could make use of the Blue Gene/L. In this case the bottleneck is the relatively high data bandwidth needed. This bottleneck made the use of an external DFTServer not providing any speedup.  The high data bandwidth is not available through network devices, but may be available within the host machine when for example a DSP board would be attached to the PCI bus.
We have also looked at the use of (cheap!) additional processing power within the local host machine by using the resources available in modern graphics cards for floating point calculations. Current graphics cards provide tens of times more computing power than a general-purpose processor, but the communication over AGP is slow in the backwards direction, although this will considerably improve with the new PCI-express cards. There is still a potential to use such cards in the predict calculations. Drawback is that double precision operations are not supported yet.


\subsubsection{Blue Gene/L}
Since the Blue Gene/L machine contains most processing power available in the central processing facility, it seems appealing to enable the BBS code to run on this machine, although the operational model may reduce the availability of BGL for calibration purposes.
However, there are some significant problems associated with the BGL in view of the BBS application:
The run time environment of the BGL is single threaded. By using the second core we can manage to operate in a kind of dual thread mode. Having only one thread available, or two at the cost of quite some extra program complexity, is a serious drawback in the implementation of Selfcal. Multiple threads are needed to overlay disk IO, parameter IO, IO with solver and the calculations themselves. If we can not overlay these tasks by using asynchronous IO, the efficiency of the calculations will be reduced considerably.
The BGL has no local storage; all disk IO runs over Gigabit Ethernet connections to external storage devices (typically NFS, GPFS etc.) With the large number of nodes used in a BGL application, this will generate extra load on the storage facility.
Some of the building stones of BBS, such as the AIPS++ components and (distributed) databases are not available on BGL.
The local RAM per node is relatively small, which reduces the possibility of data caching. In view of the disk-IO limitations mentioned above, this is a considerable performance limitation.
An advantage of the BGL, however, is that its floating point operations are all done in double precision and it can handle complex numbers efficiently.

In view of the drawbacks of the BGL for this type of applications, we consider it not feasible to execute the complete BBS application on the BGL.
However, there are possibilities to run parts of the code on BGL in co-operation with BBS code running on a Linux cluster.


\subsubsection{Minimisation}
\begin{itemize}
\item Joint solving (SDD-052, page 19)
\item Solvable parameter count (SDD-052. page 16)
\end{itemize}

In a BBS run it is possible to do a joint solve for any combination of parameters. There is a limitation that the frequency-time domains of the solvable parameters have to be the same.
The number of solvable parameters in a run can vary a lot. For two reasons it should be kept as low as possible:
1.A derivative has to be calculated for each solvable parameter. It means it is better to solve for constant values in small domains than for polynomial coefficients in larger domains. Later it is always possible to fit the constant parameter values to a polynomial (or any other function) on a larger domain.
2.Inverting the solver matrix is a $O(N^3)$ problem. As the size of the fitter matrix is determined by the number of solvable parameters, it is clear that the number should be kept as low as possible.
It means that it makes no sense to combine multiple domains in a single solve, because each domain has its own set of parameter values.
Some examples of parameters to solve are:
Ionospheric phases for a patch in the sky require 74 parameters per patch (for 37 stations).
To keep the number of parameters low it is best to solve patch by patch. In  it is argued that this approach is also best in peeling the visibility onion.
Point source fluxes and/or positions require 1-6 parameters per source assuming that no polynomial or so is used to represent the fluxes.


\subsubsection{Data handling/caching}

\subsubsection{Parameter database access / central database}
Prediffers look in the BlackBoard Database for new work orders. The load on the database can be very high if many processes are polling it to look for work orders. It might be better to turn the BlackBoard into a process or to send the work orders directly from the controller to the Solver and Prediffer processes.
A database can still be used as a logging mechanism and for monitoring by external processes.

\begin{itemize}
\item Asynchronous notification
\item Do not store intermediate results? (solve history)
\end{itemize}


\cleardoublepage

% References
\bibliographystyle{unsrt}
\bibliography{lofar}

\cleardoublepage

\appendix
\section{Configuration Syntax}

This appendix describes the syntax of the BBS configuration file (a.k.a.
parset). Its goal is to foster a common understanding and terminology. At the
moment this page is still under construction. I've added
\textcolor{red}{questions in red} to things that were not clear to me while
creating this page. Thing to do are \textcolor{green}{stated in green}.

\subsection*{Global Settings}
\begin{description}
\item [DataSet] : \emph{string} \\
    Path to the input measurement set.
\item [BBDB] : \emph{BBDB} (see page \pageref{app-bbdb}) \\
    Information about the black board database.
\item [ParmDB] : \emph{ParmDB} (see page \pageref{app-parmdb}) \\
    Information about the parameter databases (e.g. instrument parameters,
    local sky model).
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
DataSet                  = "test.ms"    # name of Measurement Set

BBDB.Host                = "127.0.0.1"  # hostname/ipaddr of BB DBMS
BBDB.Port                = 12345        # port used by BB DBMS
BBDB.DBName              = "blackboard" # name of the BB database
BBDB.UserName            = "postgres"   # username for accessing the DBMS
BBDB.PassWord            = ""           # password for accessing the DBMS

ParmDB.Instrument        = "test.mep"   # instrument parameters (MS table)
ParmDB.LocalSky          = "test.gsm"   # local sky parameters (MS table)
\end{verbatim}
}

\subsection*{Strategy}
A strategy consists of one or more (multi-)steps with an associated work domain
size and optional data integration.
\begin{description}
\item [Steps] : \emph{vector$<$string$>$} \\
    The names of the steps that compose the strategy. It is an error to leave
    this field empty.
\item [Stations] : \emph{vector$<$string$>$} \\
    Names of the participating stations. All stations will be used if this
    field is left empty.
\item [InputData] : \emph{string} \\
    Name of the column in the measurement set that contains the input data.
\item [Correlation] : \emph{Correlation} (see page \pageref{app-correlation}) \\
    Specifies which correlations to use.
\item [WorkDomainSize] : \emph{DomainSize} (see page \pageref{app-domainsize}) \\
    Size of the work domain in frequency and time. A work domain represents an
    amount of input data that is loaded into memory and processed as a single
    block.  A large work domain size should reduce the overhead due to disk
    access.
\item [Integration] : \emph{DomainSize} (see page \pageref{app-domainsize}) \\
    Cell size for integration. Allows the user to perform operations on a
    lower resolution, which should be faster in most cases.
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
Strategy.Steps                 = ["MultiStep", "SingleStep2"] \
                                                # (multi-)steps that compose this strategy
Strategy.Stations              = [ 0, 1, 2, 3 ] # ID's of stations to use
Strategy.InputData             = "INDATA"       # MS input data column
Strategy.Correlation.Selection = ALL            # one of AUTO, CROSS, ALL
Strategy.Correlation.Type      = ["XX", "YY"]   # which (cross)correlations to use
Strategy.WorkDomainSize.Freq   = 1e+6           # work domain size: f(Hz)
Strategy.WorkDomainSize.Time   = 10             # work domain size: t(s)
Strategy.Integration.Freq      = 1              # integration interval: f(Hz)
Strategy.Integration.Time      = 0.1            # integration interval: t(s)
\end{verbatim}
}

\subsection*{Step}
A \emph{single-step} describes one unit of work of the strategy. A step that
is defined in terms of a number of other steps is known as a multi-step. The
attributes of a \emph{multi-step} should be interpreted as default values for
the steps that compose the multi-step. These default values can always be
overridden.
\begin{description}
\item [Steps] : \emph{vector$<$string$>$} \\
    The names of the steps that compose this step (for multi-steps), or absent
    (for single steps).
\item [Baselines] : \emph{Baselines} (see page \pageref{app-baselines}) \\
    Baselines to use.
\item [Sources] : \emph{vector$<$string$>$} \\
    Sources to use. All sources will be used if this field is left empty.
\item [ExtraSources] : \emph{vector$<$string$>$} \\
    Additional sources to include when predicting visibilities. If this field
    is left empty, no extra sources will be included.
\item [Correlation] : \emph{Correlation}  (see page \pageref{app-correlation}) \\
    Specifies which correlations to use.
\item [Integration] : \emph{DomainSize}  (see page \pageref{app-domainsize}) \\
    Cell size for integration. Allows the user to perform operations on a
    lower resolution, which should be faster in most cases.
\item [InstrumentModel] : \emph{vector$<$string$>$} \\
    The parts of the measurement equation that should be included. \par
    \textcolor{green}{TODO: add descriptions for the various parts of the ME.}
\item [Operation] : \emph{string} \\
    The operation to be performed in this step. One of SOLVE, SUBTRACT,
    CORRECT, PREDICT, SHIFT, or REFIT. Only relevant for single steps, should
    be absent for multi-steps. \par\
    SOLVE : Find values for the parameters that minimize the difference
    between the predicted and the measured (u,v) values. \par
    \textcolor{green}{TODO: add descriptions for other values.}
\item [OutputData] : \emph{string} \\
    Column in the measurement set wherein the output values of this step
    should be written. If left empty, no data will be written.
\end{description}

\emph{Single steps should define one of the following fields, depending on the
value of \textbf{Operation}} :
\begin{description}
\item [Solve] : \emph{Solve} (see page \pageref{app-solve}) \\
    Arguments of the SOLVE operation. \par
    \textcolor{green}{TODO: specify arguments for the other operations.}
\end{description}

\subsubsection*{Example}
{\footnotesize
\begin{verbatim}
Step.MultiStep.Steps                   = ["SingleStep1", "SingleStep2"] \
                                                                   # steps that compose this multi-step
Step.MultiStep.Baselines.Station1      = [0, 0, 0, 1, 1, 2]        # baselines to use
Step.MultiStep.Baselines.Station2      = [0, 1, 2, 1, 2, 2]        # (all if empty)
Step.MultiStep.Sources                 = ["3C343"]                 # list of sources
Step.MultiStep.ExtraSources            = ["M81"]                   # list of sources outside patch
Step.MultiStep.InstrumentModel         = ["BANDPASS", "TOTALGAIN", "PATCHGAIN"] \ 
                                                                   # instrument model
Step.MultiStep.Integration.Freq        = 2                         # integration interval: f(Hz)
Step.MultiStep.Integration.Time        = 0.5                       # integration interval: t(s)
Step.MultiStep.Correlation.Selection   = CROSS                     # one of AUTO, CROSS, ALL
Step.MultiStep.Correlation.Type        = ["XX", "XY", "YX", "YY"]  # which (cross) correlations to use

Step.SingleStep1.Baselines.Station1    = [0, 1]                    # baselines to use
Step.SingleStep1.Baselines.Station2    = [1, 2]                    # (all if empty)
Step.SingleStep1.Sources               = []                        # list of sources
Step.SingleStep1.InstrumentModel       = ["BANDPASS", "TOTALGAIN"] # instrument model
Step.SingleStep1.Operation             = SOLVE                     # one of SOLVE, SUBTRACT, CORRECT, \
                                                                   # PREDICT, SHIFT, REFIT
Step.SingleStep1.OutputData            = "OUTDATA1"                # MS output data column
Step.SingleStep1.Solve.MaxIter         = 10                        # maximum number of iterations
Step.SingleStep1.Solve.Epsilon         = 1e-7                      # convergence threshold
Step.SingleStep1.Solve.MinConverged    = 0.95                      # fraction that must have converged
Step.SingleStep1.Solve.Parms           = ["PHASE:*"]               # names of solvable parameters
Step.SingleStep1.Solve.ExclParms       = [""]                      # parameters excluded from solve
Step.SingleStep1.Solve.DomainSize.Freq = 1000                      # f(Hz)
Step.SingleStep1.Solve.DomainSize.Time = 1                         # t(s)

Step.SingleStep2.Baselines.Station1    = []                        # baselines to use
Step.SingleStep2.Baselines.Station2    = []                        # (all if empty)
Step.SingleStep2.Sources               = []                        # list of sources
Step.SingleStep2.InstrumentModel       = ["DirGain", "Phase"]      # instrument model
Step.SingleStep2.Operation             = CORRECT                   # one of SOLVE, SUBTRACT, CORRECT, \
                                                                   # PREDICT, SHIFT, REFIT
Step.SingleStep2.OutputData            = "OUTDATA2"                # MS output data column
\end{verbatim}
}

\subsection*{BBDB}
\label{app-bbdb}
This contains information on how the blackboard database and the parameter
databases can be reached.
\begin{description}
\item [Host] : \emph{string} \\
    Hostname or IP address of the host on which the black board database and
    the parameter databases reside.
\item [Port] : \emph{int} \\
    Port number on which the blackboard database server is listening.
\item [DBName] : \emph{string} \\
    Name of the black board database.
\item [UserName] : \emph{string} \\
    Username to access the black board database.
\item [PassWord] : \emph{string} \\
    Password to access the black board database.
\end{description}

\subsection*{ParmDB}
\label{app-parmdb}
\begin{description}
\item [Instrument] : \emph{string} \\
    Path to the AIPS++ table containing the instrument parameters.
\item [LocalSky] : \emph{string} \\
    Path to the AIPS++ table containing the local sky model parameters.
\item [History] : \emph{string}
    Path to the AIPS++ table containing the solve history.
\end{description}

\subsection*{Correlation}
\label{app-correlation}
\begin{description}
\item [Selection] : \emph{string} \\
    Station correlations to use. Should be one of 'AUTO', 'CROSS', or 'ALL'.
    \par
    AUTO: Use only correlations of each station with itself (i.e. no base
    lines). \textcolor{red}{Not yet implemented.} \\ CROSS: Use only
    correlations between stations (i.e. base lines). \\ ALL: Use auto and
    cross correlations both.
\item [Type] : \emph{string} \\
    Correlations of which polarizations to use, one or more of 'XX', 'XY',
    'YX', 'YY'. As an example, suppose we select 'XX' here and set Selection
    to 'AUTO', then the X polarization signal of each station is correlated
    with itself.  However, if we set Selection to 'CROSS' then the X
    polarization of station A is correlated with the X polarization of station
    B for each base line (A,B)
\end{description}

\subsection*{DomainSize}
\label{app-domainsize}
\begin{description}
\item [Freq] : \emph{double} \\
    The size of the domain in frequency (Hz).
\item [Time] : \emph{double} \\
    The size of the domain in time (s).
\end{description}

\subsection*{Baselines}
\label{app-baselines}
The selected baselines. A baseline is a pair of stations. The first station of
the pair is contained in Station1, the second in Station2. For example,
suppose we have six baselines: (A, B), (A, C), (A, D), (B, C), (B, D), (C,
D). Then Station1 would contain [A, A, A, B, B, C] and Station2 would contain
[B, C, D, C, D, D]. The lengths of Station1 and Station2 should always be
equal. If both fields are left empty, all baselines are used.
\begin{description}
\item [Station1] : \emph{vector$<$string$>$} \\
    One name for each baseline: the first station in the pair that forms the
    baseline.
\item [Station2] : \emph{vector$<$string$>$} \\
    One name for each baseline: the second station in the pair that forms the
    baseline.
\end{description}

\subsection*{Solve}
\label{app-solve}
\begin{description}
\item [MaxIter] : \emph{int} \\
    Maximum number of iterations.
\item [Epsilon] : \emph{double} \\
    Minimal difference between the old and the new parameter values after each
    iteration. When the difference falls below this threshold, the solver will
    stop iterating.
\item [MinConverged] : \emph{double} \\
    Minimal fraction of solve domains that must have converged to declare
    overall convergence.
\item [Parms] : \emph{vector$<$string$>$} \\
    Parameters to solve for. Wildcards are allowed, e.g. BANDPASS:*.
\item [ExclParms] : \emph{vector$<$string$>$} \\
    Subset of the parameters selected by Parms that should not be solved for.
    For example, if we would like to solve for the gain (amplitude, phase) of
    each station, but we would also like to fix the phase of the first station
    (STATION0) this can be specified as follows: {\footnotesize
\begin{verbatim}
Solve.Parms = ["gain:*"]
Solve.ExclParms = ["gain:*:phase:STATION0"]
\end{verbatim}
}
\item [DomainSize] : \emph{DomainSize} \\
    Size of the solve domain. The work domain is divided in solve domains and
    a solution is computed for each solve domain independently.
\end{description}



\end{document}
