\documentclass{article}

% opencl uitzoeken


% zoek parallelisme: onafhankelijke berekeningen
% voor de correlator geldt dat de berekiningen onafhankelijk zijn, maar IO niet!
% met many cores is de I/O vaak de bottleneck


%% pas je algorithmen aan op many-cores


%% 1) zoek parallelisme in je algorithme.
%%    vaak aanwezig. Zoek onafhankelijke operaties:
%%     voorbeelden:
%%    - correlator: kanalen, polatisaties, subbanden zijn onafhankelijk.
%%    - polyphase: stations zijn onafhankelijk
%%    - imaging: maak parallelisme: beeld elk kanaal af op een image, tel deze later op

  
%% 2) mem bw/ops neemt af met many cores
%%    optimaliseer.

%% dus optimaliseren: 
%%     - algo specifiek (reduceer mem loads)
%%     - architectuur-specifieke optimalisaties (cache gedrag, delays, floating point instructions)

%% manycores ondersteunen complex niet erg goed. Alleen BG/P wel.
%% Vaak de reals en de imags in aparte arrays stoppen.

\usepackage{spconf}

\title{How to Build a Correlator on Many-Core Hardware}

\name{Rob V. van Nieuwpoort and John W. Romein}
%\texttt{\{nieuwpoort,romein\}@astron.nl}}

\address{Stichting ASTRON (Netherlands Institute for Radio Astronomy) \\
Oude Hoogeveensedijk 4 \\
7991 PD\ \ Dwingeloo \\
The Netherlands}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
% wat gaat de lezer leren van dit paper?

% we geven een leidraad voor het kiezen van de juiste architectuur voor het probleem van de lezer
% voor goede performance heb je nodig:
%  - kennis van algorithme
%  - kennis van de architecturen
%  - inzicht over hoe je de mapping van algorithme op architectuur het beste kunt doen
% dit paper geeft inzicht in de verschillen tussen architecturen, en inzicht over welke factoren belangrijk zijn om de mapping goed te doen.

Radio telescopes produce enormous amounts of data.
The Low Frequency Array (LOFAR) stations~\cite{Butcher:04,deVos:09}, for
instance, will produce some tens of petabits per day; the dishes from the
Australian SKA Pathfinder (ASKAP) will even produce over six exabits per day.
To extract the sky signal from the system noise, the \emph{correlator\/}
correlates the signals by multiplying the samples of each pair of receivers.
Additionally, the correlator integrates correlations over time, to reduce
the amount of data.

Typically, custom-built hardware is used to correlate the signals.
A recent development is to use a supercomputer~\cite{Romein:06,Romein:09b}.
Both approaches have important advantages and disadvantages.
Custom-built hardware is efficient and consumes modest amounts of power, but is
inflexible, expensive to design, and has a long development time.
Solutions that use a supercomputer are much more flexible, but are less
efficient, consume more power, and are expensive to purchase.
Future instruments, like the Square Kilometre Array (SKA), need several orders
of magnitude more computational resources.
It is likely that the requirements of the SKA cannot be met by using
current supercomputer technology.

During the past ten years, the high-performance computing community has
steadily adopted clusters of Graphics Processor Units (GPUs) as a viable
alternative to supercomputers, due to their unparalleled growth in
computational performance, increasing flexibility, increasing programmability,
relatively high power efficiency, and low purchase costs.
High-end GPUs are highly parallel and contain hundreds of processor cores.
However, their usefulness is often limited to applications that do not require
double-precision floating-point arithmetics, since there is no need for
double-precision calculations to play games.
Hence, the support for double-precision arithmetic is typically poor.
Fortunately, many signal-processing applications do not require double
precision.

In this article, we explain how modern multi-core architectures can be fully
exploited for signal-processing purposes.
Additionally, we give insights into their architectural limitations, and how
to best cope with them.
We treat five different, popular multi-core architectures: the IBM Cell
Broadband Engine, GPUs from Nvidia and ATI, the IBM Blue Gene/P, and
the Intel Core i7 processors.
We discuss their similarities and differences, and how the architectural
differences affect optimization choices and the eventual performance of a
correlator.
We strongly focus on correlators, but many of the findings, claims, and
optimizations hold for other signal-processing algorithms as well, both in the
area of radio astronomy and abroad.
We discuss the programmability of each of the architectures, but this paper
should be of special interest to those who are willing to put some extra
programming effort to obtain good performance, even if high-level programming
support is not available.


\section{Novel trends in modern radio astronomy}
    - LOFAR, SKA

\section{Correlating signals}


\section{Many-core architectures}

\subsection{General Purpose multi-core CPU}

As a reference, we implemented the correlator on a multi-core general
purpose architecture, a quad core Intel Core~i7 CPU.  There is 32~KB
of on-chip L1 data cache per core, 256~KB L2 cache per core, and 8~MB
of shared L3 cache.  The theoretical peak performance of the system is
85~gflops, in single precision.  The parallelism comes from four cores
with two-way hyperthreading, and a vector length of four floats,
provided by the SSE4 instruction set.

The architecture has several important drawbacks for our application.
First, there is no fused multiply-add instruction.  Since the
correlator performs mostly multiplies and adds, this can cause a
performance penalty. The processor does have multiple pipelines, and
the multiply and add instructions are executed in different pipelines,
allowing eight flops per cycle per core.

Another problem is that SSE's shuffle instructions to move data around
in vector registers are more limited than for instance on the
\mbox{Cell/B.E.} processor. This complicates an efficient
implementation.  For the future Intel Larrabee GPU, and for the next
generation of Intel processors, both a fused multiply-add instruction
and improved shuffle support has been announced.  The number of SSE
registers is small (sixteen 128-bit registers), allowing only little
data reuse.  This is a problem for the correlator, since the tile size
is limited by the number of registers.  A smaller tile size means less
opportunity for data reuse, increasing the memory bandwidth that is
required.


\subsection{IBM Blue Gene/P}

The IBM Blue Gene/P~(BG/P)~\cite{bgp} is the architecture that is
currently used for the LOFAR correlator~\cite{sc09}.  Four PowerPC
processors are integrated on each Blue Gene/P chip.  The BG/P is an
energy efficient supercomputer. This is accomplished by using many
small, low-power chips, at a low clock frequency.  The supercomputer
also has excellent I/O capabilities, there are five specialized
networks for communication.

We found that the BG/P is extremely suitable for our application,
since it is highly optimized for processing of complex numbers.  The
BG/P performs \emph{all} floating point operations in double
precision, which is overkill for our application.  In contrast to all
other architectures we evaluate, the problem is compute bound instead
of I/O bound, thanks to the BG/P's high memory bandwidth per
operation. It is 3--10 times higher than for the other architectures.
The BG/P has 32 vector registers of width 2.  Therefore, 64 floating
point numbers (with double precision) can be kept in registers
simultaneously. This is the same amount as on the general purpose
Intel chip, but an important difference is that the BG/P has 32
registers of width 2, compared to Intel's 16 of width 4.  The smaller
vector size reduces the amount of shuffle instructions needed.


\subsection{ATI GPU}

The most high-end GPU provided by ATI (recently acquired by AMD) is
the 4870~\cite{amd-manual}.  The 4870 chip contains 800 scalar 32-bit
streaming processors.  The theoretical peak performance is
1.2~teraflops. The board uses a PCI-express~2.0 interface
for communication with the host system.  Ten cores
share 16 KB of local memory and separate L1 texture cache.  The L2
cache is shared. The The application can specify if a read should be
cached or not.  The SIMD cores can exchange data using 16 KB of global
memory.

The ATI 4870 GPU has the largest number of cores of all architectures
we evaluate (800).  However, the architecture has several important
drawbacks for data-intensive applications.  First, there is no way to
synchronize threads. With other architectures, we can improve the
cache hit ratio significantly by letting threads that access the same
samples run in lock step, increasing data reuse.  Second, the
host-to-device bandwidth is too low. In practice, the achieved
PCI-express bandwidth is far from the theoretical limit. The achieved
bandwidth is not enough to keep all cores busy.  Third, we found that
overlapping communication with computation by performing asynchronous
data transfers between the host and the device has a large impact on
kernel performance. We observed kernel slowdowns of \emph{a factor of
  three} due to transfers in the background.  Fourth, the architecture
does not provide random write access to device memory, but only to
\emph{host} memory. However, for our application which is mostly
read-performance bound, this does not have a large impact.


\subsection{NVIDIA GPU}

NVIDIA's Tesla C1060 contains a GTX~280 GPU with 240 single precision
and 30 double precision ALUs.  Current NVIDIA GPUs thus have fewer
cores than ATI GPUs, but the individual cores are faster. The memory
architecture is also quite different. NVIDIA GPUs still use GDDR3
memory, while ATI already uses GDDR5 with the 4870~GPU.  The
theoretical peak performance is 933 gflops.

The number of registers is large: there are 16384 32-bit floating
point registers per multiprocessor. There also is 16~KB of shared
memory per multiprocessor.  This memory is shared between all threads
on a multiprocessor, but not globally.  There is a total amount of 64
KB of constant memory on the chip.  Finally, texture caching hardware
is available.  The application has some control over the caching
hardware.  It is possible to specify which area of device memory must
be cached, while the shared memory is completely managed by the
application.

On NVIDIA GPUs, it is possible to synchronize the threads within a
multiprocessor.  With our application, we exploit this to increase the
cache hit ratio. This improves performance considerably.  When
accessing device memory, it is important to make sure that
simultaneous memory accesses by different threads are \emph{coalesced}
into a single memory transaction.  In contrast to ATI hardware, NVIDIA
GPUs support random write access to device memory. This allows a
programming model that is much closer to traditional models, greatly
simplifying software development.  The NVIDIA GPUs suffer from a
similar problem as the ATI GPUs: the host-to-device bandwidth is
equally low.



\subsection{The Cell Broadband Engine}

The Cell Broadband Engine (\mbox{Cell/B.E.})~\cite{cell} is a
heterogeneous many-core processor, designed by Sony, Toshiba and IBM
(STI).  The \mbox{Cell/B.E.} has nine cores: the Power Processing
Element (PPE), acting as a main processor, and eight Synergistic
Processing Elements (SPEs) that provide the real processing power.
The cores, the main memory, and the external I/O are connected by a
high-bandwidth Element Interconnection Bus (EIB).  The main memory has
a high-bandwidth, and uses XDR (Rambus).  The PPE's main role is to
run the operating system and to coordinate the SPEs.  An SPE contains
a RISC-core (the Synergistic Processing Unit (SPU)), a 256KB Local
Store (LS), and a memory flow controller.

The LS is an extremely fast local memory (SRAM) for both code and data
and is managed entirely by the application with explicit DMA
transfers.  The LS can be considered the SPU's L1 cache.  The
\mbox{Cell/B.E.} has a large number of registers: each SPU has 128,
which are 128-bit (4 floats) wide.  The SPU can dispatch two
instructions in each clock cycle using the two pipelines designated
\emph{even} and \emph{odd}. Most of the arithmetic instructions
execute on the even pipe, while most of the memory instructions
execute on the odd pipe.  We use a QS21 Cell blade with two
\mbox{Cell/B.E.} processors.  The 8 SPEs of a single chip in the
system have a total theoretical single-precision peak performance of
205 gflops.

\subsection{Larrabee}

Intel's Larrabee~\cite{larrabee} (to be released) is another promising
architecture.  Larrabee will be a hybrid between a GPU and a
multi-core CPU.  It will be compatible with the x86 architecture, but
will have 4-way simultaneous multi-threading, 512-bit wide vector
units, shuffle and multiply-add instructions, and special texturing
hardware. Larrabee will use in-order execution, and will have coherent
caches.  Unlike current GPUs, but similar to the \mbox{Cell/B.E.},
Larrabee will have a ring bus for communication between cores and for 
memory transactions.

\subsection{Essential properties and differences}

\begin{table*}[t]
\begin{center}
{\small
\begin{tabular}{l|l|l}
feature                   & Cell/B.E.                      & GPUs \\
\hline
access times              & uniform                        & non-uniform \\
cache sharing level       & single thread (SPE)            & all threads in a multiprocessor \\
access to off-chip memory & only through DMA               & supported \\
memory access overlapping & asynchronous DMA               & hardware-managed thread preemption \\
communication             & DMA between SPEs               & independent thread blocks + \\
                          &                                & shared memory within a block \\
\end{tabular}
} %\small
\end{center}
\vspace{-0.5cm}
\caption{Differences between many-core memory architectures.}
\label{memory-properties}
\end{table*}


\section {optimizing the correlator algorithm}
\- optimaliseren van het algorithme: tiles, etc
\subsection{Intel}
\subsection{BG/P}
\subsection{NVIDIA}
\subsection{ATI}
\subsection{Cell}
\subsection{Larrabee}

 
\section{Programmability}


\section{Conclusions}


\bibliographystyle{IEEEbib}
\bibliography{spm}

\end{document}
