\documentclass{article}

\usepackage{spconf}

\title{How to Build a Correlator on Many-Core Hardware}

\name{Rob V. van Nieuwpoort and John W. Romein}
%\texttt{\{nieuwpoort,romein\}@astron.nl}}

\address{Stichting ASTRON (Netherlands Institute for Radio Astronomy) \\
Oude Hoogeveensedijk 4 \\
7991 PD\ \ Dwingeloo \\
The Netherlands}


\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

Radio telescopes produce enormous amounts of data.
The Low Frequency Array (LOFAR) stations~\cite{Butcher:04,deVos:09}, for
instance, will produce some tens of petabits per day; the dishes from the
Australian SKA Pathfinder (ASKAP) will even produce over six exabits per day.
To extract the sky signal from the system noise, the \emph{correlator\/}
correlates the signals by multiplying the samples of each pair of receivers.
Additionally, the correlator integrates correlations over time, to reduce
the amount of data.

Typically, custom-built hardware is used to correlate the signals.
A recent development is to use a supercomputer~\cite{Romein:06,Romein:09b}.
Both approaches have important advantages and disadvantages.
Custom-built hardware is efficient and consumes modest amounts of power, but is
inflexible, expensive to design, and has a long development time.
Solutions that use a supercomputer are much more flexible, but are less
efficient, consume more power, and are expensive to purchase.
Future instruments, like the Square Kilometre Array (SKA), need several orders
of magnitude more computational resources.
It is likely that the requirements of the SKA cannot be met by using
current supercomputer technology.

During the past ten years, the high-performance computing community has
steadily adopted clusters of Graphics Processor Units (GPUs) as a viable
alternative to supercomputers, due to their unparalleled growth in
computational performance, increasing flexibility, increasing programmability,
relatively high power efficiency, and low purchase costs.
High-end GPUs are highly parallel and contain hundreds of processor cores.
However, their usefulness is often limited to applications that do not require
double-precision floating-point arithmetics, since there is no need for
double-precision calculations to play games.
Hence, the support for double-precision arithmetic is typically poor.
Fortunately, many signal-processing applications do not require double
precision.

In this article, we explain how modern multi-core architectures can be fully
exploited for signal-processing purposes.
Additionally, we give insights into their architectural limitations, and how
to best cope with them.
We treat five different, popular multi-core architectures: the IBM Cell
Broadband Engine, GPUs from Nvidia and ATI, the IBM Blue Gene/P, and
the Intel Core i7 processors.
We discuss their similarities and differences, and how the architectural
differences affect optimization choices and the eventual performance of a
correlator.
We strongly focus on correlators, but many of the findings, claims, and
optimizations hold for other signal-processing algorithms as well, both in the
area of radio astronomy and abroad.
We discuss the programmability of each of the architectures, but this paper
should be of special interest to those who are willing to put some extra
programming effort to obtain good performance, even if high-level programming
support is not available.



\bibliographystyle{IEEEbib}
\bibliography{spm}

\end{document}
