% -----------------------  Define your Preamble Here 
\documentclass[]{lofar}
\usepackage{layout}

\include{definitions}
%\usepackage{amsmath,amsthm, amsfonts, amssymb, amsxtra,amsopn}
%\usepackage{graphicx}
\usepackage{float}
%\usepackage{times}
\usepackage{fancybox}
%\usepackage{algorithmic}
\usepackage[dvips]{hyperref}
\DeclareGraphicsExtensions{.eps}

\floatstyle{boxed}
\newfloat{assumption}{ht}{ass}[subsection]
\newfloat{prerequisite}{ht}{pre}[subsection]
\newfloat{requirement}{H}{req}[subsection]

% ------------------------  End of your preamble.
\begin{document}

  \maketitle

  \begin{abstract}

    \em{The implementation details of a blackboard application
      framework for self calibration in the LOFAR Central Processor}

    For the LOFAR project a self-calibration program shall be
    written. The idea is that this program will have the architectural
    structure of a "BlackBoard". The design and implementation details
    of this application, are described here.
   
  \end{abstract}

  \tableofcontents

  \listoffigures

%  \listoftables

  \section{introduction}
  \label{sec:introduction}\hypertarget{sec:introduction}{}

    \subsection{Purpose of this document}
    \label{subsec:purpose}\hypertarget{subsec:purpose}{}

      This document describes how the Blackboard architectural pattern
      is implemented to fit the needs and requirements for the LOFAR
      self-calibration-application. An demo application has been made,
      and test-runs have been performed in order to get a feeling of
      what is really needed. The results of those tests are described
      in \hyperlink{bib:LOFAR-ASTRON-MEM-096}{[HOO03]} and are used in
      for the description of the framework that is presented in this
      document.

    \subsection{context}
    \label{subsec:context}\hypertarget{subsec:context}{}

      This document describes a high-level design of a blackboard
      framework for use in imaging applications within th LOFAR Central
      Processor. It does not contain a design for such an application,
      even though it refers to the self-calibration a lot, it is
      intended to be as generic as possible.

  \section{architecture}
  \label{sec:architecture}\hypertarget{sec:architecture}{}

    \subsection{human perspectives}

      This section describes how different human beings can have
      different perspectives on the system.

      \subsubsection{management}

        The Blackboard framework handles the distribution of an
        application over a part of the cluster. From the management
        perspective only a priority has to be given to the
        application. On this high level this only means: ``how many
        nodes may it use?'' and ``For what period of time may it use
        them?''\footnote{It is yet to be considered what is to be done
        when tho maximum execution time is exceeded. One thing might
        be to settle for the present time intermediate
        results. Another might be to add an extra request to the
        job-queue.\label{execution-duration}}. The application has been
        given strategy scripts to handle on that node that define the
        internal priorities and the distribution within the appointed
        sub-cluster.

        \begin{requirement}
          It must be possible to configure the number of nodes to use
          for an instance of a BlackBoard application.
          \caption{number of nodes\label{req:number-of-nodes}}
        \end{requirement}

      \subsubsection{system administration}

        Given the appointed sub-cluster, the application must be able
        to run. Therefore different components and utilities must be
        deployed to the different nodes in the system
        (\hyperlink{pre:software-availability}{prerequisite \ref{pre:software-availability}}). There are
        mechanisms available in the LOFAR cluster to do this. These
        mechanisms need some scripting or configuring. A lot of this
        can be automated, but a system administrator has to be aware
        of those mechanisms and the way the BlackBoard framework uses
        them.

        \begin{requirement}
          It must be possible to configure exactly what nodes to use
          for an instance of a BlackBoard application.
          \caption{specification of nodes\label{req:specification-of-nodes}}
        \end{requirement}

        \begin{figure}
          \includegraphics[]{../figures/CEP_Cluster_Selfcal_mapped.eps}
          \hypertarget{fig:selfcalMapped}{}
          \caption{a sample mapping of a selfcal application on the
          LOFAR cluster.\label{fig:selfcalMapped}}
        \end{figure}

        \begin{prerequisite}
          It must be possible to specify what software ``might'' be
          needed at what nodes.
          \caption{software availability\label{pre:software-availability}}
        \end{prerequisite}

      \subsubsection{instrumentalist}

        The instrumentalist will be acting on behalf of the
        astronomer-user to advice on technical usability of data
        collected. Given the strategy that a user has specified, the
        instrumentalist will decide how data will be partitioned for a
        certain self-calibration job.

        \begin{requirement}
          It must be possible to specify how data is to be partitioned.
          \caption{data partitioning\label{req:data-partitioning}}
        \end{requirement}

        It is not very interesting to specify what part of the data is
        calibrated for in what part of the system. It might sometimes
        be opportune to force a certain distribution if it is foreseen
        that certain data/parameter sets are mutually dependent.

        \begin{requirement}
          It should be possible to specify how data is going to
          distributed over the cluster given a certain partitioning
          \caption{data distribution\label{req:data-distribution}}
        \end{requirement}

      \subsubsection{astronomer-user}

        The astronomer that has an observation approved by the LOFAR
        board, will have to write an script to define his solution
        strategy. In this script several things are defined. How is the data to be
        solved for, being partitioned over the appointed cluster? How
        is the responsibility over parameter to solve divided? Is a
        ``peeling'' strategy applied? And so more.

        \begin{requirement}
          It must be possible to specify what strategy, or strategies
          to use for a self-calibration job.
          \caption{strategy specification\label{req:strategy-specification}}
        \end{requirement}

        An a-user would not want to know about the number of nodes
        assigned to his job. He would certainly not want to have to
        specify how his data is going to be partitioned over these
        nodes. 

        \begin{requirement}
          It must not be needed to specify how data is going to
          distributed over the cluster given a certain partitioning
          \caption{data distribution 2\label{req:data-distribution-2}}
        \end{requirement}

        However, the a-user might want to limit the permissible
        partition possibilities to a fixed number. Or maybe even limit
        it to fixed boundaries.

        \begin{requirement}
          It must be possible to specify data can not be partitioned.
          \caption{data partitioning 2\label{req:data-partitioning-2}}
        \end{requirement}

        During the calculation the a-user might ask for temporary
        results, in order to see if things are going his way. Some
        strategy directives can then be changed on the fly.

        \begin{requirement}
          It must be possible to reconfigure strategy directives on
          the fly.
          \caption{reconfiguring\label{req:reconfiguring}}
        \end{requirement}

    \subsection{system internals}

      This section will contain some UML diagrams and accompanying
      descriptions of the intended system internals.

      \subsection{control hierarchy}

      \subsection{collaboration}

      \subsubsection{dynamic model}

        \paragraph{lifetimes of objects}

    \subsection{protocol semantics}

      There are several inter component protocols that need to be
      observed for the system to function. These are discussed in this
      section.

      First there is the way the blackboard is addressed by other
      components. In figure
      \hyperlink{fig:overview}{\ref{fig:overview}}, these are
      represented by the straight dependency connections. Then there
      are embedded in the messages posted on the blackboard protocols
      that must maintain and guarantee the validity of the
      system-state at all times. In figure
      \hyperlink{fig:overview}{\ref{fig:overview}}, these are
      represented by the dotted dependency lines. The actual
      communication is embedded in whatever is put on the
      blackboard. A valid system state in this context would mean that
      there are no solution threads laying around unfinished and
      unevaluated.

      External knowledge sources, that are being embedded in a
      BlackBoard-instance are going to have to adhere to some rules on
      the communications.

      \begin{figure}
        \includegraphics[]{../figures/overview.eps}
        \hypertarget{fig:overview}{}
        \caption{the overall structure of a BlackBoard-application\label{fig:overview}}
      \end{figure}

      Then the final group of protocols are those with external
      components like data-persistency mechanisms.

      \begin{requirement}
        It must be possible to specify which right knowledge sources
        have on certain parts of the blackboard-data.
        \caption{data access rights\label{req:data-access-rights}}
        \hypertarget{req:data-access-rights}{}
      \end{requirement}

      \subsubsection{blackboard api}

        Components that access the blackboard will own tables or areas
        on the blackboard. These are identified, and so are entries in
        these areas. Also types of components are identified to be
        able to determine how to interpret the data in an area.

        Regular insert statements will be available. Update will be
        available for specific knowledge sources on specific fields. A
        definition method for this will have to be specified in
        accordance with requirement
        \hyperlink{req:data-access-rights}{\ref{req:data-access-rights}}.

        Instead of a delete operation, there will be a
        finalize/abandon set of calls. These collapse a branch of
        intermediate results leaving only a final result, or a
        tried-and-failed entry on the start of the branch.

    \subsection{deployment}

    \subsection{code organisation}

      \subsubsection{writing enhancements}

  \section{data storage}
  \label{sec:data-storage}\hypertarget{sec:data-storage}{}

    The major classes of storage requirements are

    \begin{itemize}
      \item parameters
      \item datasets
      \item the strategy
      \item the blackboard logistics
    \end{itemize}

    \subsection{Parameter administration}
    \label{subsec:parameter-administration}\hypertarget{subsec:parameter-administration}{}

      Parameters are solved locally, on subclusters of the LOFAR
      central processor and then distributed or discarded along the
      rest of the cluster.

      \begin{prerequisite}
        A full fledged database management system (DBMS),
        object oriented or relational, must be available.
        \caption{DBMS\label{pre:dbms}}
      \end{prerequisite}

      \begin{prerequisite}
        The DBMS must be able to perform transaction based processing.
        \caption{transactions\label{pre:transactions}}
      \end{prerequisite}

      \begin{prerequisite}
        The DBMS should support table inheritance.
        \caption{table inheritence\label{pre:table-inheritance}}
      \end{prerequisite}

      \begin{requirement}
        \caption{Parameter distribution\label{req:parameter-distribution}}
        There must be a mechanism that can cleverly distribute
        parameters to any interested party upon request by the
        originator.
      \end{requirement}

      \begin{figure}
        \includegraphics[]{../figures/local-_global-commit.eps}
        \hypertarget{fig:localGlobalCommit}{}
        \caption{a simple topology for a distributed blackboard database\label{fig:localGlobalCommit}}
      \end{figure}

      In the distributed selfcal program, the available cluster will
      be partitioned as well as the data to be calibrated on. Several
      sub-clusters will have responsibility over part of the data and
      hence part of the parameters to be calibrated.

      A calibrating cluster is a set of programs running on a (set of)
      node(s), that is doing part of the calibration.  A calibrating
      cluster might use more parameters then it has responsibility
      over. Therefore it must have all parameters to its
      disposal.

      \begin{verse}{Note}

        The parameters will have to be at it's disposal in
        principal. But not necessarily available at all time?

      \end{verse}

      A calibrating cluster is converging parameters from initial
      values to corrected values. This converging might go in the
      wrong direction so a set of transactions might be leading to
      invalid results and must not be distributed. It might also lead
      to good results and be approved by a controlling program for
      distribution. The idea is that this program will then close a
      more global transaction, leading to the replication of the data.

      The essence of the requirement is that we have logical
      transactions that have to be nested. The fact that they seem to
      be centered around separate database-engines does not seem to be
      important. We need only one level of nesting.

      \subsubsection{nested cooperative transactions}
      \label{subsubsec:nested-cooperative-transaction}\hypertarget{subsubsec:nested-cooperative-transaction}{}

        One solution for this requirement is that transactions in fact
        can be nested by virtue of the database-vendor. Let's assume
        we only need one level of nesting. However several processes
        with their own connections to the database must be able to
        participate in a single global level transaction.

        \begin{figure}
          \includegraphics[]{../figures/database.eps}
          \hypertarget{fig:database}{}
          \caption{database use in a distributed blackboard\label{fig:database}}
        \end{figure}

        For this nesting to make sense, a database client must be able
        to start a transaction and get an transaction id to distribute
        among co-workers.

        These co-workers can then work on the global transaction, by
        ``connecting to'' or ``joining'' a transaction by id. Once joined,
        they can change data within the scope of global
        transaction. This can be as singular operations or as
        transactions. Any changes committed is valid an visible to any
        client joining in the transaction. Once the owner decides so
        he can close the global transaction. This closing can be a
        commit as well as a rollback, both with the expected result.
     
        So should participants be notified of the closing of the
        transaction?  No, they will notice when trying to manipulate
        data.

        \begin{figure}
          \includegraphics[]{../figures/nestedTransaction.eps}
          \hypertarget{fig:nestedTransaction}{}
          \caption{an action sequence that asks for nested transactions\label{fig:nestedTransactions}}
        \end{figure}

      \subsubsection{selective replication}
      \label{subsubsec:selective-replication}\hypertarget{subsubsec:selective-replication}{}%

        Another solution would be to use selective replication. This
        could mean that records will only be replicated after a
        specific trigger. It could also mean that only part of the
        datamodel will be replicated to other engines.

    \subsection{Datasets}
    \label{subsec:Datasets}\hypertarget{subsec:Datasets}{}%

      Datasets are high-volume and intensively queried. Updates are
      mostly copies. Using a database engine would be to much of a
      CPU-cycle and memory hog. Therefore a more direct interface to
      the filesystem is assumed.

      \begin{assumption}
        There is a measurement-set package in AIPS++ that will be used
        for manipulating measurement data, as a temporary solution. A
        more permanent solution will be provided by the Lofar Common
        Software work-package.
        \caption{measurement-set package\label{ass:aips++}}
      \end{assumption}

      \begin{prerequisite}
        There must be a package for manipulation of measurement sets available.
        \caption{measurement-set manipulation\label{pre:aips++}}
      \end{prerequisite}

    \subsection{The strategy}{A high level script}
    \label{subsec:strategy}\hypertarget{subsec:strategy}{}

      The implementation of the strategy is a python script using the
      components defined in the selfcal framework. In the final
      (c++-)implementation of selfcal, the high level control object
      is a process with an embedded python interpreter. It reads the
      script when started and executes it, creating the processes
      needed on the nodes available to the program.

      In an advanced enterprise-pro-gold version the control object
      might supply a GUI to interrupt and/or change the strategy on
      the fly. The database used supplies the possibility to watch
      progress on-line and intervene if needed.

      The in core storage of the script might be python byte code, but
      it might be a database as well. It must be taken into account
      that user representation for both database and in-core code
      being synchronized and with integrity guaranteed, will be hard
      to implement.

      \begin{figure}
        \includegraphics[]{../figures/strategy_interpreter.eps}
        \hypertarget{fig:strategy_interpreter}{}
        \caption{strategy interpreter\label{fig:strategy_interpreter}}
      \end{figure}

    \subsection{The blackboard logistics}
    \label{subsec:blackboard-logistics}\hypertarget{subsec:blackboard-logistics}{}

      For the blackboard to function several protocols need to be
      described. To guarantee loose coupling between components it is
      not desirable to have the dialogs in these protocols needing
      real-time message interchange. Therefor a database-based
      approach is chosen, which is described in
      \hyperlink{sec:logistical-datamodel}{Section
      \ref{sec:logistical-datamodel}}.

      \begin{requirement}
        Protocol dialogues between components in the BlackBoard should
        be asynchronous.
        \caption{asynchronous dialogues\label{req:asynchronous}}
      \end{requirement}

  \section{distributed hierarchical blackboard}
  \label{sec:distributed-hierarchical-blackboard}\hypertarget{sec:distributed-hierarchical-blackboard}{}%

      \begin{figure}
        \includegraphics[]{../figures/main.usecase.eps}
        \hypertarget{fig:main.usecase}{}
        \caption{the main use-cases for the calibration program\label{fig:main.usecase}}
      \end{figure}

    \subsection{distribution over nodes and sub-clusters}
    \label{subsec:distribution}\hypertarget{subsec:distribution}{}%

      For performance reasons we want to divide the measurement data
      over the cluster in a way that distributes the disk-usage and
      minimizes the network-usage.

      Logically there is only one Blackboard. It is a distributed
      application however, each instance dividing it's workload to
      it's own liking. Each blackboard knows what nodes it has to
      start KnowledgeSources on.

      For this to work it has to be possible to distribute
      responsibility in this fashion as well. For instance; If a
      blackboard has responsibility for a certain time-frame, it must
      be able to change any time dependent parameters at will as long
      as they fall completely in this time frame.
      \begin{em}\large{Note: }Continuity in a function for a parameter
      over time will pose a problem at the edges of the time
      frame.\end{em}

    \subsection{Workload distribution}
    \label{subsec:workload-distribution}\hypertarget{subsec:workload-distribution}{}%

      The workload for a self-calibration process, might be
      distributed in several ways. It is a design choice to decide
      that only partitioning to data, as opposed to partitioning to
      parameters, is opportune. This is based on the order of
      magnitude that the volume of data is bigger then the amount of
      parameters. The data has a limited number of dimensions.

      \begin{assumption}
        The number of dimensions in a measurement set is four.
        \caption{dimensions\label{ass:dimensions}}
      \end{assumption}

      \begin{assumption}
        Data can be partitioned over the dimensions in the dataset,
        without disturbing the self-calibration process.
        \caption{data partitioning\label{ass:partitioning}}
      \end{assumption}

      These are.

      \begin{itemize}

        \item 

          time

	\item 

          frequency

	\item 

          baseline

	\item 

          observatory-direction or field.

      \end{itemize}

      The Blackboard will be configured to delegate tasks to children,
      divided along one of these dimensions. For simplicity's sake it
      is desirable not to partition in any dimension more then
      once. From a logical point of view it doesn't seem necessary to
      do so, even though it might be convenient at times. The
      controlling software would be much simpler though if the
      dimensions are fixed, and partitioned over only once,
      preferably in a fixed order.

      \begin{assumption}
        It isn't necessary to divide along a dimension more then once.
        \caption{data partitioning 2\label{ass:devision}}
      \end{assumption}

    \subsection{components}
    \label{subsec:components}\hypertarget{subsec:components}{}

      \subsubsection{controllers}
      \label{subsubsec:controllers}\hypertarget{subsubsec:controllers}{}

        A Blackboard has a controller that can be asked to start a
        process for a blackboard. Such a process can be a
        child-blackboard including its controller, or one of the
        knowledge sources associated with the blackboard.

      \subsubsection{watchers}
      \label{subsubsec:watchers}\hypertarget{subsubsec:watchers}{}

        \paragraph{simple watchers}
        \label{par:simple-watchers}\hypertarget{par:simple-watchers}{}
          \begin{figure}
            \includegraphics[]{../figures/watch.eps}
            \hypertarget{fig:watchers}{}
            \caption{The case that a watcher decides to advice
            abandoning a solution thread.\label{fig:watchers}}
          \end{figure}

          A watcher polls for changes in a couple of solution
          threads. It might decide that one of them is remarkably more
          sound then the others(see
          hyperlink{fig:watchers}{[\ref{fig:watchers}]}).

          Usually it will have to decide that it can't decide yet.

          \begin{requirement}
            A watcher has to be able to make a tristate decision on
            which of two threads is leading to desirable results.
            \caption{tristate decisions\label{req:no-decision}}
          \end{requirement}

        \paragraph{more complex watchers}
        \label{par:complex-watchers}\hypertarget{par:complex-watchers}{}

          More complex watcher could decide that the results of two or
          more threads should be merged.

          \begin{requirement}
            A watcher should be able to see a possibility to merge the
            results of two solution threads.
            \caption{thread merging\label{req:merge}}
          \end{requirement}

  \section{logistical database}
  \label{sec:logistical-datamodel}
  \hypertarget{sec:logistical-datamodel}{}

    \subsection{Workload distribution}
    \label{subsec:workload-distribution-2}\hypertarget{subsec:workload-distribution-2}{}

      The data, resulting from an observation will be several TB in
      volume
      \hyperlink{bib:LOFAR-ASTRON-MEM-035}{[SCH02]}. To
      conveniently handle this it will have to be partitioned. To
      handle part of the data a specialised sub-cluster will be
      defined. (see \hyperlink{fig:bb.subcluster}{Figure \ref{fig:bb.subcluster}}).

      \begin{figure}
        \includegraphics[]{../figures/bb.subcluster.eps}
        \hypertarget{fig:bb.subcluster}{}
        \caption{the layout of a self-calibration sub-cluster.\label{fig:bb.subcluster}}
      \end{figure}

      As a result of this partitioning the Blackboard will be
      responsible for a subset of the data. This it will know:

      \{self\}1, \{parent-id\}1, \{range\}1

      \begin{itemize}

        \item 

          "self" is the id of this blackboard.

        \item 

          "parent\_id" is the obvious.

        \item 

          "range" is the set of values that defines the dataset used
          for solving parameters.

      \end{itemize}

      the range can vary over several dimensions

      \{time\}1, \{frequency\}1, \{interferometers\}1, \{direction\}1

      \begin{itemize}

	\item 

          "time" is a time range this blackboard is concerned with.
          \begin{requirement}
            It must be possible to handle time-gaps in a measurement set.
            \caption{time gaps\label{req:time-gaps}}
          \end{requirement}

	\item 

          "frequency" is the frequency range or band this blackboard
          is concerned with.

          \begin{requirement}
            It must be possible to handle multiple bands of frequencies in a
            measurement set.
            \caption{frequency bands\label{req:frequency-bands}}
          \end{requirement}

	\item 

          "interferometers" is the set of baselines that this
          BlackBoard is responsible for.

	\item 

          "directions" is a range of points in the sky that function
          as field-centers.

      \end{itemize}

      If for some reason it is considered that on one of the
      dimensions the range to evaluate is to large to do in one step,
      the Blackboard can ask it's BlackBoardController to spawn
      children. The Blackboard will have to know what children it has:

      \{child-id\}1, \{self\}1, \{range\}1

      For all of "time" through "objects" goes that they are a subset
      of the parents fields with the same name.

    \subsection{Workload level}
    \label{subsec:workload-level}\hypertarget{subsec:workload-level}{}%

      The idea is that several analysis threads can exist
      simultaneously. In each of these threads several processes are
      active. In our blackboard model we have the following
      Knowledge-Sources

      \begin{itemize}

	\item 

          A SelfCalEngine. Which contains processes at several
          nodes. There are a solve process and an array of predict
          processes.

	\item 

          A SelfCalController starts engines and forks new
          child-threads if opportune.

	\item 

          A SelfCalWatcher that monitors threads and decides on what
          is or is not a dead end.

      \end{itemize}

      All three have their controlling BB tables.

      \subsubsection{SelfcalEngine}
      \label{subsubsec:SelfcalEngine}\hypertarget{subsubsec:SelfcalEngine}{}

        A SelfcalEngine solves for a set of parameters based on part
        of the data.

        A SelfcalEngine writes a record with globally the following
        structure.

        \{meta-data\}1, \{next-action-type\}1, \{quality\}1, \{p-name, p-value, p-delta\}1-*

        meta-data will contain:

        \{workload-id\}1, \{engine-id\}1, \{controller-id\}1, \{parent-id\}1, \{range\}1

        \begin{itemize}

	  \item 

            the id for the SelfcalEngine that produced the result.

	  \item 

            the id for the SelfcalController that created the workload.

	  \item 

            The parent id of this workload. This could be a thread-id
            or the id of another workload.

	  \item 

            The time that the data was captured.

	  \item 

            The frequency-range examined.

        \end{itemize}

      \subsubsection{SelfcalController}
      \label{subsubsec:SelfcalController}\hypertarget{subsubsec:SelfcalController}{}

        A SelfcalController writes records of workloads like the
        SelfcalEngine, except the it does give a quality and it
        doesn't write the SelfcalEngine id. An engine writes its id in
        a record as it accepts it and starts the associated
        calculation. When it is finished it writes the deltas for all
        parameters and the quality.

        A SelfcalController decides what the next step in a analysis
        path should be. The range might be smaller then that of the
        parent dataset.

        \{controller-id\}1, \{strategy-description\}1, \{range\}1, \{child-id\}0-*

      \subsubsection{SelfcalWatcher}
      \label{subsubsec:SelfcalWatcher}\hypertarget{subsubsec:SelfcalWatcher}{}

        A SelfcalWatcher has to have access to the records of the
        engines. It has a domain to watch containing several analysis
        threads.

        \{watcher-id\}1 \{controller-id\}2-*

  \section{to do}
  \label{sec:todo}\hypertarget{sec:todo}{}

    Whether in code or in configuration, it should be easy to adjust
    the BlackBoard. This should result in a different
    state-machine. An interface has to be written for that. It will
    include definitions of preconditions, postconditions and
    quality-of-results types for different knowledge-sources.

    All of this can be hard-coded in c++, as long as it is easy to
    write and, in the first place efficient to run.

    \subsection{extra figures}
    \begin{figure}
      \includegraphics[]{../figures/control-hierarchy.eps}
    \end{figure}

    \begin{figure}
      \includegraphics[]{../figures/fork.eps}
    \end{figure}

    \begin{figure}
      \includegraphics[]{../figures/solve-step.eps}
    \end{figure}

    \begin{figure}
      \includegraphics[]{../figures/strategy-distribution.eps}
    \end{figure}

\newcommand{\dbappendix}[1]{\section{#1}}%

\appendix

  \dbappendix{assumptions}
  \label{app:assumptions}\hypertarget{app:assumptions}{}

    \listof{assumption}{}

  \dbappendix{prerequisites}
  \label{app:prerequisites}\hypertarget{app:prerequisites}{}

    \listof{prerequisite}{}

  \dbappendix{requirements-list}
  \label{app:requirements-list}\hypertarget{app:requirements-list}{}%

   \listof{requirement}{}

  \dbappendix{revision log}
  \label{app:revision-log}\hypertarget{app:revision-log}{}%

    Hand copy log entries from the cvs log, or make up your own.

    \begin{itemize}

      \item 

        Revision 1.1 2003/06/18 12:06:25 daan

        initial version containing:
        \begin{itemize}

	  \item 

            storage requirements.

        \end{itemize}

    \end{itemize}

  \bibliography{}
  \begin{thebibliography}{WIDELABEL}

    \bibitem[SCH02]{LOFAR-ASTRON-MEM-035}
      \emph{CEP Requirements Analysis, Architectural Design and
      Description} , Kjeld van der Schaaf, Copyright \copyright{} 2002
      ASTRON. \label{bib:LOFAR-ASTRON-MEM-035}

    \bibitem[HOO03]{LOFAR-ASTRON-MEM-096}
      \emph{The use of the Blackboard architectural pattern, in the LOFAR Central Processor, for the selfcal control system},
      D.\ Hoogland,
      Copyright \copyright{} 2003 ASTRON.
      \label{bib:LOFAR-ASTRON-MEM-096}

  \end{thebibliography}

\end{document}
