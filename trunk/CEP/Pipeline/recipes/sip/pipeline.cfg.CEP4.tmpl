# This pipeline.cfg is used on CEP4, to run jobs through Docker and SLURM.

[DEFAULT]
lofarroot = /opt/lofar
casaroot = /opt/casacore
pyraproot = /opt/python-casacore/pyrap
hdf5root = 
wcsroot = /opt/wcslib
pythonpath = /opt/lofar/lib/python2.7/site-packages
# runtime dir is a global FS (nfs, lustre) to exchange small files (parsets, vds, map files, etc)
runtime_directory = /data/share/pipeline
recipe_directories = [%(pythonpath)s/lofarpipe/recipes]
# working dir is the local dir in which input/output dataproducts reside
working_directory = /data/scratch/pipeline
task_files = [%(lofarroot)s/share/pipeline/tasks.cfg, %(lofarroot)s/share/pipeline/tasks.cfg.CEP4]

[layout]
job_directory = %(runtime_directory)s/%(job_name)s

[cluster]
clusterdesc = %(lofarroot)s/share/local.clusterdesc

[deploy]
engine_ppath = %(pythonpath)s:%(pyraproot)s/lib:/opt/cep/pythonlibs/lib/python/site-packages
engine_lpath = %(lofarroot)s/lib:%(casaroot)s/lib:%(pyraproot)s/lib:%(hdf5root)s/lib:%(wcsroot)s/lib

[logging]
log_file = %(runtime_directory)s/%(job_name)s/logs/%(start_time)s/pipeline.log
xml_stat_file = %(runtime_directory)s/%(job_name)s/logs/%(start_time)s/statistics.xml

[feedback]
# Method of providing feedback to LOFAR.
# Valid options:
#    messagebus    Send feedback and status using LCS/MessageBus
#    none          Do NOT send feedback and status
method = messagebus

# Report final state on the message bus? (yes for CEP2 using "startPython.sh", no for CEP4 using "runPipeline.sh")
send_status = no

[remote]
method = custom_cmdline
globalfs = yes

# We take the following path to start a remote container:
#
#  [container] --SSH--> [host] --SRUN--> [worker node] --DOCKER--> [container]
#
# This path is needed because running SRUN from within the container needs a lot of cluster-specific infra
# (SLURM config files, Munge keys, correct Munge user ID, munged).
#
# Throughout this path, we maintain:
#   * userid, which is set to the user that started the master script container (-u `id -u`)
#   * environment (sudo -p, docker -e K=V, etc)
#   * pseudo-tty to prevent buffering logs (ssh -tt, docker -t)

#
# host -> worker node:       srun ...
#                            (Add -w {host} for systems that do not have a global file system, to force job
#                            execution on the host that contains the data)
#
# worker node -> container:  docker run ...
#                            Starts the container on the worker node, with pretty much the same parameters as the master container:
#
#                            --rm: don't linger resources when the container exits
#                            -u (uid): set the user to run as (the calling user)
#                            -h {host}: set container hostname (for easier debugging) (doesnt work yet, using --net=host instead)
#                            -v /data:/data:  map CEP4's global fs. This includes %(working_directory)s and %(runtime_directory)s so those do not have to be mapped as well.
#
#                            /bin/bash -c
#
#                            Required because the pipeline framework needs some bash functionality in the commands it starts.
cmdline = ssh -n -tt -x localhost srun --exclusive --ntasks=1 --cpus-per-task={nr_cores} --jobid={slurm_job_id} --job-name={job_name} docker run --rm -u {uid} -v /data:/data --net=host {docker_env} lofar-pipeline:${LOFAR_TAG} {command}
